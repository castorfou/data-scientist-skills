{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up template\n",
    "For our first project template exercise, we will write a cookiecutter.json file that will contain defaults for our project template.\n",
    "\n",
    "Our cookiecutter.json file will contain three keys:\n",
    "\n",
    "- project\n",
    "- package\n",
    "- license\n",
    "\n",
    "The package key's value is a Jinja2 template string that will use the project key's value to create a snake_case package name by converting the input string to lowercase and replacing spaces with underscores.\n",
    "\n",
    "Inside the double curly braces ({{}}) of the Jinja2 template string, we can use any Python code necessary to create the desired final value.\n",
    "\n",
    "The license key's value is a list of possible license types:\n",
    "\n",
    "- MIT\n",
    "- BSD\n",
    "- GPL3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T06:32:04.283226Z",
     "start_time": "2020-04-28T06:32:04.279218Z"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path.write_text(json.dumps({\n",
    "    \"project\": \"Creating Robust Python Workflows\",\n",
    "  \t# Convert the project name into snake_case\n",
    "    \"package\": \"{{ cookiecutter.project.lower().replace(' ', '_') }}\",\n",
    "    # Fill in the default license value\n",
    "    \"license\": [\"MIT\", \"BSD\", \"GPL3\"]\n",
    "}))\n",
    "\n",
    "pprint(json.loads(json_path.read_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create project\n",
    "In this project template exercise, we will first list the keys in a local cookiecutter.json file.\n",
    "\n",
    "The paths to the template directory and its cookiecutter.json are assigned to template_root and json_path variables, respectively.\n",
    "\n",
    "While template_root is a string, json_path is a pathlib.Path object.\n",
    "\n",
    "We will use the json module to obtain cookiecutter.json file contents as a Python dictionary and unpack this dictionary into a list to see its keys.\n",
    "\n",
    "We need to see the keys in the cookiecutter.json file to know how override the default project name in the template, because the key in the extra_context argument passed to the cookiecutter() function must match the corresponding key in cookiecutter.json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain keys from the local template's cookiecutter.json\n",
    "keys = [*json.load(json_path.open())]\n",
    "vals = \"Your name here\", \"My Amazing Python Project\"\n",
    "\n",
    "# Create a cookiecutter project without prompting for input\n",
    "main.cookiecutter(template_root.as_posix(), no_input=True,\n",
    "                  extra_context=dict(zip(keys, vals)))\n",
    "\n",
    "for path in pathlib.Path.cwd().glob(\"**\"):\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executable projects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipapp\n",
    "In this exercise, we will\n",
    "\n",
    "zip up a project called myproject\n",
    "make the zipped project command-line executable\n",
    "create a __main__.py file in the zipped project\n",
    "all with a single call to the create_archive() function from the standard library zipapp module.\n",
    "\n",
    "The python interpreter we want to use is /usr/bin/env python,\n",
    "\n",
    "while the function we want __main__.py to run is called print_name_and_file():\n",
    "\n",
    "def print_name_and_file():\n",
    "    print(f\"Name is {__name__}. File is {__file__}.\")\n",
    "The print_name_and_file() function is in the mymodule.py file inside the top-level mypackage directory, as shown below:\n",
    "\n",
    "myproject\n",
    "└── mypackage\n",
    "    ├── __init__.py\n",
    "    └── mymodule.pyµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipapp.create_archive(\n",
    "    # Zip up a project called \"myproject\"\n",
    "    \"myproject\",                    \n",
    "    interpreter=\"/usr/bin/env python\",\n",
    "    # Generate a __main__.py file\n",
    "    main=\"mypackage.mymodule:print_name_and_file\")\n",
    "\n",
    "print(subprocess.run([\".venv/bin/python\", \"myproject.pyz\"],\n",
    "                     stdout=-1).stdout.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Argparse main()\n",
    "Next, we'll create a __main__.py file to pass shell arguments to classify() or regress(), functions based on code we wrote in Chapter 1.\n",
    "\n",
    "We will provide default values for all arguments, so that the code can run even if no shell arguments are provided.\n",
    "\n",
    "To do this, we'll instantiate the ArgumentParser class from the argparse module as parser and use its add_argument() method to create arguments called dataset and model with the following defaults: diabetes and linear_model.Ridge.\n",
    "\n",
    "Setting nargs to ? means that each argument can accept either one value or none at all.\n",
    "\n",
    "We will create a keyword arguments (kwargs) variable and unpack kwargs into the classify() or regress() functions in the main() function's return statement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Scikit datasets only!\")\n",
    "    # Set the default for the dataset argument\n",
    "    parser.add_argument(\"dataset\", nargs=\"?\", default=\"diabetes\")\n",
    "    parser.add_argument(\"model\", nargs=\"?\", default=\"linear_model.Ridge\")\n",
    "    args = parser.parse_args()\n",
    "    # Create a dictionary of the shell arguments\n",
    "    kwargs = dict(dataset=args.dataset, model=args.model)\n",
    "    return (classify(**kwargs) if args.dataset in (\"digits\", \"iris\", \"wine\")\n",
    "            else regress(**kwargs) if args.dataset in (\"boston\", \"diabetes\")\n",
    "            else print(f\"{args.dataset} is not a supported dataset!\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook workflows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrize notebooks\n",
    "To practice notebook parametrization, we will work with a Jupyter notebook called sklearn.ipynb.\n",
    "\n",
    "This notebook can run any scikit-learn model on any built-in scikit-learn dataset.\n",
    "\n",
    "The dataset and model that the notebook will use depend on the four parameters it receives.\n",
    "\n",
    "To find the parameter names, we will use papermill to look at the source attribute of an nbformat NotebookNode object cell.\n",
    "\n",
    "We will need the parameter names to create a dictionary of parameters that we will then use to execute the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the notebook to find the default parameter names\n",
    "pprint(nbformat.read(\"sklearn.ipynb\", as_version=4).cells[0].source)\n",
    "keys = [\"dataset_name\", \"model_type\", \"model_name\", \"hyperparameters\"]\n",
    "vals = [\"diabetes\", \"ensemble\", \"RandomForestRegressor\",\n",
    "        dict(max_depth=3, n_estimators=100, random_state=0)]\n",
    "parameter_dictionary = dict(zip(keys, vals))\n",
    "\n",
    "# Execute the notebook with custom parameters\n",
    "pprint(pm.execute_notebook(\n",
    "    \"sklearn.ipynb\", \"rf_diabetes.ipynb\", \n",
    "    kernel_name=\"python3\", parameters=parameter_dictionary\n",
    "\t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize notebooks\n",
    "In the last notebook workflow exercise, we will use scrapbook to\n",
    "\n",
    "read in a Jupyter notebook called rf_diabetes.ipynb\n",
    "create a dataframe that contains variables that were saved in the notebook with the glue() function\n",
    "create a second dataframe of parameters that were passed to the notebook by papermill\n",
    "This exercise demonstrates how we can use scrapbook to access notebook data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapbook as sb\n",
    "\n",
    "# Read in the notebook and assign the notebook object to nb\n",
    "nb = sb.read_notebook(\"rf_diabetes.ipynb\")\n",
    "\n",
    "# Create a dataframe of scraps (recorded values)\n",
    "scrap_df = nb.scrap_dataframe\n",
    "print(scrap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel computing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask dataframe\n",
    "To practice working with Dask dataframes, we will\n",
    "\n",
    "- read in a .csv file containing the diabetes dataset as Dask dataframe,\n",
    "- create a new binary variable from the age column, and\n",
    "- compute the means of all variables for the resulting two age groups.\n",
    "\n",
    "The code in this exercise could easily be adapted to work with a Pandas dataframe instead of a Dask dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T13:27:48.652555Z",
     "start_time": "2020-04-29T13:27:47.201522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargements à lancer\n",
      "{'numpy.ndarray': {'diabetes.csv': 'https://file.io/scSnKGFJ'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 86290    0 86290    0     0  94928      0 --:--:-- --:--:-- --:--:-- 94824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "##### file\n",
    "###################\n",
    "\n",
    "#upload and download\n",
    "\n",
    "from downloadfromFileIO import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO_pushto_fileio('diabetes.csv')\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{numpy.ndarray: {'diabetes.csv': 'https://file.io/scSnKGFJ'}}\n",
    "\"\"\"\n",
    "prefixToc = '4.1'\n",
    "prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-29T13:29:03.138599Z",
     "start_time": "2020-04-29T13:29:03.049330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              age       sex       bmi       map        tc       ldl       hdl  \\\n",
      "bin_age                                                                         \n",
      "0       -0.042991 -0.008306 -0.006153 -0.014011 -0.008585 -0.007124  0.002693   \n",
      "1        0.036184  0.006991  0.005178  0.011792  0.007226  0.005996 -0.002266   \n",
      "\n",
      "              tch       ltg       glu  \n",
      "bin_age                                \n",
      "0       -0.006677 -0.009723 -0.011084  \n",
      "1        0.005620  0.008184  0.009329  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\site-packages\\dask\\dataframe\\core.py:5916: UserWarning: Insufficient elements for `head`. 5 elements requested, only 2 elements available. Try passing larger `npartitions` to `head`.\n",
      "  Dask.DataFrame objects are partitioned along their index.  Often when\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Read in a csv file using a dask.dataframe method\n",
    "df = dd.read_csv(prefix+\"diabetes.csv\")\n",
    "\n",
    "df[\"bin_age\"] = (df.age > 0).astype(int)\n",
    "\n",
    "# Compute the columns means in the two age groups\n",
    "print(df.groupby(\"bin_age\").mean().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joblib\n",
    "In the last exercise of this course, we will use the grid search technique to find the optimal hyperparameters for an elastic net model.\n",
    "\n",
    "Grid search is computationally intensive. To speed up the search, we will use the joblib parallel_backend() function.\n",
    "\n",
    "The scikit-learn GridSearchCV class has already been instantiated as engrid with a grid of two hyperparameters:\n",
    "\n",
    "l1_ratio: the mix of Lasso (L1) and Ridge (L2) regression penalties used to shrink model coefficients\n",
    "alpha: the severity of the penalty\n",
    "Applying penalties to model coefficients helps to avoid overfitting and produce models that perform better on new data.\n",
    "\n",
    "We will use the optimal l1_ratio to create a enet_path() plot that shows how coefficients shrink as alpha increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Dask client with 4 threads and 1 worker\n",
    "Client(processes=False, threads_per_worker=4, n_workers=1)\n",
    "\n",
    "# Run grid search using joblib and a Dask backend\n",
    "with joblib.parallel_backend(\"dask\"):\n",
    "    engrid.fit(x_train, y_train)\n",
    "\n",
    "plot_enet(*enet_path(x_test, y_test, eps=5e-5, fit_intercept=False,\n",
    "                    l1_ratio=engrid.best_params_[\"l1_ratio\"])[:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datacamp] *",
   "language": "python",
   "name": "conda-env-datacamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
