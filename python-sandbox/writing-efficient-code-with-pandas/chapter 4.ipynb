{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation using .groupby().transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The min-max normalization using .transform()\n",
    "A very common operation is the min-max normalization. It consists in rescaling our value of interest by deducting the minimum value and dividing the result by the difference between the maximum and the minimum value. For example, to rescale student's weight data spanning from 160 pounds to 200 pounds, you subtract 160 from each student's weight and divide the result by 40 (200 - 160).\n",
    "\n",
    "You're going to define and apply the min-max normalization to all the numerical variables in the restaurant data. You will first group the entries by the time the meal took place (Lunch or Dinner) and then apply the normalization to each group separately.\n",
    "\n",
    "Remember you can always explore the dataset and see how it changes in the IPython Shell, and refer to the slides in the Slides tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:12:33.349961Z",
     "start_time": "2020-06-16T08:12:31.807373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargements à lancer\n",
      "{'pandas.core.frame.DataFrame': {'restaurant_data.csv': 'https://file.io/aQNaDHlV'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  8810    0  8810    0     0  14707      0 --:--:-- --:--:-- --:--:-- 14707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "##### Dataframe\n",
    "###################\n",
    "\n",
    "#upload and download\n",
    "\n",
    "from downloadfromFileIO import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(restaurant_data)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.frame.DataFrame: {'restaurant_data.csv': 'https://file.io/aQNaDHlV'}}\n",
    "\"\"\"\n",
    "prefixToc='1.1'\n",
    "prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=\"\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "restaurant_data = pd.read_csv(prefix+'restaurant_data.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:13:54.255005Z",
     "start_time": "2020-06-16T08:13:54.213732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_bill       tip  size\n",
      "0    0.291579  0.001111   0.2\n",
      "1    0.152283  0.073333   0.4\n",
      "2    0.375786  0.277778   0.4\n",
      "3    0.431713  0.256667   0.2\n",
      "4    0.450775  0.290000   0.6\n"
     ]
    }
   ],
   "source": [
    "# Define the min-max transformation\n",
    "min_max_tr = lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "# Group the data according to the time\n",
    "restaurant_grouped = restaurant_data.groupby('time')\n",
    "\n",
    "# Apply the transformation\n",
    "restaurant_min_max_group = restaurant_grouped.transform(min_max_tr)\n",
    "print(restaurant_min_max_group.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming values to probabilities\n",
    "In this exercise, we will apply a probability distribution function to a pandas DataFrame with group related parameters by transforming the tip variable to probabilities.\n",
    "\n",
    "The transformation will be a exponential transformation. The exponential distribution is defined as\n",
    "\n",
    "e−λ∗x∗λ\n",
    "where λ (lambda) is the mean of the group that the observation x belongs to.\n",
    "\n",
    "You're going to apply the exponential distribution transformation to the size of each table in the dataset, after grouping the data according to the time of the day the meal took place. Remember to use each group's mean for the value of λ.\n",
    "\n",
    "In Python, you can use the exponential as np.exp() from the NumPy library and the mean value as .mean()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:15:13.914101Z",
     "start_time": "2020-06-16T08:15:13.911093Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:16:05.760196Z",
     "start_time": "2020-06-16T08:16:05.748164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.135141\n",
      "1    0.017986\n",
      "2    0.000060\n",
      "3    0.000108\n",
      "4    0.000042\n",
      "Name: tip, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Define the exponential transformation\n",
    "exp_tr = lambda x: np.exp(-x.mean()*x) * x.mean()\n",
    "\n",
    "# Group the data according to the time\n",
    "restaurant_grouped = restaurant_data.groupby('time')\n",
    "\n",
    "# Apply the transformation\n",
    "restaurant_exp_group = restaurant_grouped['tip'].transform(exp_tr)\n",
    "print(restaurant_exp_group.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of normalization\n",
    "For this exercise, we will perform a z-score normalization and verify that it was performed correctly.\n",
    "\n",
    "A distinct characteristic of normalized values is that they have a mean equal to zero and standard deviation equal to one.\n",
    "\n",
    "After you apply the normalization transformation, you can group again on the same variable, and then check the mean and the standard deviation of each group.\n",
    "\n",
    "You will apply the normalization transformation to every numeric variable in the poker_grouped dataset, which is the poker_hands dataset grouped by Class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:17:34.026316Z",
     "start_time": "2020-06-16T08:17:32.717311Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargements à lancer\n",
      "{'pandas.core.frame.DataFrame': {'poker_hands.csv': 'https://file.io/2ruAxPwk'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  710k    0  710k    0     0   633k      0 --:--:--  0:00:01 --:--:--  633k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "##### Dataframe\n",
    "###################\n",
    "\n",
    "#upload and download\n",
    "\n",
    "from downloadfromFileIO import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(poker_hands)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.frame.DataFrame: {'poker_hands.csv': 'https://file.io/2ruAxPwk'}}\n",
    "\"\"\"\n",
    "prefixToc='1.3'\n",
    "prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=\"\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "poker_hands = pd.read_csv(prefix+'poker_hands.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:18:49.063672Z",
     "start_time": "2020-06-16T08:18:49.060715Z"
    }
   },
   "outputs": [],
   "source": [
    "poker_grouped = poker_hands.groupby('Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:18:53.764010Z",
     "start_time": "2020-06-16T08:18:53.664399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         S1        R1        S2        R2        S3        R3        S4  \\\n",
      "0 -1.380537  0.270364 -1.380537 -0.730297 -1.380537  0.631224 -1.380537   \n",
      "1 -0.613572  0.495666 -0.613572  1.095445 -0.613572  0.039451 -0.613572   \n",
      "2  0.153393  0.720969  0.153393 -0.730297  0.153393  0.631224  0.153393   \n",
      "3  0.920358  0.270364  0.920358 -0.730297  0.920358 -1.735866  0.920358   \n",
      "4  0.920358 -1.757363  0.920358  1.095445  0.920358  0.433966  0.920358   \n",
      "\n",
      "         R4        S5        R5  \n",
      "0  0.350823 -1.380537 -0.724286  \n",
      "1  0.350823 -0.613572 -0.724286  \n",
      "2 -1.403293  0.153393 -0.724286  \n",
      "3  1.227881  0.920358  1.267500  \n",
      "4 -0.526235  0.920358  0.905357  \n"
     ]
    }
   ],
   "source": [
    "zscore = lambda x: (x - x.mean()) / x.std()\n",
    "\n",
    "# Apply the transformation\n",
    "poker_trans = poker_grouped.transform(zscore)\n",
    "print(poker_trans.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:21:26.491078Z",
     "start_time": "2020-06-16T08:21:26.466985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        S1   R1   S2   R2   S3   R3   S4   R4   S5   R5\n",
      "Class                                                  \n",
      "0     -0.0 -0.0  0.0 -0.0  0.0  0.0  0.0  0.0 -0.0  0.0\n",
      "1      0.0  0.0 -0.0  0.0 -0.0  0.0  0.0  0.0 -0.0  0.0\n",
      "2     -0.0 -0.0  0.0 -0.0 -0.0  0.0  0.0 -0.0 -0.0  0.0\n",
      "3      0.0  0.0  0.0 -0.0 -0.0 -0.0 -0.0 -0.0  0.0 -0.0\n",
      "4     -0.0 -0.0 -0.0 -0.0  0.0 -0.0 -0.0  0.0  0.0  0.0\n",
      "5     -0.0 -0.0 -0.0  0.0 -0.0  0.0 -0.0 -0.0 -0.0  0.0\n",
      "6     -0.0 -0.0 -0.0  0.0  0.0 -0.0  0.0  0.0 -0.0  0.0\n",
      "7      0.0 -0.0 -0.0  0.0 -0.0  0.0  0.0 -0.0 -0.0 -0.0\n",
      "8     -0.0  0.0 -0.0  0.0 -0.0  0.0 -0.0  0.0 -0.0 -0.0\n",
      "9      0.0 -0.0  0.0 -0.0  0.0 -0.0  0.0  0.0  0.0 -0.0\n",
      "        S1   R1   S2   R2   S3   R3   S4   R4   S5   R5\n",
      "Class                                                  \n",
      "0      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
      "1      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
      "2      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
      "3      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
      "4      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
      "5      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
      "6      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
      "7      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
      "8      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
      "9      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "# Re-group the grouped object and print each group's means and standard deviation\n",
    "poker_regrouped = poker_trans.groupby(poker_hands['Class'])\n",
    "\n",
    "print(np.round(poker_regrouped.mean(), 3))\n",
    "print(poker_regrouped.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing value imputation using transform()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying missing values\n",
    "The first step before missing value imputation is to identify if there are missing values in our data, and if so, from which group they arise.\n",
    "\n",
    "For the same restaurant_data data you encountered in the lesson, an employee erased by mistake the tips left in 65 tables. The question at stake is how many missing entries came from tables that smokers where present vs tables with no-smokers present.\n",
    "\n",
    "Your task is to group both datasets according to the smoker variable, count the number or present values and then calculate the difference.\n",
    "\n",
    "We're imputing tips to get you to practice the concepts taught in the lesson. From an ethical standpoint, you should not impute financial data in real life, as it could be considered fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:28:31.927233Z",
     "start_time": "2020-06-16T08:28:30.505638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargements à lancer\n",
      "{'pandas.core.frame.DataFrame': {'restaurant_data.csv': 'https://file.io/JxreuBrl', 'restaurant_nan.csv': 'https://file.io/dudKDy2a'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  8810    0  8810    0     0  14466      0 --:--:-- --:--:-- --:--:-- 14442\n",
      "\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  8587    0  8587    0     0  16706      0 --:--:-- --:--:-- --:--:-- 16706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "##### Dataframe\n",
    "###################\n",
    "\n",
    "#upload and download\n",
    "\n",
    "from downloadfromFileIO import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(restaurant_data, restaurant_nan)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.frame.DataFrame: {'restaurant_data.csv': 'https://file.io/JxreuBrl',\n",
    "  'restaurant_nan.csv': 'https://file.io/dudKDy2a'}}\n",
    "\"\"\"\n",
    "prefixToc='2.1'\n",
    "prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=\"\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "restaurant_data = pd.read_csv(prefix+'restaurant_data.csv',index_col=0)\n",
    "restaurant_nan = pd.read_csv(prefix+'restaurant_nan.csv',index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T08:30:21.191545Z",
     "start_time": "2020-06-16T08:30:21.184510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoker\n",
      "No     41\n",
      "Yes    24\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Group both objects according to smoke condition\n",
    "restaurant_nan_grouped = restaurant_nan.groupby('smoker')\n",
    "\n",
    "# Store the number of present values\n",
    "restaurant_nan_nval = restaurant_nan_grouped['tip'].count()\n",
    "\n",
    "# Print the group-wise missing entries\n",
    "print(restaurant_nan_grouped['total_bill'].count() - restaurant_nan_nval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing value imputation\n",
    "As the majority of the real world data contain missing entries, replacing these entries with sensible values can increase the insight you can get from our data.\n",
    "\n",
    "In the restaurant dataset, the \"total_bill\" column has some missing entries, meaning that you have not recorded how much some tables have paid. Your task in this exercise is to replace the missing entries with the median value of the amount paid, according to whether the entry was recorded on lunch or dinner (time variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T09:21:49.322581Z",
     "start_time": "2020-06-16T09:21:49.318570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the lambda function\n",
    "missing_trans = lambda x: x.fillna(x.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T09:22:18.461705Z",
     "start_time": "2020-06-16T09:22:18.439341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   total_bill   tip  size\n",
      "0       16.99  1.01     2\n",
      "1       10.34  1.66     3\n",
      "2       21.01  3.50     3\n",
      "3       23.68  3.31     2\n",
      "4       24.59  3.61     4\n"
     ]
    }
   ],
   "source": [
    "# Group the data according to time\n",
    "restaurant_grouped = restaurant_data.groupby('time')\n",
    "\n",
    "# Apply the transformation\n",
    "restaurant_impute = restaurant_grouped.transform(missing_trans)\n",
    "print(restaurant_impute.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data filtration using the filter() function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filtration\n",
    "As you noticed in the video lesson, you may need to filter your data for various reasons.\n",
    "\n",
    "In this exercise, you will use filtering to select a specific part of our DataFrame:\n",
    "\n",
    "by the number of entries recorded in each day of the week\n",
    "by the mean amount of money the customers paid to the restaurant each day of the week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T09:28:06.399896Z",
     "start_time": "2020-06-16T09:28:06.390838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables where total_bill is greater than $40: 225\n"
     ]
    }
   ],
   "source": [
    "# Filter the days where the count of total_bill is greater than $40\n",
    "total_bill_40 = restaurant_data.groupby('day').filter(lambda x: x['total_bill'].count() > 40)\n",
    "\n",
    "# Print the number of tables where total_bill is greater than $40\n",
    "print('Number of tables where total_bill is greater than $40:', total_bill_40.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T09:29:01.395029Z",
     "start_time": "2020-06-16T09:29:01.386006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days of the week that have a mean total_bill greater than $20: ['Sun' 'Sat']\n"
     ]
    }
   ],
   "source": [
    "# Select only the entries that have a mean total_bill greater than $20\n",
    "total_bill_20 = total_bill_40.groupby('day').filter(lambda x : x['total_bill'].mean() > 20)\n",
    "\n",
    "# Print days of the week that have a mean total_bill greater than $20\n",
    "print('Days of the week that have a mean total_bill greater than $20:', total_bill_20.day.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datacamp] *",
   "language": "python",
   "name": "conda-env-datacamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
