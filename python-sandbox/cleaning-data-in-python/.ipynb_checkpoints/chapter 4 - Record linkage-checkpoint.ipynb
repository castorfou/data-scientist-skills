{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing strings\n",
    "\n",
    "```python\n",
    "# Simple string comparison\n",
    "# Lets us compare between two strings\n",
    "from fuzzywuzzy import fuzz\n",
    "# Compare reeding vs reading\n",
    "fuzz.WRatio('Reeding', 'Reading') #86\n",
    "\n",
    "\n",
    "# Comparison with arrays\n",
    "# Import process\n",
    "from fuzzywuzzy import process\n",
    "# Define string and array of possible matches\n",
    "string = \"Houston Rockets vs Los Angeles Lakers\"\n",
    "choices = pd.Series(['Rockets vs Lakers', 'Lakers vs Rockets',\n",
    "'Houson vs Los Angeles', 'Heat vs Bulls'])\n",
    "process.extract(string, choices, limit = 2)\n",
    "[('Rockets vs Lakers', 86, 0), ('Lakers vs Rockets', 86, 1)]\n",
    "\n",
    "# Collapsing all of the state\n",
    "# For each correct category\n",
    "for state in categories['state']:\n",
    "    # Find potential matches in states with typoes\n",
    "    matches = process.extract(state, survey['state'], limit = survey.shape[0])\n",
    "    # For each potential match match\n",
    "    for potential_match in matches:\n",
    "        # If high similarity score\n",
    "        if potential_match[1] >= 80:\n",
    "            # Replace typo with correct category\n",
    "            survey.loc[survey['state'] == potential_match[0], 'state'] = state\n",
    "\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cutoff point\n",
    "> \n",
    "> In this exercise, and throughout this chapter, you'll be working with the `restaurants` DataFrame which has data on various restaurants. Your ultimate goal is to create a restaurant recommendation engine, but you need to first clean your data.\n",
    "> \n",
    "> This version of `restaurants` has been collected from many sources, where the `cuisine_type` column is riddled with typos, and should contain only `italian`, `american` and `asian` cuisine types. There are so many unique categories that remapping them manually isn't scalable, and it's best to use string similarity instead.\n",
    "> \n",
    "> Before doing so, you want to establish the cutoff point for the similarity score using the `fuzzywuzzy`'s `process.extract()` function by finding the similarity score of the most _distant_ typo of each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T07:27:26.018528Z",
     "start_time": "2021-04-30T07:27:24.987216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargements à lancer\n",
      "{'pandas.core.frame.DataFrame': {'restaurants.csv': 'https://file.io/0DYxl1uuHNU5'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 21018    0 21018    0     0  27986      0 --:--:-- --:--:-- --:--:-- 27949\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "##### Dataframe\n",
    "###################\n",
    "\n",
    "#upload and download\n",
    "\n",
    "from downloadfromFileIO import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(restaurants)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.frame.DataFrame: {'restaurants.csv': 'https://file.io/0DYxl1uuHNU5'}}\n",
    "\"\"\"\n",
    "prefixToc='1.1'\n",
    "prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc)\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "restaurants = pd.read_csv(prefix+'restaurants.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The cutoff point | Python](https://campus.datacamp.com/courses/cleaning-data-in-python/record-linkage-4?ex=3)\n",
    "\n",
    "> -   Import `process` from `fuzzywuzzy`.\n",
    "> -   Store the unique `cuisine_type`s into `unique_types`.\n",
    "> -   Calculate the similarity of `'asian'`, `'american'`, and `'italian'` to all possible `cuisine_type`s using `process.extract()`, while returning all possible matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T07:32:12.140834Z",
     "start_time": "2021-04-30T07:32:12.135060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asian', 100), ('asiane', 91), ('asiann', 91), ('asiian', 91), ('asiaan', 91), ('asianne', 83), ('asiat', 80), ('italiann', 72), ('italiano', 72), ('italianne', 72), ('italian', 67), ('amurican', 62), ('american', 62), ('italiaan', 62), ('italiian', 62), ('itallian', 62), ('americann', 57), ('americano', 57), ('ameerican', 57), ('aamerican', 57), ('ameriican', 57), ('amerrican', 57), ('ammericann', 54), ('ameerrican', 54), ('ammereican', 54), ('america', 50), ('merican', 50), ('murican', 50), ('italien', 50), ('americen', 46), ('americin', 46), ('amerycan', 46), ('itali', 40)]\n",
      "[('american', 100), ('americann', 94), ('americano', 94), ('ameerican', 94), ('aamerican', 94), ('ameriican', 94), ('amerrican', 94), ('america', 93), ('merican', 93), ('ammericann', 89), ('ameerrican', 89), ('ammereican', 89), ('amurican', 88), ('americen', 88), ('americin', 88), ('amerycan', 88), ('murican', 80), ('asian', 62), ('asiane', 57), ('asiann', 57), ('asiian', 57), ('asiaan', 57), ('italian', 53), ('asianne', 53), ('italiann', 50), ('italiano', 50), ('italiaan', 50), ('italiian', 50), ('itallian', 50), ('italianne', 47), ('asiat', 46), ('itali', 40), ('italien', 40)]\n",
      "[('italian', 100), ('italiann', 93), ('italiano', 93), ('italiaan', 93), ('italiian', 93), ('itallian', 93), ('italianne', 88), ('italien', 86), ('itali', 83), ('asian', 67), ('asiane', 62), ('asiann', 62), ('asiian', 62), ('asiaan', 62), ('asianne', 57), ('amurican', 53), ('american', 53), ('americann', 50), ('asiat', 50), ('americano', 50), ('ameerican', 50), ('aamerican', 50), ('ameriican', 50), ('amerrican', 50), ('ammericann', 47), ('ameerrican', 47), ('ammereican', 47), ('america', 43), ('merican', 43), ('murican', 43), ('americen', 40), ('americin', 40), ('amerycan', 40)]\n"
     ]
    }
   ],
   "source": [
    "# Import process from fuzzywuzzy\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Store the unique values of cuisine_type in unique_types\n",
    "unique_types = restaurants['cuisine_type'].unique()\n",
    "\n",
    "# Calculate similarity of 'asian' to all values of unique_types\n",
    "print(process.extract('asian', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'american' to all values of unique_types\n",
    "print(process.extract('american', unique_types, limit = len(unique_types)))\n",
    "\n",
    "# Calculate similarity of 'italian' to all values of unique_types\n",
    "print(process.extract('italian', unique_types, limit = len(unique_types)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remapping categories II\n",
    "> \n",
    "> In the last exercise, you determined that the distance cutoff point for remapping typos of `'american'`, `'asian'`, and `'italian'` cuisine types stored in the `cuisine_type` column should be 80.\n",
    "> \n",
    "> In this exercise, you're going to put it all together by finding matches with similarity scores equal to or higher than 80 by using `fuzywuzzy.process`'s `extract()` function, for each correct cuisine type, and replacing these matches with it. Remember, when comparing a string with an array of strings using `process.extract()`, the output is a list of tuples where each is formatted like:\n",
    "> \n",
    ">     (closest match, similarity score, index of match)\n",
    ">     \n",
    "> \n",
    "> The `restaurants` DataFrame is in your environment, and you have access to a `categories` list containing the correct cuisine types (`'italian'`, `'asian'`, and `'american'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T07:36:12.631930Z",
     "start_time": "2021-04-30T07:36:12.624217Z"
    }
   },
   "outputs": [],
   "source": [
    "categories = ['italian', 'asian', 'american']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T07:36:20.363800Z",
     "start_time": "2021-04-30T07:36:20.359241Z"
    }
   },
   "source": [
    "[Remapping categories II | Python](https://campus.datacamp.com/courses/cleaning-data-in-python/record-linkage-4?ex=4)\n",
    "\n",
    "> Return all of the unique values in the `cuisine_type` column of `restaurants`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T07:36:45.835098Z",
     "start_time": "2021-04-30T07:36:45.824750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['america' 'merican' 'amurican' 'americen' 'americann' 'asiane' 'itali'\n",
      " 'asiann' 'murican' 'italien' 'italian' 'asiat' 'american' 'americano'\n",
      " 'italiann' 'ameerican' 'asianne' 'italiano' 'americin' 'ammericann'\n",
      " 'amerycan' 'aamerican' 'ameriican' 'italiaan' 'asiian' 'asiaan'\n",
      " 'amerrican' 'ameerrican' 'ammereican' 'asian' 'italianne' 'italiian'\n",
      " 'itallian']\n"
     ]
    }
   ],
   "source": [
    "# Inspect the unique values of the cuisine_type column\n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Remapping categories II | Python](https://campus.datacamp.com/courses/cleaning-data-in-python/record-linkage-4?ex=4)\n",
    "\n",
    "> Okay! Looks like you will need to use some string matching to correct these misspellings!\n",
    "> \n",
    "> -   As a first step, create a list of `matches`, comparing `'italian'` with the restaurant types listed in the `cuisine_type` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T07:39:38.539381Z",
     "start_time": "2021-04-30T07:39:38.529638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('italian', 100, 11), ('italian', 100, 25), ('italian', 100, 41), ('italian', 100, 47), ('italian', 100, 49)]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit = len(restaurants['cuisine_type']))\n",
    "\n",
    "# Inspect the first 5 matches\n",
    "print(matches[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Remapping categories II | Python](https://campus.datacamp.com/courses/cleaning-data-in-python/record-linkage-4?ex=4)\n",
    "\n",
    "> Now you're getting somewhere! Now you can iterate through `matches` to reassign similar entries.\n",
    "> \n",
    "> -   Within the `for` loop, use an `if` statement to check whether the similarity score in each `match` is greater than or equal to 80.\n",
    "> -   If it is, use `.loc` to select rows where `cuisine_type` in `restaurants` is _equal_ to the current match (which is the first element of `match`), and reassign them to be `'italian'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T07:44:44.879891Z",
     "start_time": "2021-04-30T07:44:44.845480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of matches, comparing 'italian' with the cuisine_type column\n",
    "matches = process.extract('italian', restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "# Iterate through the list of matches to italian\n",
    "for match in matches:\n",
    "  # Check whether the similarity score is greater than or equal to 80\n",
    "  if (match[1] >= 80):\n",
    "    # Select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "    restaurants.loc[restaurants['cuisine_type'] == match[0], 'cuisine_type'] = 'italian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Remapping categories II | Python](https://campus.datacamp.com/courses/cleaning-data-in-python/record-linkage-4?ex=4)\n",
    "\n",
    "> Finally, you'll adapt your code to work with every restaurant type in `categories`.\n",
    "> \n",
    "> -   Using the variable `cuisine` to iterate through `categories`, embed your code from the previous step in an outer `for` loop.\n",
    "> -   Inspect the final result. _This has been done for you._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T07:45:39.017727Z",
     "start_time": "2021-04-30T07:45:38.917576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['american' 'asian' 'italian']\n"
     ]
    }
   ],
   "source": [
    "# Iterate through categories\n",
    "for cuisine in categories:  \n",
    "  # Create a list of matches, comparing cuisine with the cuisine_type column\n",
    "  matches = process.extract(cuisine, restaurants['cuisine_type'], limit=len(restaurants.cuisine_type))\n",
    "\n",
    "  # Iterate through the list of matches\n",
    "  for match in matches:\n",
    "     # Check whether the similarity score is greater than or equal to 80\n",
    "    if match[1] >= 80:\n",
    "      # If it is, select all rows where the cuisine_type is spelled this way, and set them to the correct cuisine\n",
    "      restaurants.loc[restaurants['cuisine_type'] == match[0]] = cuisine\n",
    "      \n",
    "# Inspect the final result\n",
    "print(restaurants['cuisine_type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating pairs\n",
    "\n",
    "```python\n",
    "\n",
    "# Generating pairs\n",
    "# Import recordlinkage\n",
    "import recordlinkage\n",
    "# Create indexing object\n",
    "indexer = recordlinkage.Index()\n",
    "# Generate pairs blocked on state\n",
    "indexer.block('state')\n",
    "pairs = indexer.index(census_A, census_B)\n",
    "\n",
    "# Comparing the DataFrames\n",
    "# Generate the pairs\n",
    "pairs = indexer.index(census_A, census_B)\n",
    "# Create a Compare object\n",
    "compare_cl = recordlinkage.Compare()\n",
    "# Find exact matches for pairs of date_of_birth and state\n",
    "compare_cl.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
    "compare_cl.exact('state', 'state', label='state')\n",
    "# Find similar matches for pairs of surname and address_1 using string similarity\n",
    "compare_cl.string('surname', 'surname', threshold=0.85, label='surname')\n",
    "compare_cl.string('address_1', 'address_1', threshold=0.85, label='address_1')\n",
    "# Find matches\n",
    "potential_matches = compare_cl.compute(pairs, census_A, census_B)\n",
    "\n",
    "\n",
    "# Finding the only pairs we want\n",
    "potential_matches[potential_matches.sum(axis = 1) => 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairs of restaurants\n",
    "> \n",
    "> In the last lesson, you cleaned the `restaurants` dataset to make it ready for building a restaurants recommendation engine. You have a new DataFrame named `restaurants_new` with new restaurants to train your model on, that's been scraped from a new data source.\n",
    "> \n",
    "> You've already cleaned the `cuisine_type` and `city` columns using the techniques learned throughout the course. However you saw duplicates with typos in restaurants names that require record linkage instead of joins with `restaurants`.\n",
    "> \n",
    "> In this exercise, you will perform the first step in record linkage and generate possible pairs of rows between `restaurants` and `restaurants_new`. Both DataFrames, `pandas` and `recordlinkage` are in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T07:56:30.829492Z",
     "start_time": "2021-04-30T07:56:30.409996Z"
    }
   },
   "outputs": [],
   "source": [
    "import recordlinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T07:56:31.597625Z",
     "start_time": "2021-04-30T07:56:31.583660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargements déjà effectués - SKIP\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "##### Dataframe\n",
    "###################\n",
    "\n",
    "#upload and download\n",
    "\n",
    "from downloadfromFileIO import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(restaurants, restaurants_new)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.frame.DataFrame: {'restaurants.csv': 'https://file.io/qQPzMnqha5Pv',\n",
    "  'restaurants_new.csv': 'https://file.io/5Z6r6lR8n4G6'}}\n",
    "  \"\"\"\n",
    "prefixToc='2.1'\n",
    "prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc)\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "restaurants = pd.read_csv(prefix+'restaurants.csv',index_col=0)\n",
    "restaurants_new = pd.read_csv(prefix+'restaurants_new.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pairs of restaurants | Python](https://campus.datacamp.com/courses/cleaning-data-in-python/record-linkage-4?ex=7)\n",
    "\n",
    "> -   Instantiate an indexing object by using the `Index()` function from `recordlinkage`.\n",
    "> -   Block your pairing on `cuisine_type` by using `indexer`'s' `.block()` method.\n",
    "> -   Generate pairs by indexing `restaurants` and `restaurants_new` in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T07:58:34.263666Z",
     "start_time": "2021-04-30T07:58:34.230334Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an indexer and object and find possible pairs\n",
    "indexer = recordlinkage.Index()\n",
    "\n",
    "# Block pairing on cuisine_type\n",
    "indexer.block('cuisine_type')\n",
    "\n",
    "# Generate pairs\n",
    "pairs = indexer.index(restaurants, restaurants_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar restaurants\n",
    "> \n",
    "> In the last exercise, you generated pairs between `restaurants` and `restaurants_new` in an effort to cleanly merge both DataFrames using record linkage.\n",
    "> \n",
    "> When performing record linkage, there are different types of matching you can perform between different columns of your DataFrames, including exact matches, string similarities, and more.\n",
    "> \n",
    "> Now that your pairs have been generated and stored in `pairs`, you will find exact matches in the `city` and `cuisine_type` columns between each pair, and similar strings for each pair in the `rest_name` column. Both DataFrames, `pandas` and `recordlinkage` are in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Similar restaurants | Python](https://campus.datacamp.com/courses/cleaning-data-in-python/record-linkage-4?ex=8)\n",
    "\n",
    "> Instantiate a comparison object using the `recordlinkage.Compare()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T08:00:17.324732Z",
     "start_time": "2021-04-30T08:00:17.318999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a comparison object\n",
    "comp_cl = recordlinkage.Compare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Similar restaurants | Python](https://campus.datacamp.com/courses/cleaning-data-in-python/record-linkage-4?ex=8)\n",
    "\n",
    "> -   Use the appropriate `comp_cl` method to find exact matches between the `city` and `cuisine_type` columns of both DataFrames.\n",
    "> -   Use the appropriate `comp_cl` method to find similar strings with a `0.8` similarity threshold in the `rest_name` column of both DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T08:03:02.365229Z",
     "start_time": "2021-04-30T08:03:02.358385Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compare>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find exact matches on city, cuisine_types \n",
    "comp_cl.exact('city', 'city', label='city')\n",
    "comp_cl.exact('cuisine_type', 'cuisine_type', label = 'cuisine_type')\n",
    "\n",
    "# Find similar matches of rest_name\n",
    "comp_cl.string('rest_name', 'rest_name', label='name', threshold = 0.8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Similar restaurants | Python](https://campus.datacamp.com/courses/cleaning-data-in-python/record-linkage-4?ex=8)\n",
    "\n",
    "> Compute the comparison of the pairs by using the `.compute()` method of `comp_cl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T08:04:01.757601Z",
     "start_time": "2021-04-30T08:04:01.693492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        city  cuisine_type  name\n",
      "0   0      0             1   0.0\n",
      "    1      0             1   0.0\n",
      "    7      0             1   0.0\n",
      "    12     0             1   0.0\n",
      "    13     0             1   0.0\n",
      "...      ...           ...   ...\n",
      "40  18     0             1   0.0\n",
      "281 18     0             1   0.0\n",
      "288 18     0             1   0.0\n",
      "302 18     0             1   0.0\n",
      "308 18     0             1   0.0\n",
      "\n",
      "[3631 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get potential matches and print\n",
    "potential_matches = comp_cl.compute(pairs, restaurants, restaurants_new)\n",
    "print(potential_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Similar restaurants | Python](https://campus.datacamp.com/courses/cleaning-data-in-python/record-linkage-4?ex=8)\n",
    "\n",
    "> Question\n",
    "> \n",
    "> Print out `potential_matches`, the columns are the columns being compared, with values being 1 for a match, and 0 for not a match for each pair of rows in your DataFrames. To find potential matches, you need to find rows with more than matching value in a column. You can find them with\n",
    "> \n",
    ">     potential_matches[potential_matches.sum(axis = 1) >= n]\n",
    ">     \n",
    "> \n",
    "> Where `n` is the minimum number of columns you want matching to ensure a proper duplicate find, what do you think should the value of `n` be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T08:05:52.834216Z",
     "start_time": "2021-04-30T08:05:52.822904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>cuisine_type</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>74</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>73</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <th>75</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city  cuisine_type  name\n",
       "0  40     1             1   1.0\n",
       "1  28     1             1   1.0\n",
       "2  74     1             1   1.0\n",
       "3  1      1             1   1.0\n",
       "4  53     1             1   1.0\n",
       "8  43     1             1   1.0\n",
       "9  50     1             1   1.0\n",
       "13 7      1             1   1.0\n",
       "14 67     1             1   1.0\n",
       "17 12     1             1   1.0\n",
       "20 20     1             1   1.0\n",
       "21 27     1             1   1.0\n",
       "5  65     1             1   1.0\n",
       "7  79     1             1   1.0\n",
       "12 26     1             1   1.0\n",
       "18 71     1             1   1.0\n",
       "6  73     1             1   1.0\n",
       "10 75     1             1   1.0\n",
       "11 21     1             1   1.0\n",
       "16 57     1             1   1.0\n",
       "19 47     1             1   1.0\n",
       "15 55     1             1   1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "potential_matches[potential_matches.sum(axis = 1) >= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking DataFrames\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "# Probable matches\n",
    "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
    "print(matches)\n",
    "\n",
    "# Get the indices\n",
    "matches.index\n",
    "MultiIndex(levels=[['rec-1007-org', 'rec-1016-org', 'rec-1054-org', 'rec-1066-org',\n",
    "'rec-1070-org', 'rec-1075-org', 'rec-1080-org', 'rec-110-org', ...\n",
    "# Get indices from census_B only\n",
    "duplicate_rows = matches.index.get_level_values(1)\n",
    "print(census_B_index)\n",
    "                    \n",
    "# Linking DataFrames\n",
    "# Finding duplicates in census_B\n",
    "census_B_duplicates = census_B[census_B.index.isin(duplicate_rows)]\n",
    "# Finding new rows in census_B\n",
    "census_B_new = census_B[~census_B.index.isin(duplicate_rows)]\n",
    "# Link the DataFrames!\n",
    "full_census = census_A.append(census_B_new)   \n",
    "                    \n",
    "\n",
    "# Recap\n",
    "# Import recordlinkage and generate pairs and compare across columns\n",
    "...\n",
    "# Generate potential matches\n",
    "potential_matches = compare_cl.compute(full_pairs, census_A, census_B)\n",
    "# Isolate matches with matching values for 3 or more columns\n",
    "matches = potential_matches[potential_matches.sum(axis = 1) >= 3]\n",
    "# Get index for matching census_B rows only\n",
    "duplicate_rows = matches.index.get_level_values(1)\n",
    "# Finding new rows in census_B\n",
    "census_B_new = census_B[~census_B.index.isin(duplicate_rows)]\n",
    "# Link the DataFrames!\n",
    "full_census = census_A.append(census_B_new)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking them together!\n",
    "> \n",
    "> In the last lesson, you've finished the bulk of the work on your effort to link `restaurants` and `restaurants_new`. You've generated the different pairs of potentially matching rows, searched for exact matches between the `cuisine_type` and `city` columns, but compared for similar strings in the `rest_name` column. You stored the DataFrame containing the scores in `potential_matches`.\n",
    "> \n",
    "> Now it's finally time to link both DataFrames. You will do so by first extracting all row indices of `restaurants_new` that are matching across the columns mentioned above from `potential_matches`. Then you will subset `restaurants_new` on these indices, then append the non-duplicate values to `restaurants`. All DataFrames are in your environment, alongside `pandas` imported as `pd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Linking them together! | Python](https://campus.datacamp.com/courses/cleaning-data-in-python/record-linkage-4?ex=11)\n",
    "\n",
    "> -   Isolate instances of `potential_matches` where the row sum is above or equal to 3 by using the `.sum()` method.\n",
    "> -   Extract the second column index from `matches`, which represents row indices of matching record from `restaurants_new` by using the `.get_level_values()` method.\n",
    "> -   Subset `restaurants_new` for rows that are not in `matching_indices`.\n",
    "> -   Append `non_dup` to `restaurants`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-30T08:13:24.114247Z",
     "start_time": "2021-04-30T08:13:24.088025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    rest_name                  rest_addr               city  \\\n",
      "0   arnie morton's of chicago   435 s. la cienega blv .         los angeles   \n",
      "1          art's delicatessen       12224 ventura blvd.         studio city   \n",
      "2                   campanile       624 s. la brea ave.         los angeles   \n",
      "3                       fenix    8358 sunset blvd. west           hollywood   \n",
      "4          grill on the alley           9560 dayton way         los angeles   \n",
      "..                        ...                        ...                ...   \n",
      "76                        don        1136 westwood blvd.           westwood   \n",
      "77                      feast        1949 westwood blvd.            west la   \n",
      "78                   mulberry        17040 ventura blvd.             encino   \n",
      "80                    jiraffe      502 santa monica blvd       santa monica   \n",
      "81                   martha's  22nd street grill 25 22nd  st. hermosa beach   \n",
      "\n",
      "         phone cuisine_type  \n",
      "0   3102461501     american  \n",
      "1   8187621221     american  \n",
      "2   2139381447     american  \n",
      "3   2138486677     american  \n",
      "4   3102760615     american  \n",
      "..         ...          ...  \n",
      "76  3102091422      italian  \n",
      "77  3104750400      chinese  \n",
      "78  8189068881        pizza  \n",
      "80  3109176671  californian  \n",
      "81  3103767786     american  \n",
      "\n",
      "[396 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Isolate potential matches with row sum >=3\n",
    "matches = potential_matches[potential_matches.sum(axis=1) >= 3]\n",
    "\n",
    "# Get values of second column index of matches\n",
    "matching_indices = matches.index.get_level_values(1)\n",
    "\n",
    "# Subset restaurants_new based on non-duplicate values\n",
    "non_dup = restaurants_new[~restaurants_new.index.isin(matching_indices)]\n",
    "\n",
    "# Append non_dup to restaurants\n",
    "full_restaurants = restaurants.append(non_dup)\n",
    "print(full_restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datacamp] *",
   "language": "python",
   "name": "conda-env-datacamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
