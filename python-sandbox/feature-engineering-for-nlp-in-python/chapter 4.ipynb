{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building tf-idf document vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf vectors for TED talks\n",
    "In this exercise, you have been given a corpus ted which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks.\n",
    "\n",
    "In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:07:47.808172Z",
     "start_time": "2020-01-21T16:07:31.710612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {pandas.core.series.Series: {\"ted.csv\": \"https://file.io/TyIcyb\"}}\n",
      "\n",
      "{'pandas.core.series.Series': {'ted.csv': 'https://file.io/TyIcyb'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "100 1560k    0 1560k    0     0   333k      0 --:--:--  0:00:04 --:--:--  333k\n",
      "100 1605k    0 1605k    0     0   342k      0 --:--:--  0:00:04 --:--:--  342k\n",
      "100 2820k    0 2820k    0     0   453k      0 --:--:--  0:00:06 --:--:--  578k\n",
      "100 3660k    0 3660k    0     0   547k      0 --:--:--  0:00:06 --:--:--  842k\n",
      "100 5468k    0 5468k    0     0   716k      0 --:--:--  0:00:07 --:--:-- 1111k\n"
     ]
    }
   ],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" Ã  executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(ted)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    " {pandas.core.series.Series: {'ted.csv': 'https://file.io/TyIcyb'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap4-Exercise1.1_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "ted = pd.read_csv(prefix+'ted.csv',index_col=0, header=None,squeeze=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:08:55.506302Z",
     "start_time": "2020-01-21T16:08:53.542588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 29158)\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(ted)\n",
    "\n",
    "# Print the shape of tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing dot product\n",
    "In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the numpy library. More specifically, we will use the np.dot() function to compute the dot product of two numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:14:55.665939Z",
     "start_time": "2020-01-21T16:14:55.660936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Initialize numpy vectors\n",
    "A = np.array([1,3])\n",
    "B = np.array([-2,2])\n",
    "\n",
    "# Compute dot product\n",
    "dot_prod = np.dot(A, B)\n",
    "\n",
    "# Print dot product\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity matrix of a corpus\n",
    "In this exercise, you have been given a corpus, which is a list containing five sentences. The corpus is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf).\n",
    "\n",
    "Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:15:45.397790Z",
     "start_time": "2020-01-21T16:15:45.394782Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ['The sun is the largest celestial body in the solar system', 'The solar system consists of the sun and eight revolving planets', 'Ra was the Egyptian Sun God', 'The Pyramids were the pinnacle of Egyptian architecture', 'The quick brown fox jumps over the lazy dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:16:30.934591Z",
     "start_time": "2020-01-21T16:16:30.862237Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:17:35.832229Z",
     "start_time": "2020-01-21T16:17:35.825806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
      " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
      " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
      " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
      " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an instance of tf-idf Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Generate the tf-idf vectors for the corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Compute and print the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a plot line based recommender\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing linear_kernel and cosine_similarity\n",
    "In this exercise, you have been given tfidf_matrix which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using cosine_similarity and then, using linear_kernel.\n",
    "\n",
    "We will then compare the computation times for both functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start = time.time()\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Print cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# Print time taken\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot recommendation engine\n",
    "In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a get_recommendations() function that takes in the title of a movie, a similarity matrix and an indices series as its arguments and outputs a list of most similar movies. indices has already been provided to you.\n",
    "\n",
    "You have also been given a movie_plots Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots.\n",
    "\n",
    "Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:34:44.892760Z",
     "start_time": "2020-01-21T16:34:44.872577Z"
    }
   },
   "outputs": [],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" Ã  executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(indices, movie_plots)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.series.Series: {'indices.csv': 'https://file.io/iTkOnT',\n",
    "  'movie_plots.csv': 'https://file.io/c4BzRd'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap4-Exercise3.2_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "indices = pd.read_csv(prefix+'indices.csv',index_col=0, header=None,squeeze=True)\n",
    "movie_plots = pd.read_csv(prefix+'movie_plots.csv',index_col=0, header=None,squeeze=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:37:17.107983Z",
     "start_time": "2020-01-21T16:37:15.366181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{pandas.core.frame.DataFrame: {\"metadata.csv\": \"https://file.io/QGNgSl\"}}\n",
      "\n",
      "{'pandas.core.frame.DataFrame': {'metadata.csv': 'https://file.io/QGNgSl'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100  385k    0  385k    0     0   269k      0 --:--:--  0:00:01 --:--:--  269k\n"
     ]
    }
   ],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" Ã  executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(metadata)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.frame.DataFrame: {'metadata.csv': 'https://file.io/QGNgSl'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap4-Exercise3.2_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "metadata = pd.read_csv(prefix+'metadata.csv',index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:37:25.600870Z",
     "start_time": "2020-01-21T16:37:25.595393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: 1, dtype: object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_plots[movie_plots.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:37:26.136251Z",
     "start_time": "2020-01-21T16:37:26.130176Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Ã  executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "import inspect\n",
    "print_func(get_recommendations)\n",
    "\"\"\"\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "    # Get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]\n",
    "\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:37:27.610232Z",
     "start_time": "2020-01-21T16:37:27.532969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                              Batman Forever\n",
      "2                                      Batman\n",
      "3                              Batman Returns\n",
      "8                  Batman: Under the Red Hood\n",
      "9                            Batman: Year One\n",
      "10    Batman: The Dark Knight Returns, Part 1\n",
      "11    Batman: The Dark Knight Returns, Part 2\n",
      "5                Batman: Mask of the Phantasm\n",
      "7                               Batman Begins\n",
      "4                              Batman & Robin\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The recommender function\n",
    "In this exercise, we will build a recommender function get_recommendations(), as discussed in the lesson and the previous exercise. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself).\n",
    "\n",
    "You have been given a dataset metadata that consists of the movie titles and overviews. The head of this dataset has been printed to console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:40:25.466367Z",
     "start_time": "2020-01-21T16:40:24.251312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{pandas.core.frame.DataFrame: {\"metadata.csv\": \"https://file.io/bhVPmN\"}}\n",
      "\n",
      "{'pandas.core.frame.DataFrame': {'metadata.csv': 'https://file.io/bhVPmN'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 63829    0 63829    0     0  64149      0 --:--:-- --:--:-- --:--:-- 64149\n"
     ]
    }
   ],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" Ã  executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(metadata)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.frame.DataFrame: {'metadata.csv': 'https://file.io/bhVPmN'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap4-Exercise3.3_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "metadata = pd.read_csv(prefix+'metadata.csv',index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:40:34.846563Z",
     "start_time": "2020-01-21T16:40:34.838542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>tagline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>Cinema Paradiso</td>\n",
       "      <td>A celebration of youth, friendship, and the ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>Spy Hard</td>\n",
       "      <td>All the action. All the women. Half the intell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>Stonewall</td>\n",
       "      <td>The fight for the right to love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>Killer</td>\n",
       "      <td>You only hurt the one you love.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Jason's Lyric</td>\n",
       "      <td>Love is courage.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               title                                            tagline\n",
       "938  Cinema Paradiso  A celebration of youth, friendship, and the ev...\n",
       "630         Spy Hard  All the action. All the women. Half the intell...\n",
       "682        Stonewall                    The fight for the right to love\n",
       "514           Killer                    You only hurt the one you love.\n",
       "365    Jason's Lyric                                   Love is courage."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:42:07.203110Z",
     "start_time": "2020-01-21T16:42:07.197129Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate mapping between titles and index\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get index of movie that matches title\n",
    "    idx = indices[title]\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TED talk recommender\n",
    "In this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts. You have been given a get_recommendations() function that takes in the title of a talk, a similarity matrix and an indices series as its arguments, and outputs a list of most similar talks. indices has already been provided to you.\n",
    "\n",
    "You have also been given a transcripts series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts.\n",
    "\n",
    "Consequently, we will generate recommendations for a talk titled '5 ways to kill your dreams' by Brazilian entrepreneur Bel Pesce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:45:23.976602Z",
     "start_time": "2020-01-21T16:45:19.712704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{pandas.core.series.Series: {\"indices.csv\": \"https://file.io/WS3cZP\",\n",
      "  \"transcripts.csv\": \"https://file.io/6L6VlS\"}}\n",
      "\n",
      "{'pandas.core.series.Series': {'indices.csv': 'https://file.io/WS3cZP', 'transcripts.csv': 'https://file.io/6L6VlS'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 20516    0 20516    0     0  27875      0 --:--:-- --:--:-- --:--:-- 27912\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "100 4920k    0 4920k    0     0  1667k      0 --:--:--  0:00:02 --:--:-- 1667k\n",
      "100 5586k    0 5586k    0     0  1822k      0 --:--:--  0:00:03 --:--:-- 1823k\n"
     ]
    }
   ],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" Ã  executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(indices, transcripts)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.series.Series: {'indices.csv': 'https://file.io/WS3cZP',\n",
    "  'transcripts.csv': 'https://file.io/6L6VlS'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap4-Exercise3.4_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "indices = pd.read_csv(prefix+'indices.csv',index_col=0, header=None,squeeze=True)\n",
    "transcripts = pd.read_csv(prefix+'transcripts.csv',index_col=0, header=None,squeeze=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:50:49.667400Z",
     "start_time": "2020-01-21T16:50:45.246452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{pandas.core.frame.DataFrame: {\"ted.csv\": \"https://file.io/80JHyW\"}}\n",
      "\n",
      "{'pandas.core.frame.DataFrame': {'ted.csv': 'https://file.io/80JHyW'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "100 1110k    0 1110k    0     0   331k      0 --:--:--  0:00:03 --:--:--  331k\n",
      "100 5646k    0 5646k    0     0  1357k      0 --:--:--  0:00:04 --:--:-- 1357k\n"
     ]
    }
   ],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" Ã  executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(ted)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.frame.DataFrame: {'ted.csv': 'https://file.io/80JHyW'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap4-Exercise3.4_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "ted = pd.read_csv(prefix+'ted.csv',index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:50:51.602290Z",
     "start_time": "2020-01-21T16:50:51.597243Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "    # Get the pairwsie similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores for 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "    # Get the movie indices\n",
    "    talk_indices = [i[0] for i in sim_scores]\n",
    "    # Return the top 10 most similar movies\n",
    "    return ted['title'].iloc[talk_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:50:54.946364Z",
     "start_time": "2020-01-21T16:50:54.184655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453             Success is a continuous journey\n",
      "157                        Why we do what we do\n",
      "494                   How to find work you love\n",
      "149          My journey into movies that matter\n",
      "447                        One Laptop per Child\n",
      "230             How to get your ideas to spread\n",
      "497         Plug into your hard-wired happiness\n",
      "495    Why you will fail to have a great career\n",
      "179             Be suspicious of simple stories\n",
      "53                          To upgrade is human\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Construct the TF-IDF matrix\n",
    "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
    "\n",
    "# Generate the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix)\n",
    " \n",
    "# Generate recommendations \n",
    "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond n-grams: word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating word vectors\n",
    "In this exercise, we will generate the pairwise similarity scores of all the words in a sentence. The sentence is available as sent and has been printed to the console for your convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:59:07.995113Z",
     "start_time": "2020-01-21T16:59:07.298391Z"
    }
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:59:24.574196Z",
     "start_time": "2020-01-21T16:59:24.047389Z"
    }
   },
   "outputs": [],
   "source": [
    "sent = 'I like apples and oranges'\n",
    "import spacy\n",
    "# Load model and create Doc object\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T16:59:26.675824Z",
     "start_time": "2020-01-21T16:59:26.404653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I I 1.0\n",
      "I like 0.11147683\n",
      "I apples 0.006771503\n",
      "I and -0.11766664\n",
      "I oranges -0.0021670808\n",
      "like I 0.11147683\n",
      "like like 1.0\n",
      "like apples 0.05282909\n",
      "like and -0.15288135\n",
      "like oranges -0.04325358\n",
      "apples I 0.006771503\n",
      "apples like 0.05282909\n",
      "apples apples 1.0\n",
      "apples and -0.00111599\n",
      "apples oranges 0.6354259\n",
      "and I -0.11766664\n",
      "and like -0.15288135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and apples -0.00111599\n",
      "and and 1.0\n",
      "and oranges 0.08937079\n",
      "oranges I -0.0021670808\n",
      "oranges like -0.04325358\n",
      "oranges apples 0.6354259\n",
      "oranges and 0.08937079\n",
      "oranges oranges 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "# Create the doc object\n",
    "doc = nlp(sent)\n",
    "\n",
    "# Compute pairwise similarity scores\n",
    "for token1 in doc:\n",
    "  for token2 in doc:\n",
    "    print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing similarity of Pink Floyd songs\n",
    "In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely 'High Hopes', 'Hey You' and 'Mother'. The lyrics to these songs are available as hopes, hey and mother respectively.\n",
    "\n",
    "Your task is to compute the pairwise similarity between mother and hopes, and mother and hey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:00:28.111410Z",
     "start_time": "2020-01-21T17:00:28.106396Z"
    }
   },
   "outputs": [],
   "source": [
    "hopes = \"\\nBeyond the horizon of the place we lived when we were young\\nIn a world of magnets and miracles\\nOur thoughts strayed constantly and without boundary\\nThe ringing of the division bell had begun\\nAlong the Long Road and on down the Causeway\\nDo they still meet there by the Cut\\nThere was a ragged band that followed in our footsteps\\nRunning before times took our dreams away\\nLeaving the myriad small creatures trying to tie us to the ground\\nTo a life consumed by slow decay\\nThe grass was greener\\nThe light was brighter\\nWhen friends surrounded\\nThe nights of wonder\\nLooking beyond the embers of bridges glowing behind us\\nTo a glimpse of how green it was on the other side\\nSteps taken forwards but sleepwalking back again\\nDragged by the force of some in a tide\\nAt a higher altitude with flag unfurled\\nWe reached the dizzy heights of that dreamed of world\\nEncumbered forever by desire and ambition\\nThere's a hunger still unsatisfied\\nOur weary eyes still stray to the horizon\\nThough down this road we've been so many times\\nThe grass was greener\\nThe light was brighter\\nThe taste was sweeter\\nThe nights of wonder\\nWith friends surrounded\\nThe dawn mist glowing\\nThe water flowing\\nThe endless river\\nForever and ever\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:00:45.217291Z",
     "start_time": "2020-01-21T17:00:45.215301Z"
    }
   },
   "outputs": [],
   "source": [
    "mother = \"\\nMother do you think they'll drop the bomb?\\nMother do you think they'll like this song?\\nMother do you think they'll try to break my balls?\\nOoh, ah\\nMother should I build the wall?\\nMother should I run for President?\\nMother should I trust the government?\\nMother will they put me in the firing mine?\\nOoh ah,\\nIs it just a waste of time?\\nHush now baby, baby, don't you cry.\\nMama's gonna make all your nightmares come true.\\nMama's gonna put all her fears into you.\\nMama's gonna keep you right here under her wing.\\nShe won't let you fly, but she might let you sing.\\nMama's gonna keep baby cozy and warm.\\nOoh baby, ooh baby, ooh baby,\\nOf course mama's gonna help build the wall.\\nMother do you think she's good enough, for me?\\nMother do you think she's dangerous, to me?\\nMother will she tear your little boy apart?\\nOoh ah,\\nMother will she break my heart?\\nHush now baby, baby don't you cry.\\nMama's gonna check out all your girlfriends for you.\\nMama won't let anyone dirty get through.\\nMama's gonna wait up until you get in.\\nMama will always find out where you've been.\\nMama's gonna keep baby healthy and clean.\\nOoh baby, ooh baby, ooh baby,\\nYou'll always be baby to me.\\nMother, did it need to be so high?\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:01:06.085108Z",
     "start_time": "2020-01-21T17:01:06.081098Z"
    }
   },
   "outputs": [],
   "source": [
    "hey = \"\\nHey you, out there in the cold\\nGetting lonely, getting old\\nCan you feel me?\\nHey you, standing in the aisles\\nWith itchy feet and fading smiles\\nCan you feel me?\\nHey you, don't help them to bury the light\\nDon't give in without a fight\\nHey you out there on your own\\nSitting naked by the phone\\nWould you touch me?\\nHey you with you ear against the wall\\nWaiting for someone to call out\\nWould you touch me?\\nHey you, would you help me to carry the stone?\\nOpen your heart, I'm coming home\\nBut it was only fantasy\\nThe wall was too high\\nAs you can see\\nNo matter how he tried\\nHe could not break free\\nAnd the worms ate into his brain\\nHey you, out there on the road\\nAlways doing what you're told\\nCan you help me?\\nHey you, out there beyond the wall\\nBreaking bottles in the hall\\nCan you help me?\\nHey you, don't tell me there's no hope at all\\nTogether we stand, divided we fall\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T17:01:52.910995Z",
     "start_time": "2020-01-21T17:01:52.763447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6172701119657092\n",
      "0.9102045252431249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\envs\\datacamp\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "# Create Doc objects\n",
    "mother_doc = nlp(mother)\n",
    "hopes_doc = nlp(hopes)\n",
    "hey_doc = nlp(hey)\n",
    "\n",
    "# Print similarity between mother and hopes\n",
    "print(mother_doc.similarity(hopes_doc))\n",
    "\n",
    "# Print similarity between mother and hey\n",
    "print(mother_doc.similarity(hey_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datacamp] *",
   "language": "python",
   "name": "conda-env-datacamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
