{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a bag of words model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW model for movie taglines\n",
    "In this exercise, you have been provided with a corpus of more than 7000 movie tag lines. Your job is to generate the bag of words representation bow_matrix for these taglines. For this exercise, we will ignore the text preprocessing step and generate bow_matrix directly.\n",
    "\n",
    "We will also investigate the shape of the resultant bow_matrix. The first five taglines in corpus have been printed to the console for you to examine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:17:09.330426Z",
     "start_time": "2020-01-21T15:17:06.881430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{pandas.core.series.Series: {\"corpus.csv\": \"https://file.io/eVXVNs\"}}\n",
      "\n",
      "{'pandas.core.series.Series': {'corpus.csv': 'https://file.io/eVXVNs'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100  370k    0  370k    0     0   258k      0 --:--:--  0:00:01 --:--:--  258k\n"
     ]
    }
   ],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(corpus)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.series.Series: {'corpus.csv': 'https://file.io/eVXVNs'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap3-Exercise1.1_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "corpus = pd.read_csv(prefix+'corpus.csv',index_col=0, header=None,squeeze=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:20:26.930740Z",
     "start_time": "2020-01-21T15:20:26.815178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7033, 6614)\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the shape of bow_matrix\n",
    "print(bow_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing dimensionality and preprocessing\n",
    "In this exercise, you have been provided with a lem_corpus which contains the pre-processed versions of the movie taglines from the previous exercise. In other words, the taglines have been lowercased and lemmatized, and stopwords have been removed.\n",
    "\n",
    "Your job is to generate the bag of words representation bow_lem_matrix for these lemmatized taglines and compare its shape with that of bow_matrix obtained in the previous exercise. The first five lemmatized taglines in lem_corpus have been printed to the console for you to examine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:22:09.592874Z",
     "start_time": "2020-01-21T15:22:08.179569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{pandas.core.series.Series: {\"lem_corpus.csv\": \"https://file.io/3fzva3\"}}\n",
      "\n",
      "{'pandas.core.series.Series': {'lem_corpus.csv': 'https://file.io/3fzva3'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  205k    0  205k    0     0   187k      0 --:--:--  0:00:01 --:--:--  187k\n"
     ]
    }
   ],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(lem_corpus)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.series.Series: {'lem_corpus.csv': 'https://file.io/3fzva3'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap3-Exercise1.2_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "lem_corpus = pd.read_csv(prefix+'lem_corpus.csv',index_col=0, header=None,squeeze=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:22:45.805292Z",
     "start_time": "2020-01-21T15:22:45.747045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6959, 5223)\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n",
    "\n",
    "# Print the shape of bow_lem_matrix\n",
    "print(bow_lem_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping feature indices with feature names\n",
    "In the lesson video, we had seen that CountVectorizer doesn't necessarily index the vocabulary in alphabetical order. In this exercise, we will learn to map each feature index to its corresponding feature name from the vocabulary.\n",
    "\n",
    "We will use the same three sentences on lions from the video. The sentences are available in a list named corpus and has already been printed to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:23:43.143478Z",
     "start_time": "2020-01-21T15:23:43.139466Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = ['The lion is the king of the jungle',\n",
    " 'Lions have lifespans of a decade',\n",
    " 'The lion is an endangered species']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:24:26.573141Z",
     "start_time": "2020-01-21T15:24:26.563098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   an  decade  endangered  have  is  jungle  king  lifespans  lion  lions  of  \\\n",
      "0   0       0           0     0   1       1     1          0     1      0   1   \n",
      "1   0       1           0     1   0       0     0          1     0      1   1   \n",
      "2   1       0           1     0   1       0     0          0     1      0   0   \n",
      "\n",
      "   species  the  \n",
      "0        0    3  \n",
      "1        0    0  \n",
      "2        1    1  \n"
     ]
    }
   ],
   "source": [
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Generate matrix of word vectors\n",
    "bow_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert bow_matrix into a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
    "\n",
    "# Map the column names to vocabulary \n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "# Print bow_df\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a BoW Naive Bayes classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW vectors for movie reviews\n",
    "In this exercise, you have been given two pandas Series, X_train and X_test, which consist of movie reviews. They represent the training and the test review data respectively. Your task is to preprocess the reviews and generate BoW vectors for these two sets using CountVectorizer.\n",
    "\n",
    "Once we have generated the BoW vector matrices X_train_bow and X_test_bow, we will be in a very good position to apply a machine learning model to it and conduct sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:32:04.564323Z",
     "start_time": "2020-01-21T15:32:01.677761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{pandas.core.series.Series: {\"X_test.csv\": \"https://file.io/VMCtL5\",\n",
      "  \"X_train.csv\": \"https://file.io/zBLW39\"}}\n",
      "\n",
      "{'pandas.core.series.Series': {'X_test.csv': 'https://file.io/VMCtL5', 'X_train.csv': 'https://file.io/zBLW39'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100  881k    0  881k    0     0   508k      0 --:--:--  0:00:01 --:--:--  508k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  319k    0  319k    0     0   493k      0 --:--:-- --:--:-- --:--:--  493k\n"
     ]
    }
   ],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(X_train, X_test)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.series.Series: {'X_test.csv': 'https://file.io/VMCtL5',\n",
    "  'X_train.csv': 'https://file.io/zBLW39'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap3-Exercise2.1_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "X_train = pd.read_csv(prefix+'X_train.csv',index_col=0, header=None,squeeze=True)\n",
    "X_test = pd.read_csv(prefix+'X_test.csv',index_col=0, header=None,squeeze=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:33:13.429719Z",
     "start_time": "2020-01-21T15:33:13.241523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 8158)\n",
      "(750, 8158)\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "# Fit and transform X_train\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform X_test\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "# Print shape of X_train_bow and X_test_bow\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the sentiment of a movie review\n",
    "In the previous exercise, you generated the bag-of-words representations for the training and test movie review data. In this exercise, we will use this model to train a Naive Bayes classifier that can detect the sentiment of a movie review and compute its accuracy. Note that since this is a binary classification problem, the model is only capable of classifying a review as either positive (1) or negative (0). It is incapable of detecting neutral reviews.\n",
    "\n",
    "In case you don't recall, the training and test BoW vectors are available as X_train_bow and X_test_bow respectively. The corresponding labels are available as y_train and y_test respectively. Also, for you reference, the original movie review dataset is available as df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:36:12.070912Z",
     "start_time": "2020-01-21T15:36:08.754774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{pandas.core.frame.DataFrame: {\"df.csv\": \"https://file.io/xRv5dz\"},\n",
      " pandas.core.series.Series: {\"y_test.csv\": \"https://file.io/dp0Ah3\",\n",
      "  \"y_train.csv\": \"https://file.io/MbLw61\"}}\n",
      "\n",
      "{'pandas.core.frame.DataFrame': {'df.csv': 'https://file.io/xRv5dz'}, 'pandas.core.series.Series': {'y_test.csv': 'https://file.io/dp0Ah3', 'y_train.csv': 'https://file.io/MbLw61'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 1202k    0 1202k    0     0   553k      0 --:--:--  0:00:02 --:--:--  554k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  4415    0  4415    0     0  14765      0 --:--:-- --:--:-- --:--:-- 14815\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1475    0  1475    0     0   5524      0 --:--:-- --:--:-- --:--:--  5524\n"
     ]
    }
   ],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(df, y_train, y_test)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.frame.DataFrame: {'df.csv': 'https://file.io/xRv5dz'},\n",
    " pandas.core.series.Series: {'y_test.csv': 'https://file.io/dp0Ah3',\n",
    "  'y_train.csv': 'https://file.io/MbLw61'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap3-Exercise2.2_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(prefix+'df.csv',index_col=0)\n",
    "y_train = pd.read_csv(prefix+'y_train.csv',index_col=0, header=None,squeeze=True)\n",
    "y_test = pd.read_csv(prefix+'y_test.csv',index_col=0, header=None,squeeze=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:36:55.060048Z",
     "start_time": "2020-01-21T15:36:55.052063Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:37:47.077384Z",
     "start_time": "2020-01-21T15:37:46.971523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.732\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "# Create a MultinomialNB object\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building n-gram models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram models for movie tag lines\n",
    "In this exercise, we have been provided with a corpus of more than 9000 movie tag lines. Our job is to generate n-gram models up to n equal to 1, n equal to 2 and n equal to 3 for this data and discover the number of features for each model.\n",
    "\n",
    "We will then compare the number of features generated for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:46:24.745559Z",
     "start_time": "2020-01-21T15:46:24.726509Z"
    }
   },
   "outputs": [],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(corpus)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.series.Series: {'corpus.csv': 'https://file.io/KkQJic'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap3-Exercise3.1_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "corpus = pd.read_csv(prefix+'corpus.csv',index_col=0, header=None,squeeze=True)\n",
    "\n",
    "corpus.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:46:26.591740Z",
     "start_time": "2020-01-21T15:46:25.977148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng1, ng2 and ng3 have 6614, 37100 and 76881 features respectively\n"
     ]
    }
   ],
   "source": [
    "# Generate n-grams upto n=1\n",
    "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
    "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=2\n",
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
    "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
    "\n",
    "# Generate n-grams upto n=3\n",
    "vectorizer_ng3 = CountVectorizer(ngram_range=(1, 3))\n",
    "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
    "\n",
    "# Print the number of features for each model\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher order n-grams for sentiment analysis\n",
    "Similar to a previous exercise, we are going to build a classifier that can detect if the review of a particular movie is positive or negative. However, this time, we will use n-grams up to n=2 for the task.\n",
    "\n",
    "The n-gram training reviews are available as X_train_ng. The corresponding test reviews are available as X_test_ng. Finally, use y_train and y_test to access the training and test sentiment classes respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:51:34.283872Z",
     "start_time": "2020-01-21T15:51:29.943834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{pandas.core.series.Series: {\"X_test.csv\": \"https://file.io/nTwjmp\",\n",
      "  \"X_train.csv\": \"https://file.io/zqbOGc\",\n",
      "  \"y_test.csv\": \"https://file.io/0IS0B6\",\n",
      "  \"y_train.csv\": \"https://file.io/xPeTm2\"}}\n",
      "\n",
      "{'pandas.core.series.Series': {'X_test.csv': 'https://file.io/nTwjmp', 'X_train.csv': 'https://file.io/zqbOGc', 'y_test.csv': 'https://file.io/0IS0B6', 'y_train.csv': 'https://file.io/xPeTm2'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100  135k    0  135k    0     0  83076      0 --:--:--  0:00:01 --:--:-- 83076\n",
      "100  591k    0  591k    0     0   293k      0 --:--:--  0:00:02 --:--:--  293k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  609k    0  609k    0     0   723k      0 --:--:-- --:--:-- --:--:--  723k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  2949    0  2949    0     0  10457      0 --:--:-- --:--:-- --:--:-- 10494\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  2941    0  2941    0     0   6449      0 --:--:-- --:--:-- --:--:--  6463\n"
     ]
    }
   ],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(X_train, X_test, y_train, y_test)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.series.Series: {'X_test.csv': 'https://file.io/nTwjmp',\n",
    "  'X_train.csv': 'https://file.io/zqbOGc',\n",
    "  'y_test.csv': 'https://file.io/0IS0B6',\n",
    "  'y_train.csv': 'https://file.io/xPeTm2'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap3-Exercise3.2_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "X_train = pd.read_csv(prefix+'X_train.csv',index_col=0, header=None,squeeze=True)\n",
    "X_test = pd.read_csv(prefix+'X_test.csv',index_col=0, header=None,squeeze=True)\n",
    "y_train = pd.read_csv(prefix+'y_train.csv',index_col=0, header=None,squeeze=True)\n",
    "y_test = pd.read_csv(prefix+'y_test.csv',index_col=0, header=None,squeeze=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:55:31.823070Z",
     "start_time": "2020-01-21T15:55:31.358479Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate n-grams upto n=2\n",
    "ng_vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X_train_ng = ng_vectorizer.fit_transform(X_train)\n",
    "X_test_ng = ng_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:55:32.862763Z",
     "start_time": "2020-01-21T15:55:32.843640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.758\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "# Define an instance of MultinomialNB \n",
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# Fit the classifier \n",
    "clf_ng.fit(X_train_ng, y_train)\n",
    "\n",
    "# Measure the accuracy \n",
    "accuracy = clf_ng.score(X_test_ng, y_test)\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing performance of n-gram models\n",
    "You now know how to conduct sentiment analysis by converting text into various n-gram representations and feeding them to a classifier. In this exercise, we will conduct sentiment analysis for the same movie reviews from before using two n-gram models: unigrams and n-grams upto n equal to 3.\n",
    "\n",
    "We will then compare the performance using three criteria: accuracy of the model on the test set, time taken to execute the program and the number of features created when generating the n-gram representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:57:41.024273Z",
     "start_time": "2020-01-21T15:57:38.160406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{pandas.core.frame.DataFrame: {\"df.csv\": \"https://file.io/SzR8im\"}}\n",
      "\n",
      "{'pandas.core.frame.DataFrame': {'df.csv': 'https://file.io/SzR8im'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100  690k    0  690k    0     0   284k      0 --:--:--  0:00:02 --:--:--  284k\n",
      "100 1202k    0 1202k    0     0   457k      0 --:--:--  0:00:02 --:--:--  457k\n"
     ]
    }
   ],
   "source": [
    "#upload and download\n",
    "\n",
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(df)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{pandas.core.frame.DataFrame: {'df.csv': 'https://file.io/SzR8im'}}\n",
    "\"\"\"\n",
    "prefix='data_from_datacamp/Chap3-Exercise3.3_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(prefix+'df.csv',index_col=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:59:27.996455Z",
     "start_time": "2020-01-21T15:59:27.950900Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:59:28.228566Z",
     "start_time": "2020-01-21T15:59:28.033489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 0.189 seconds to complete. The accuracy on the test set is 0.75. The ngram representation had 12347 features.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:59:55.292546Z",
     "start_time": "2020-01-21T15:59:54.364917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 0.918 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 178240 features.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
    "\n",
    "# Generating ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
    "train_X = vectorizer.fit_transform(train_X)\n",
    "test_X = vectorizer.transform(test_X)\n",
    "\n",
    "# Fit classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "# Print accuracy, time and number of dimensions\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datacamp] *",
   "language": "python",
   "name": "conda-env-datacamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "303.333px",
    "left": "1348px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
