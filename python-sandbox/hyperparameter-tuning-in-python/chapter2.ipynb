{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Grid Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Grid Search functions\n",
    "In data science it is a great idea to try building algorithms, models and processes 'from scratch' so you can really understand what is happening at a deeper level. Of course there are great packages and libraries for this work (and we will get to that very soon!) but building from scratch will give you a great edge in your data science work.\n",
    "\n",
    "In this exercise, you will create a function to take in 2 hyperparameters, build models and return results. You will use this function in a future exercise.\n",
    "\n",
    "You will have available the X_train, X_test, y_train and y_test datasets available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init: 2 dataframes, 2 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:46:23.373333Z",
     "start_time": "2019-12-17T13:46:23.370332Z"
    }
   },
   "outputs": [],
   "source": [
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\n",
    "#uploadToFileIO(X_train, X_test, y_train, y_test)\n",
    "tobedownloaded=\"{pandas.core.frame.DataFrame: {'X_test.csv': 'https://file.io/KNty1a',\\\n",
    "  'X_train.csv': 'https://file.io/bCWSyc'},\\\n",
    " numpy.ndarray: {'y_test.csv': 'https://file.io/wZlesY',\\\n",
    "  'y_train.csv': 'https://file.io/sNmdly'}}\"\n",
    "prefix='data_from_datacamp/Chap2-Exercise1.1_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:46:24.271521Z",
     "start_time": "2019-12-17T13:46:24.254478Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from uploadfromdatacamp import loadNDArrayFromCsv\n",
    "\n",
    "X_train=pd.read_csv(prefix+'X_train.csv',index_col=0)\n",
    "X_test=pd.read_csv(prefix+'X_test.csv',index_col=0)\n",
    "y_train = loadNDArrayFromCsv(prefix+'y_train.csv')\n",
    "y_test = loadNDArrayFromCsv(prefix+'y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build a function that takes two parameters called learn_rate and max_depth for the learning rate and maximum depth.\n",
    "- Add capability in the function to build a GBM model and fit it to the data with the input hyperparameters.\n",
    "- Have the function return the results of that model and the chosen hyperparameters (learn_rate and max_depth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:46:45.910919Z",
     "start_time": "2019-12-17T13:46:45.907910Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:46:46.259004Z",
     "start_time": "2019-12-17T13:46:46.254024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the function\n",
    "def gbm_grid_search(learn_rate, max_depth):\n",
    "\n",
    "\t# Create the model\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth)\n",
    "    \n",
    "    # Use the model to make predictions\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Return the hyperparameters and score\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteratively tune multiple hyperparameters\n",
    "In this exercise, you will build on the function you previously created to take in 2 hyperparameters, build a model and return the results. You will now use that to loop through some values and then extend this function and loop with another hyperparameter.\n",
    "\n",
    "The function gbm_grid_search(learn_rate, max_depth) is available in this exercise.\n",
    "\n",
    "If you need to remind yourself of the function you can run the function print_func() that has been created for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a for-loop to test the values (0.01, 0.1, 0.5) for the learning_rate and (2, 4, 6) for the max_depth using the function you created gbm_grid_search and print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:46:49.790162Z",
     "start_time": "2019-12-17T13:46:48.581414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01, 2, 0.78], [0.01, 4, 0.77], [0.01, 6, 0.76], [0.1, 2, 0.74], [0.1, 4, 0.75], [0.1, 6, 0.76], [0.5, 2, 0.73], [0.5, 4, 0.73], [0.5, 6, 0.75]]\n"
     ]
    }
   ],
   "source": [
    "# Create the relevant lists\n",
    "results_list = []\n",
    "learn_rate_list = [0.01,0.1,0.5]\n",
    "max_depth_list = [2,4,6]\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        results_list.append(gbm_grid_search(learn_rate,max_depth))\n",
    "\n",
    "# Print the results\n",
    "print(results_list)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend the gbm_grid_search function to include the hyperparameter subsample. Name this new function gbm_grid_search_extended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:47:59.107627Z",
     "start_time": "2019-12-17T13:47:59.103579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extend the function input\n",
    "def gbm_grid_search_extended(learn_rate, max_depth, subsample):\n",
    "\n",
    "\t# Extend the model creation section\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth, subsample=subsample)\n",
    "    \n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Extend the return part\n",
    "    return([learn_rate, max_depth, subsample, accuracy_score(y_test, predictions)])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend your loop to call gbm_grid_search (available in your console), then test the values [0.4 , 0.6] for the subsample hyperparameter and print the results. max_depth_list & learn_rate_list are available in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:49:11.826244Z",
     "start_time": "2019-12-17T13:49:09.225773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01, 2, 0.4, 0.74], [0.01, 2, 0.6, 0.74], [0.01, 4, 0.4, 0.76], [0.01, 4, 0.6, 0.75], [0.01, 6, 0.4, 0.73], [0.01, 6, 0.6, 0.75], [0.1, 2, 0.4, 0.73], [0.1, 2, 0.6, 0.74], [0.1, 4, 0.4, 0.72], [0.1, 4, 0.6, 0.74], [0.1, 6, 0.4, 0.73], [0.1, 6, 0.6, 0.74], [0.5, 2, 0.4, 0.65], [0.5, 2, 0.6, 0.69], [0.5, 4, 0.4, 0.59], [0.5, 4, 0.6, 0.76], [0.5, 6, 0.4, 0.61], [0.5, 6, 0.6, 0.68]]\n"
     ]
    }
   ],
   "source": [
    "results_list = []\n",
    "\n",
    "# Create the new list to test\n",
    "subsample_list = [0.4, 0.6]\n",
    "\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "    \n",
    "    \t# Extend the for loop\n",
    "        for subsample in subsample_list:\n",
    "        \t\n",
    "            # Extend the results to include the new hyperparameter\n",
    "            results_list.append(gbm_grid_search_extended(learn_rate, max_depth, subsample))\n",
    "            \n",
    "# Print results\n",
    "print(results_list)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Many Models?\n",
    "Adding more hyperparameters or values, you increase the amount of models created but the increases is not linear it is proportional to how many values and hyperparameters you already have.\n",
    "\n",
    "How many models would be created when running a grid search over the following hyperparameters and values for a GBM algorithm?\n",
    "\n",
    "learning_rate = [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 1, 2]\n",
    "max_depth = [4,6,8,10,12,14,16,18, 20]\n",
    "subsample = [0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "These lists are in your console so you can utilize properties of them to help you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:50:17.737059Z",
     "start_time": "2019-12-17T13:50:17.731030Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 1, 2] \n",
    "max_depth = [4,6,8,10,12,14,16,18, 20] \n",
    "subsample = [0.4, 0.6, 0.7, 0.8, 0.9] \n",
    "max_features = ['auto', 'sqrt', 'log2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T13:51:53.709914Z",
     "start_time": "2019-12-17T13:51:53.704889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(learning_rate)*len(max_depth)*len(subsample)*len(max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search with Scikit Learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV with Scikit Learn\n",
    "The GridSearchCV module from Scikit Learn provides many useful features to assist with efficiently undertaking a grid search. You will now put your learning into practice by creating a GridSearchCV object with certain parameters.\n",
    "\n",
    "The desired options are:\n",
    "\n",
    "A Random Forest Estimator, with the split criterion as 'entropy'\n",
    "5-fold cross validation\n",
    "The hyperparameters max_depth (2, 4, 8, 15) and max_features ('auto' vs 'sqrt')\n",
    "Use roc_auc to score the models\n",
    "Use 4 cores for processing in parallel\n",
    "Ensure you refit the best model and return training scores\n",
    "You will have available X_train, X_test, y_train & y_test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init: 2 dataframes, 2 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T14:01:49.666306Z",
     "start_time": "2019-12-17T14:01:43.603300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{pandas.core.frame.DataFrame: {\"X_test.csv\": \"https://file.io/Lprt4S\",  \"X_train.csv\": \"https://file.io/SeJEvv\"}, numpy.ndarray: {\"y_test.csv\": \"https://file.io/fJZGcM\",  \"y_train.csv\": \"https://file.io/LFExum\"}}\n",
      "{'pandas.core.frame.DataFrame': {'X_test.csv': 'https://file.io/Lprt4S', 'X_train.csv': 'https://file.io/SeJEvv'}, 'numpy.ndarray': {'y_test.csv': 'https://file.io/fJZGcM', 'y_train.csv': 'https://file.io/LFExum'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "100 10922    0 10922    0     0   6930      0 --:--:--  0:00:01 --:--:--  6930\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 15360    0 15360    0     0  10862      0 --:--:--  0:00:01 --:--:-- 10862\n",
      "100 43329    0 43329    0     0  30599      0 --:--:--  0:00:01 --:--:-- 30599\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   600    0   600    0     0    587      0 --:--:--  0:00:01 --:--:--   587\n",
      "100   600    0   600    0     0    587      0 --:--:--  0:00:01 --:--:--   587\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  2400    0  2400    0     0   1840      0 --:--:--  0:00:01 --:--:--  1841\n"
     ]
    }
   ],
   "source": [
    "from uploadfromdatacamp import saveFromFileIO\n",
    "\n",
    "#uploadToFileIO(X_train, X_test, y_train, y_test)\n",
    "tobedownloaded=\"{pandas.core.frame.DataFrame: {'X_test.csv': 'https://file.io/Lprt4S',\\\n",
    "  'X_train.csv': 'https://file.io/SeJEvv'},\\\n",
    " numpy.ndarray: {'y_test.csv': 'https://file.io/fJZGcM',\\\n",
    "  'y_train.csv': 'https://file.io/LFExum'}}\"\n",
    "prefix='data_from_datacamp/Chap2-Exercise2.1_'\n",
    "#saveFromFileIO(tobedownloaded, prefix=prefix, proxy=\"10.225.92.1:80\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T14:02:01.894425Z",
     "start_time": "2019-12-17T14:02:01.877839Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from uploadfromdatacamp import loadNDArrayFromCsv\n",
    "\n",
    "X_train=pd.read_csv(prefix+'X_train.csv',index_col=0)\n",
    "X_test=pd.read_csv(prefix+'X_test.csv',index_col=0)\n",
    "y_train = loadNDArrayFromCsv(prefix+'y_train.csv')\n",
    "y_test = loadNDArrayFromCsv(prefix+'y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T14:05:35.105210Z",
     "start_time": "2019-12-17T14:05:35.102165Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T14:05:35.509514Z",
     "start_time": "2019-12-17T14:05:35.502495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
      "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
      "                                              criterion='entropy',\n",
      "                                              max_depth=None,\n",
      "                                              max_features='auto',\n",
      "                                              max_leaf_nodes=None,\n",
      "                                              min_impurity_decrease=0.0,\n",
      "                                              min_impurity_split=None,\n",
      "                                              min_samples_leaf=1,\n",
      "                                              min_samples_split=2,\n",
      "                                              min_weight_fraction_leaf=0.0,\n",
      "                                              n_estimators='warn', n_jobs=None,\n",
      "                                              oob_score=False,\n",
      "                                              random_state=None, verbose=0,\n",
      "                                              warm_start=False),\n",
      "             iid='warn', n_jobs=4,\n",
      "             param_grid={'max_depth': [2, 4, 8, 15],\n",
      "                         'max_features': ['auto', 'sqrt']},\n",
      "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "             scoring='roc_auc', verbose=0)\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Classifier with specified criterion\n",
    "rf_class = RandomForestClassifier(criterion='entropy')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': [2,4,8,15], 'max_features': ['auto', 'sqrt']} \n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_rf_class = GridSearchCV(\n",
    "    estimator=rf_class,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    cv=5,\n",
    "    refit=True, return_train_score=True)\n",
    "print(grid_rf_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding a grid search output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the grid search results\n",
    "You will now explore the cv_results_ property of the GridSearchCV object defined in the video. This is a dictionary that we can read into a pandas DataFrame and contains a lot of useful information about the grid search we just undertook.\n",
    "\n",
    "A reminder of the different column types in this property:\n",
    "\n",
    "time_columns\n",
    "params_ columns and the params column\n",
    "test_score columns for each cv fold including the mean_test_score and std_test_score columns\n",
    "a rank_test_score column\n",
    "train_score columns for each cv fold including the mean_train_score and std_train_score columns\n",
    "We will firstly read the cv_results property into a DataFrame, then extract and explore different elements using pandas loc & iloc attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T14:17:15.541979Z",
     "start_time": "2019-12-17T14:17:12.079783Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\F279814\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                              criterion='entropy',\n",
       "                                              max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='warn', n_jobs=4,\n",
       "             param_grid={'max_depth': [2, 4, 8, 15],\n",
       "                         'max_features': ['auto', 'sqrt']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T14:18:13.014270Z",
     "start_time": "2019-12-17T14:18:12.980188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       0.013135      0.001285         0.002908        0.000737   \n",
      "1       0.012734      0.001725         0.003008        0.000634   \n",
      "2       0.013436      0.002136         0.003008        0.000634   \n",
      "3       0.013937      0.001748         0.002306        0.000602   \n",
      "4       0.014639      0.002251         0.002808        0.000511   \n",
      "5       0.016544      0.002286         0.003008        0.000549   \n",
      "6       0.018649      0.001160         0.002908        0.000737   \n",
      "7       0.014338      0.001944         0.002609        0.000735   \n",
      "\n",
      "  param_max_depth param_max_features  \\\n",
      "0               2               auto   \n",
      "1               2               sqrt   \n",
      "2               4               auto   \n",
      "3               4               sqrt   \n",
      "4               8               auto   \n",
      "5               8               sqrt   \n",
      "6              15               auto   \n",
      "7              15               sqrt   \n",
      "\n",
      "                                      params  split0_test_score  \\\n",
      "0   {'max_depth': 2, 'max_features': 'auto'}           0.782377   \n",
      "1   {'max_depth': 2, 'max_features': 'sqrt'}           0.809016   \n",
      "2   {'max_depth': 4, 'max_features': 'auto'}           0.682377   \n",
      "3   {'max_depth': 4, 'max_features': 'sqrt'}           0.762705   \n",
      "4   {'max_depth': 8, 'max_features': 'auto'}           0.778689   \n",
      "5   {'max_depth': 8, 'max_features': 'sqrt'}           0.701230   \n",
      "6  {'max_depth': 15, 'max_features': 'auto'}           0.677459   \n",
      "7  {'max_depth': 15, 'max_features': 'sqrt'}           0.625000   \n",
      "\n",
      "   split1_test_score  split2_test_score  ...  mean_test_score  std_test_score  \\\n",
      "0           0.721721           0.702761  ...         0.750651        0.055621   \n",
      "1           0.726230           0.694996  ...         0.755665        0.042849   \n",
      "2           0.657377           0.697584  ...         0.699964        0.039769   \n",
      "3           0.687295           0.739431  ...         0.738454        0.039864   \n",
      "4           0.743033           0.695858  ...         0.722954        0.042861   \n",
      "5           0.703279           0.596635  ...         0.685351        0.045259   \n",
      "6           0.590984           0.637187  ...         0.654300        0.044195   \n",
      "7           0.566393           0.609577  ...         0.627979        0.043178   \n",
      "\n",
      "   rank_test_score  split0_train_score  split1_train_score  \\\n",
      "0                2            0.780455            0.826822   \n",
      "1                1            0.777745            0.836267   \n",
      "2                5            0.898438            0.917463   \n",
      "3                3            0.896319            0.897499   \n",
      "4                4            0.979715            0.994043   \n",
      "5                6            0.982908            0.979956   \n",
      "6                7            0.999168            0.999195   \n",
      "7                8            0.999356            0.999356   \n",
      "\n",
      "   split2_train_score  split3_train_score  split4_train_score  \\\n",
      "0            0.833121            0.843331            0.805265   \n",
      "1            0.832353            0.815211            0.810462   \n",
      "2            0.887291            0.881951            0.901709   \n",
      "3            0.896880            0.880764            0.892714   \n",
      "4            0.983974            0.989079            0.990503   \n",
      "5            0.984848            0.985544            0.990371   \n",
      "6            0.997404            0.999921            0.998285   \n",
      "7            0.999020            0.999393            0.999525   \n",
      "\n",
      "   mean_train_score  std_train_score  \n",
      "0          0.817799         0.022449  \n",
      "1          0.814408         0.020786  \n",
      "2          0.897370         0.012353  \n",
      "3          0.892835         0.006261  \n",
      "4          0.987463         0.005047  \n",
      "5          0.984725         0.003424  \n",
      "6          0.998795         0.000867  \n",
      "7          0.999330         0.000167  \n",
      "\n",
      "[8 rows x 22 columns]\n",
      "                                      params\n",
      "0   {'max_depth': 2, 'max_features': 'auto'}\n",
      "1   {'max_depth': 2, 'max_features': 'sqrt'}\n",
      "2   {'max_depth': 4, 'max_features': 'auto'}\n",
      "3   {'max_depth': 4, 'max_features': 'sqrt'}\n",
      "4   {'max_depth': 8, 'max_features': 'auto'}\n",
      "5   {'max_depth': 8, 'max_features': 'sqrt'}\n",
      "6  {'max_depth': 15, 'max_features': 'auto'}\n",
      "7  {'max_depth': 15, 'max_features': 'sqrt'}\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "1       0.012734      0.001725         0.003008        0.000634   \n",
      "\n",
      "  param_max_depth param_max_features  \\\n",
      "1               2               sqrt   \n",
      "\n",
      "                                     params  split0_test_score  \\\n",
      "1  {'max_depth': 2, 'max_features': 'sqrt'}           0.809016   \n",
      "\n",
      "   split1_test_score  split2_test_score  ...  mean_test_score  std_test_score  \\\n",
      "1            0.72623           0.694996  ...         0.755665        0.042849   \n",
      "\n",
      "   rank_test_score  split0_train_score  split1_train_score  \\\n",
      "1                1            0.777745            0.836267   \n",
      "\n",
      "   split2_train_score  split3_train_score  split4_train_score  \\\n",
      "1            0.832353            0.815211            0.810462   \n",
      "\n",
      "   mean_train_score  std_train_score  \n",
      "1          0.814408         0.020786  \n",
      "\n",
      "[1 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the cv_results property into a dataframe & print it out\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "print(cv_results_df)\n",
    "\n",
    "# Get and show the column with dictionaries of the hyperparameters used\n",
    "column = cv_results_df.loc[:, ['params']]\n",
    "print(column)\n",
    "\n",
    "# Get and show the row that had the best mean test score\n",
    "best_row = cv_results_df[cv_results_df['rank_test_score'] == 1 ]\n",
    "print(best_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the best results\n",
    "The cv_results_ property has a lot of information in it. At the end of the day, we primarily care about the best square on that grid search. Luckily Scikit Learn's gridSearchCv objects also have a number of parameters that provide key information on just the best square (or row in cv_results_).\n",
    "\n",
    "These three properties are:\n",
    "\n",
    "- best_params_ – Which is a dictionary of the parameters that gave the best score.\n",
    "- best_score_ – The actual best score.\n",
    "- best_index_ – The index of the row in cv_results_ that was the best.\n",
    "\n",
    "The grid search object grid_rf_class has been loaded for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T14:24:17.783009Z",
     "start_time": "2019-12-17T14:24:17.769942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7556649410411274\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "1       0.012734      0.001725         0.003008        0.000634   \n",
      "\n",
      "  param_max_depth param_max_features  \\\n",
      "1               2               sqrt   \n",
      "\n",
      "                                     params  split0_test_score  \\\n",
      "1  {'max_depth': 2, 'max_features': 'sqrt'}           0.809016   \n",
      "\n",
      "   split1_test_score  split2_test_score  ...  mean_test_score  std_test_score  \\\n",
      "1            0.72623           0.694996  ...         0.755665        0.042849   \n",
      "\n",
      "   rank_test_score  split0_train_score  split1_train_score  \\\n",
      "1                1            0.777745            0.836267   \n",
      "\n",
      "   split2_train_score  split3_train_score  split4_train_score  \\\n",
      "1            0.832353            0.815211            0.810462   \n",
      "\n",
      "   mean_train_score  std_train_score  \n",
      "1          0.814408         0.020786  \n",
      "\n",
      "[1 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Print out the ROC_AUC score from the best grid search square\n",
    "best_score = grid_rf_class.best_score_\n",
    "print(best_score)\n",
    "\n",
    "# Recreate the best_row variable\n",
    "best_row = cv_results_df.loc[[grid_rf_class.best_index_]]\n",
    "print(best_row)\n",
    "\n",
    "# Get the n_estimators from the best grid search\n",
    "best_n_estimators = grid_rf_class.best_estimator_.get_params()['n_estimators']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the best results\n",
    "While it is interesting to analyze the results of our grid search, our final goal is practical in nature; we want to make predictions on our test set using our estimator object.\n",
    "\n",
    "We can access this object through the best_estimator_ property of our grid search object.\n",
    "\n",
    "In this exercise we will take a look inside the best_estimator_ property and then use this to make predictions on our test set for credit card defaults and generate a variety of scores. Remember to use predict_proba rather than predict since we need probability values rather than class labels for our roc_auc score. We use a slice [:,1] to get probabilities of the positive class.\n",
    "\n",
    "You have available the X_test and y_test datasets to use and the grid_rf_class object from previous exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T14:26:41.261224Z",
     "start_time": "2019-12-17T14:26:41.258215Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T14:26:41.501458Z",
     "start_time": "2019-12-17T14:26:41.487424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
      "[0. 0. 0. 0. 0.]\n",
      "Confustion Matrix \n",
      " [[73  2]\n",
      " [22  3]]\n",
      "ROC-AUC Score \n",
      " 0.7037333333333334\n"
     ]
    }
   ],
   "source": [
    "# See what type of object the best_estimator_ property is\n",
    "print(type(grid_rf_class.best_estimator_))\n",
    "\n",
    "# Create an array of predictions directly using the best_estimator_ property\n",
    "predictions = grid_rf_class.best_estimator_.predict(X_test)\n",
    "\n",
    "# Take a look to confirm it worked, this should be an array of 1's and 0's\n",
    "print(predictions[0:5])\n",
    "\n",
    "# Now create a confusion matrix \n",
    "print(\"Confustion Matrix \\n\", confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the ROC-AUC score\n",
    "predictions_proba = grid_rf_class.best_estimator_.predict_proba(X_test)[:,1]\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test, predictions_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "648px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
