{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with file systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double trouble\n",
    "The CEO at your startup has been very happy with previous Data Engineering solution you created that eliminates duplicates in a tree full of Terabytes of data. You have been tasked with another similar task of finding all of the .csv files in your company's data lake. These files will need to later move to a specific directory for a machine learning task. Your code could save hours of time if it performs as expected.\n",
    "\n",
    "In this exercise, you will search for files that match specific patterns in a directory test_dir. The os module has already been imported for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T07:45:42.687471Z",
     "start_time": "2020-06-19T07:45:42.684338Z"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T07:47:00.532684Z",
     "start_time": "2020-06-19T07:47:00.526307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: test_dir/file_0.csv\n",
      "Processing file: test_dir/file_1.csv\n",
      "Processing file: test_dir/file_2.csv\n",
      "Processing file: test_dir/file_3.csv\n",
      "Processing file: test_dir/file_4.csv\n",
      "Processing file: test_dir/file_5.csv\n",
      "Processing file: test_dir/file_6.csv\n",
      "Processing file: test_dir/file_7.csv\n",
      "Processing file: test_dir/file_8.csv\n",
      "Processing file: test_dir/file_9.csv\n",
      "['test_dir/file_0.csv', 'test_dir/file_1.csv', 'test_dir/file_2.csv', 'test_dir/file_3.csv', 'test_dir/file_4.csv', 'test_dir/file_5.csv', 'test_dir/file_6.csv', 'test_dir/file_7.csv', 'test_dir/file_8.csv', 'test_dir/file_9.csv']\n"
     ]
    }
   ],
   "source": [
    "matches = []\n",
    "# Walk the filesystem starting at the test_dir\n",
    "for root, _, files in os.walk('test_dir'):\n",
    "    for name in files:\n",
    "      \t# Create the full path to the file by using os.path.join()\n",
    "        fullpath = os.path.join(root, name)\n",
    "        print(f\"Processing file: {fullpath}\")\n",
    "        # Split off the extension and discard the rest of the path\n",
    "        _, ext = os.path.splitext(fullpath)\n",
    "        # Match the extension pattern .csv\n",
    "        if ext == \".csv\":\n",
    "            matches.append(fullpath)\n",
    "            \n",
    "# Print the matches you find          \n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y'all got some renaming to do\n",
    "After escaping the oppressive rent of the Bay Area by moving to Texas, you got a job at a dude ranch as their only programmer. Once a year they sell all of their cattle in an auction. You wrote their inventory control system from scratch in Python and the CEO said, \"Not bad for a kid from 'Frisco'\". The day before the auction the CEO comes up to you in a panic because the names of all of the cattle are wrong in the inventory control system. The CEO tells you, \"Y'all got some renaming to do!\".\n",
    "\n",
    "longhorn\n",
    "\n",
    "Rename all of the files in the cattle directory by replacing the phrase 'shorthorn' with 'longhorn'. The os and pathlib modules have been imported for you. Remember that the name variable will need to be split to be renamed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T07:59:19.859026Z",
     "start_time": "2020-06-19T07:59:18.838341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargements à lancer\n",
      "{'numpy.ndarray': {'cattle.tar.gz': 'https://file.io/QUWGSDa8'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   278    0   278    0     0    457      0 --:--:-- --:--:-- --:--:--   456\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### sur datacamp\n",
    "\n",
    "\"\"\"\n",
    "!zip -r cattle.zip cattle\n",
    "!tar zcvf cattle.tar.gz cattle\n",
    "\"\"\"\n",
    "\n",
    "###################\n",
    "##### file\n",
    "###################\n",
    "\n",
    "#upload and download\n",
    "\n",
    "from downloadfromFileIO import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO_pushto_fileio('cattle.tar.gz')\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{numpy.ndarray: {'cattle.tar.gz': 'https://file.io/QUWGSDa8'}}\n",
    "\"\"\"\n",
    "prefixToc = '1.2'\n",
    "prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:02:04.158987Z",
     "start_time": "2020-06-19T08:02:04.156318Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:05:45.040636Z",
     "start_time": "2020-06-19T08:05:45.028219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: data_from_datacamp/cattle/beefy/large/george_shorthorn\n",
      "Renaming file george_shorthorn to george_longhorn\n",
      "Processing file: data_from_datacamp/cattle/beefy/large/john_shorthorn\n",
      "Renaming file john_shorthorn to john_longhorn\n",
      "Processing file: data_from_datacamp/cattle/beefy/large/paul_shorthorn\n",
      "Renaming file paul_shorthorn to paul_longhorn\n",
      "Processing file: data_from_datacamp/cattle/beefy/large/ringo_shorthorn\n",
      "Renaming file ringo_shorthorn to ringo_longhorn\n"
     ]
    }
   ],
   "source": [
    "# Walk the filesystem starting at the test_dir\n",
    "for root, _, files in os.walk('data_from_datacamp/'+'cattle'):\n",
    "    for name in files:\n",
    "      \t\n",
    "        # Create the full path to the file by using os.path.join()\n",
    "        fullpath = os.path.join(root, name)\n",
    "        print(f\"Processing file: {fullpath}\")\n",
    "        \n",
    "        # Rename file\n",
    "        if \"shorthorn\" in name:\n",
    "            p = pathlib.Path(fullpath)\n",
    "            shortname = name.split(\"_\")[0] # You need to split the name by underscore\n",
    "            new_name = f\"{shortname}_longhorn\"\n",
    "            print(f\"Renaming file {name} to {new_name}\")\n",
    "            p.rename(new_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweet pickle\n",
    "\"It was the best of times, it was the worst of times..\", Charles Dickens said in a Tale of Two Cities. He could also be talking about your startup. Initially things were amazing and you and your co-workers laughed in delight as the CTO churned out machine learning models dozens by the day. Often this would be at 2AM and you would arrive in the morning and find the serialized sklearn models waiting for the Data Science team to deploy to production.\n",
    "\n",
    "Unfortunately, this was in fact too good to be true. Many of the models had serious flaws and this ultimately led to the CTO stepping down. IT Auditors want to determine how flawed these ML models were and back test the predictions for accuracy.\n",
    "\n",
    "Use the os.walk module to find serialized models and test them for accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:08:50.535445Z",
     "start_time": "2020-06-19T08:08:46.978776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargements à lancer\n",
      "{'numpy.ndarray': {'my.tar.gz': 'https://file.io/16RyfkJp'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 57324    0 57324    0     0  16477      0 --:--:--  0:00:03 --:--:-- 16477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### sur datacamp\n",
    "\n",
    "\"\"\"\n",
    "!tar zcvf my.tar.gz my\n",
    "\"\"\"\n",
    "\n",
    "###################\n",
    "##### file\n",
    "###################\n",
    "\n",
    "#upload and download\n",
    "\n",
    "from downloadfromFileIO import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO_pushto_fileio('my.tar.gz')\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{numpy.ndarray: {'my.tar.gz': 'https://file.io/16RyfkJp'}}\n",
    "\"\"\"\n",
    "prefixToc = '1.3'\n",
    "prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:13:55.281462Z",
     "start_time": "2020-06-19T08:13:53.469796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargements à lancer\n",
      "{'numpy.ndarray': {'X_digits.csv': 'https://file.io/by0qFE2F'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  673k    0  673k    0     0   406k      0 --:--:--  0:00:01 --:--:--  406k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "##### numpy ndarray float\n",
    "###################\n",
    "\n",
    "#upload and download\n",
    "\n",
    "from downloadfromFileIO import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO(X_digits)\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    " {numpy.ndarray: {'X_digits.csv': 'https://file.io/by0qFE2F'}}\n",
    " \"\"\"\n",
    "prefixToc='1.3'\n",
    "prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=\"\")\n",
    "\n",
    "#initialisation\n",
    "\n",
    "from downloadfromFileIO import loadNDArrayFromCsv\n",
    "X_digits = loadNDArrayFromCsv(prefix+'X_digits.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:16:08.659215Z",
     "start_time": "2020-06-19T08:16:08.656205Z"
    }
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:16:09.725840Z",
     "start_time": "2020-06-19T08:16:09.709437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: data_from_datacamp/my/models/anotherbadmodel.junk\n",
      "Processing file: data_from_datacamp/my/models/badmodel.alsojunk\n",
      "Processing file: data_from_datacamp/my/models/good/digits_prediction.joblib\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.externals.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d56b3182d552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Match the extension pattern .joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".joblib\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d075/lib/python3.8/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    583\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d075/lib/python3.8/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[0;32m~/anaconda3/envs/d075/lib/python3.8/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1208\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1210\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1211\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d075/lib/python3.8/pickle.py\u001b[0m in \u001b[0;36mload_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1525\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1526\u001b[0;31m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1527\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1528\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGLOBAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d075/lib/python3.8/pickle.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1575\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compat_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPORT_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1577\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1578\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_getattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.externals.joblib'"
     ]
    }
   ],
   "source": [
    "# Walk the filesystem starting at the my path\n",
    "for root, _, files in os.walk('data_from_datacamp/'+'my'):\n",
    "    for name in files:\n",
    "      \t# Create the full path to the file by using os.path.join()\n",
    "        fullpath = os.path.join(root, name)\n",
    "        print(f\"Processing file: {fullpath}\")\n",
    "        _, ext = os.path.splitext(fullpath)\n",
    "        # Match the extension pattern .joblib\n",
    "        if ext == \".joblib\":\n",
    "            clf = joblib.load(fullpath)\n",
    "            break\n",
    "\n",
    "# Predict from pickled model\n",
    "print(clf.predict(X_digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find files matching a pattern\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rogue founder code\n",
    "Working as employee number 10 at a small startup with millions in funding seemed like a dream job. Now it seems like a nightmare. There are constant outages in production and during the middle of those outages one of the founders builds Java .jar files on their laptop and via ssh loads the .jar files into production servers thinking this will fix the problem. You have mentioned that all software should go through a continuous deployment system.\n",
    "\n",
    "After a several day continuous outage that caused permanent customer data loss caused by the founder's rogue coding practices, the founder finally listens to you. They ask you to help them identify all of the .jar files located on servers in the prod directory. Make sure you use the powerful recursive glob technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:24:58.907301Z",
     "start_time": "2020-06-19T08:24:58.207709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargements à lancer\n",
      "{'numpy.ndarray': {'prod.tar.gz': 'https://file.io/sOhWKx9k'}}\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   209    0   209    0     0    334      0 --:--:-- --:--:-- --:--:--   334\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### sur datacamp\n",
    "\n",
    "\"\"\"\n",
    "!tar zcvf prod.tar.gz prod\n",
    "\"\"\"\n",
    "\n",
    "###################\n",
    "##### file\n",
    "###################\n",
    "\n",
    "#upload and download\n",
    "\n",
    "from downloadfromFileIO import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO_pushto_fileio('prod.tar.gz')\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{numpy.ndarray: {'prod.tar.gz': 'https://file.io/sOhWKx9k'}}\n",
    "\"\"\"\n",
    "prefixToc = '2.1'\n",
    "prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:27:53.592503Z",
     "start_time": "2020-06-19T08:27:53.585896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found rogue .jar file in production: data_from_datacamp/prod/hope_this_works.jar\n",
      "Found rogue .jar file in production: data_from_datacamp/prod/no_no_no.jar\n",
      "Found rogue .jar file in production: data_from_datacamp/prod/please_please_work.jar\n",
      "Found rogue .jar file in production: data_from_datacamp/prod/why_me.jar\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import os\n",
    "\n",
    "path = pathlib.Path('data_from_datacamp/'+'prod')\n",
    "matches = sorted(path.glob('*.jar'))\n",
    "for match in matches:\n",
    "  print(f\"Found rogue .jar file in production: {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this pattern True?\n",
    "As the head of data science you often get called in to help a new data scientists track down an important intermediate csv file that has gone missing. This has taken so much time, that you have decided to write an automated script that identify all csv files that are created by a user and then copy them to centralized storage. As the first step to create this near line backup solution you have to write a function that can filter and return only csv matches.\n",
    "\n",
    "Use the fnmatch.filter function to filter for csv files from a list of files. Make sure you write a Python function so it can be portable code that a larger system can be built from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:31:03.931918Z",
     "start_time": "2020-06-19T08:31:03.927085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found matches: ['data1.csv', 'data2.csv']\n"
     ]
    }
   ],
   "source": [
    "import fnmatch\n",
    "\n",
    "# List of file names to process\n",
    "files = [\"data1.csv\", \"script.py\", \"image.png\", \"data2.csv\", \"all.py\"]\n",
    "\n",
    "# Function that returns \n",
    "def csv_matches(list_of_files):\n",
    "    \"\"\"Return matches for csv files\"\"\"\n",
    "\n",
    "    matches = fnmatch.filter(list_of_files, \"*.csv\")\n",
    "    return matches\n",
    "\n",
    "# Call function to find matches\n",
    "matches = csv_matches(files)\n",
    "print(f\"Found matches: {matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level file and directory operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goons over my shammy\n",
    "You are many light years from earth. Python programming aliens (Goons) abducted you and are forcing you to wash computer monitors. Goons keep stepping on your shammy (towel) and stopping you from working. You tell the Goons if they quit stepping on your shammy, you will write a Python script that programmatically creates self-destructing files and directories.\n",
    "\n",
    "One of the uses for self-destructing files is to create integration tests. Integration tests can use temporary directory and files as a way of validating that processes in an application is doing what is expected. They tell you, \"you had us at self-destruction\". The tempfile and os module have been imported for you. Remember that tempfile.NameTemporaryFile object has many useful methods on it including .name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:36:53.544870Z",
     "start_time": "2020-06-19T08:36:53.541745Z"
    }
   },
   "outputs": [],
   "source": [
    "import tempfile, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:39:48.773060Z",
     "start_time": "2020-06-19T08:39:48.765678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp file created: /tmp/tmp9roqmt6z\n",
      "b'This message will self-destruct in 5....4...\\n'\n",
      "self-destruction verified: /tmp/tmp9roqmt6z\n"
     ]
    }
   ],
   "source": [
    "# Create a self-destructing temporary file\n",
    "with tempfile.NamedTemporaryFile() as exploding_file:\n",
    "  \t# This file will be deleted automatically after the with statement block\n",
    "    print(f\"Temp file created: {exploding_file.name}\")\n",
    "    exploding_file.write(b\"This message will self-destruct in 5....4...\\n\")\n",
    "    \n",
    "    # Get to the top of the file\n",
    "    exploding_file.seek(0)\n",
    "\n",
    "    #Print the message\n",
    "    print(exploding_file.read())\n",
    "\n",
    "# Check to sure file self-destructed\n",
    "if not os.path.exists(exploding_file.name): \n",
    "    print(f\"self-destruction verified: {exploding_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive Users\n",
    "At your university they have hired you to be an assistance for the machine learning course. It has been a very rewarding job, but some parts of the job are frustrating. You get frequent requests to retrieve items from a user's project after the course is over. This has consumed much of your time. You believe you can write an automation script that will archive all user folders and email them the archived copy. If you can do this, it will eliminate about 80% of the work you perform each semester. You are hoping to spend the recovered time helping the lead instructor deliver machine learning content. Use the shutil.archive function to archive a user directory. You will create two archive types: gztar and zip. make_archive and rmtree have been imported for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:41:39.935106Z",
     "start_time": "2020-06-19T08:41:39.932472Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:45:01.105708Z",
     "start_time": "2020-06-19T08:45:01.076719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user1.tar.gz', 'user1.zip']\n"
     ]
    }
   ],
   "source": [
    "username = \"user1\"\n",
    "root_dir = \"/tmp\"\n",
    "# archive root\n",
    "apath = \"/tmp/archive\"\n",
    "# archive base\n",
    "final_archive_base = f\"{apath}/{username}\"\n",
    "\n",
    "# create tar and gzipped archive\n",
    "shutil.make_archive(final_archive_base, 'gztar', apath)\n",
    "\n",
    "# create zip archive\n",
    "shutil.make_archive(final_archive_base, 'zip', apath)\n",
    "\n",
    "# print out archives\n",
    "print(os.listdir(apath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pathlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does it even exist?\n",
    "Your a social media company with a huge problem. Your NoSQL database cluster was upgraded to 1.0.0-beta because it had some really cool new features. Around this same time files started disappearing in production and social media posts were suddenly vanishing. It turns out the beta version of the database was actually deleting data between cluster nodes, not syncing data. Even worse, the backups were never tested and the same backup from a year previous was being run over and over again. You have a list of all social media posts that should exist in production, and you need to write a script that audits which files actually exist.\n",
    "\n",
    "Write a script using pathlib that validates if a list of files exists on disk. Remember you can explore pathlib.Path in IPython."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T08:56:56.026922Z",
     "start_time": "2020-06-19T08:56:55.348725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargements à lancer\n",
      "{'numpy.ndarray': {'socialposts.tar.gz': 'https://file.io/iwg30QD8'}}\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['curl', '-q', 'https://file.io/iwg30QD8', '--output', 'data_from_datacamp/chapter 3-Exercise4.1_socialposts.tar.gz']' returned non-zero exit status 23.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6f9f6707711e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \"\"\"\n\u001b[1;32m     21\u001b[0m \u001b[0mprefixToc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'4.1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaveFromFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtobedownloaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefixToc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefixToc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/d/git/data-scientist-skills/python-sandbox/command-line-automation-in-python/downloadfromFileIO.py\u001b[0m in \u001b[0;36msaveFromFileIO\u001b[0;34m(dict_urls, prefix, proxy, prefixToc, forceDownload)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m#print(prefix+filename, url)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;31m#print('#debug', ['curl', curl_proxy_option, url, '--output',prefix+filename])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'curl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurl_proxy_option\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--output'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_lock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d075/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'universal_newlines'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    412\u001b[0m                **kwargs).stdout\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d075/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    513\u001b[0m                                      output=stdout, stderr=stderr)\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['curl', '-q', 'https://file.io/iwg30QD8', '--output', 'data_from_datacamp/chapter 3-Exercise4.1_socialposts.tar.gz']' returned non-zero exit status 23."
     ]
    }
   ],
   "source": [
    "### sur datacamp\n",
    "\n",
    "\"\"\"\n",
    "!tar zcvf socialposts.tar.gz *\n",
    "\"\"\"\n",
    "\n",
    "###################\n",
    "##### file\n",
    "###################\n",
    "\n",
    "#upload and download\n",
    "\n",
    "from downloadfromFileIO import saveFromFileIO\n",
    "\"\"\" à executer sur datacamp: (apres copie du code uploadfromdatacamp.py)\n",
    "uploadToFileIO_pushto_fileio('socialposts.tar.gz')\n",
    "\"\"\"\n",
    "\n",
    "tobedownloaded=\"\"\"\n",
    "{numpy.ndarray: {'socialposts.tar.gz': 'https://file.io/iwg30QD8'}}\n",
    "\"\"\"\n",
    "prefixToc = '4.1'\n",
    "prefix = saveFromFileIO(tobedownloaded, prefixToc=prefixToc, proxy=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T09:02:15.118828Z",
     "start_time": "2020-06-19T09:02:15.109199Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_from_datacamp/posts_index.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-a3cd240c2821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Read the index of social media posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_from_datacamp/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"posts_index.txt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_from_datacamp/posts_index.txt'"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "# Read the index of social media posts\n",
    "with open('data_from_datacamp/'+\"posts_index.txt\") as posts:\n",
    "  for post in posts.readlines():\n",
    "    \n",
    "    # Create a pathlib object\n",
    "    path = pathlib.Path(post.strip())\n",
    "    \n",
    "    # Check if the social media post still exists on disk\n",
    "    if path.exists():\n",
    "      print(f\"Found active post: {post}\")\n",
    "    else:\n",
    "      print(f\"Post is missing: {post}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File writing one-liner\n",
    "As a Data Engineer at a Fortune 500 company you need to make sure your large scale data pipeline is running smoothly. Recently you have been experienced unpredictable errors when running Spark Python jobs. You want to write an integration test that programatically creates Python files, gives them executable permission and the runs them. You want to run this everytime you create IaC (Infrastructure as Code) scripts that provision new Spark clusters.\n",
    "\n",
    "Create an integration script that creates several Python files and writes Python to them. After that run them all with python3 and subprocess to obtain the scripts' output. The Path module is imported for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T09:04:52.115585Z",
     "start_time": "2020-06-19T09:04:52.113347Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T09:05:29.295646Z",
     "start_time": "2020-06-19T09:05:29.279423Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/test/file_0.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c9792969fe0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"/tmp/test/file_{i}.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#!/usr/bin/env python\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"import datetime;print(datetime.datetime.now())\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d075/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36mwrite_text\u001b[0;34m(self, data, encoding, errors)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             raise TypeError('data must be str, not %s' %\n\u001b[1;32m   1250\u001b[0m                             data.__class__.__name__)\n\u001b[0;32m-> 1251\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d075/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m         return io.open(self, mode, buffering, encoding, errors, newline,\n\u001b[0m\u001b[1;32m   1219\u001b[0m                        opener=self._opener)\n\u001b[1;32m   1220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/d075/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[0;34m(self, name, flags, mode)\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o666\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0o777\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/test/file_0.py'"
     ]
    }
   ],
   "source": [
    "from subprocess import run, PIPE\n",
    "\n",
    "# Find all the python files you created and print them out\n",
    "for i in range(3):\n",
    "  path = Path(f\"/tmp/test/file_{i}.py\")\n",
    "  path.write_text(\"#!/usr/bin/env python\\n\")\n",
    "  path.write_text(\"import datetime;print(datetime.datetime.now())\")\n",
    "  \n",
    "\n",
    "# Find all the python files you created and print them out\n",
    "for file in Path(\"/tmp/test/\").glob(\"*.py\"):\n",
    "  # gets the resolved full path\n",
    "  fullpath = str(file.resolve())\n",
    "  proc = run([\"python3\", fullpath], stdout=PIPE)\n",
    "  print(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
