The sequential API - 
	The sequential API
		Input layer
		Hidden layers
		Output layer
		Ordered in sequence

Building a sequential model
	# Import tensorflow
	from tensorflow import keras
	# Define a sequential model
	model = keras.Sequential()
	# Define first hidden layer
	model.add(keras.layers.Dense(16, activation='relu', input_shape=(28*28,)))
	# Define second hidden layer
	model.add(keras.layers.Dense(8, activation='relu'))
	# Define output layer
	model.add(keras.layers.Dense(4, activation='softmax'))
	# Compile the model
	model.compile('adam', loss='categorical_crossentropy')
	# Summarize the model
	print(model.summary())

Using the functional API
	# Import tensorflow
	import tensorflow as tf
	# Define model 1 input layer shape
	model1_inputs = tf.keras.Input(shape=(28*28,))
	# Define model 2 input layer shape
	model2_inputs = tf.keras.Input(shape=(10,))
	# Define layer 1 for model 1
	model1_layer1 = tf.keras.layers.Dense(12, activation='relu')(model1_inputs)
	# Define layer 2 for model 1
	model1_layer2 = tf.keras.layers.Dense(4, activation='softmax')(model1_layer1)
	# Define layer 1 for model 2
	model2_layer1 = tf.keras.layers.Dense(8, activation='relu')(model2_inputs)
	# Define layer 2 for model 2
	model2_layer2 = tf.keras.layers.Dense(4, activation='softmax')(model2_layer1)
	# Merge model 1 and model 2
	merged = tf.keras.layers.add([model1_layer2, model2_layer2])
	# Define a functional model
	model = tf.keras.Model(inputs=[model1_inputs, model2_inputs], outputs=merged)
	# Compile the model
	model.compile('adam', loss='categorical_crossentropy')

Exercise - The sequential model in Keras

In chapter 3, we used components of the keras API in tensorflow to define a neural network, but we stopped short of using its full capabilities to streamline model definition and training. In this exercise, you will use the keras sequential model API to define a neural network that can be used to classify images of sign language letters. You will also use the .summary() method to print the model's architecture, including the shape and number of parameters associated with each layer.

Note that the images were reshaped from (28, 28) to (784,), so that they could be used as inputs to a dense layer. Additionally, note that keras has been imported from tensorflow for you.

		# Define a Keras sequential model
		model = keras.Sequential()
		# Define the first dense layer
		model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))

		# Define the second dense layer
		model.add(keras.layers.Dense(8, activation='relu',))

		# Define the output layer
		model.add(keras.layers.Dense(4, activation='softmax'))

		# Print the model architecture
		print(model.summary())

Layer (type)                 Output Shape              Param #   
=================================================================
dense_2 (Dense)              (None, 16)                12560     
_________________________________________________________________
dense_3 (Dense)              (None, 8)                 136       
_________________________________________________________________
dense_4 (Dense)              (None, 4)                 36        
=================================================================
Total params: 12,732
Trainable params: 12,732
Non-trainable params: 0
________________________________________________________________
		
Excellent work! Notice that we've defined a model, but we haven't compiled it. The compilation step in keras allows us to set the optimizer, loss function, and other useful training parameters in a single line of code. Furthermore, the .summary() method allows us to view the model's architecture.

Exercise - Compiling a sequential model

In this exercise, you will work towards classifying letters from the Sign Language MNIST dataset; however, you will adopt a different network architecture than what you used in the previous exercise. There will be fewer layers, but more nodes. You will also apply dropout to prevent overfitting. Finally, you will compile the model to use the adam optimizer and the categorical_crossentropy loss. You will also use a method in keras to summarize your model's architecture. Note that keras has been imported from tensorflow for you and a sequential keras model has been defined as model.

		# Define the first dense layer
		model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))

		# Apply dropout to the first layer's output
		model.add(keras.layers.Dropout(0.25))

		# Define the output layer
		model.add(keras.layers.Dense(4, activation='softmax'))

		# Compile the model
		model.compile('adam', loss='categorical_crossentropy')

		# Print a model summary
		print(model.summary())

Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_7 (Dense)              (None, 16)                12560     
_________________________________________________________________
dropout_1 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_8 (Dense)              (None, 4)                 68        
=================================================================
Total params: 12,628
Trainable params: 12,628
Non-trainable params: 0
_________________________________________________________________
None

Great work! You've now defined and compiled a neural network using the keras sequential model. Notice that printing the .summary() method shows the layer type, output shape, and number of parameters of each layer.

Exercise - Defining a multiple input model

In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this. In this exercise, we will see how to do this. We will also use the .summary() method to examine the joint model's architecture.

Note that keras has been imported from tensorflow for you. Additionally, the input layers of the first and second models have been defined as m1_inputs and m2_inputs, respectively. Note that the two models have the same architecture, but one of them uses a sigmoid activation in the first layer and the other uses a relu.

		# For model 1, pass the input layer to layer 1 and layer 1 to layer 2
		m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)
		m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)

		# For model 2, pass the input layer to layer 1 and layer 1 to layer 2
		m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)
		m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)

		# Merge model outputs and define a functional model
		merged = keras.layers.add([m1_layer2, m2_layer2])
		model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)

		# Print a model summary
		print(model.summary())

Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 784)]        0                                            
__________________________________________________________________________________________________
input_4 (InputLayer)            [(None, 784)]        0                                            
__________________________________________________________________________________________________
dense_13 (Dense)                (None, 12)           9420        input_3[0][0]                    
__________________________________________________________________________________________________
dense_15 (Dense)                (None, 12)           9420        input_4[0][0]                    
__________________________________________________________________________________________________
dense_14 (Dense)                (None, 4)            52          dense_13[0][0]                   
__________________________________________________________________________________________________
dense_16 (Dense)                (None, 4)            52          dense_15[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 4)            0           dense_14[0][0]                   
                                                                 dense_16[0][0]                   
==================================================================================================
Total params: 18,944
Trainable params: 18,944
Non-trainable params: 0
__________________________________________________________________________________________________
None

Nice work! Notice that the .summary() method yields a new column: connected to. This column tells you how layers connect to each other within the network. We can see that dense_2, for instance, is connected to the input_2 layer. We can also see that the add layer, which merged the two models, connected to both dense_1 and dense_3.

Overview of training and evaluation
	1. Load and clean data
	2. Define model
	3. Train and validate model
	4. Evaluate model
	
	How to train a model
	# Import tensorflow
	import tensorflow as tf
	# Define a sequential model
	model = tf.keras.Sequential()
	# Define the hidden layer
	model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(784,)))
	# Define the output layer
	model.add(tf.keras.layers.Dense(4, activation='softmax'))
	# Compile model
	model.compile('adam', loss='categorical_crossentropy')
	# Train model
	model.fit(image_features, image_labels)

	Performing validation
	# Train model with validation split
	model.fit(features, labels, epochs=10, validation_split=0.20)

	Changing the metric
	# Recomile the model with the accuracy metric
	model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])
	# Train model with validation split
	model.fit(features, labels, epochs=10, validation_split=0.20)

	The evaluation() operation
	# Evaluate the test set
	model.evaluate(test)

	
Exercise - Training with Keras

In this exercise, we return to our sign language letter classification problem. We have 2000 images of four letters--A, B, C, and D--and we want to classify them with a high level of accuracy. We will complete all parts of the problem, including the model definition, compilation, and training.

Note that keras has been imported from tensorflow for you. Additionally, the features are available as sign_language_features and the targets are available as sign_language_labels.

		# Define a sequential model
		model = tf.keras.Sequential()

		# Define a hidden layer
		model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))

		# Define the output layer
		model.add(tf.keras.layers.Dense(4, activation='softmax'))

		# Compile the model
		model.compile('SGD', loss='categorical_crossentropy')

		# Complete the fitting operation
		model.fit(sign_language_features, sign_language_labels, epochs=5)


Great work! You probably noticed that your only measure of performance improvement was the value of the loss function in the training sample, which is not particularly informative. You will improve on this in the next exercise.

Train on 1999 samples
Epoch 1/5
1999/1999 [==============================] - 0s 110us/sample - loss: 1.3613
Epoch 2/5
1999/1999 [==============================] - 0s 40us/sample - loss: 1.2260
Epoch 3/5
1999/1999 [==============================] - 0s 35us/sample - loss: 1.0694
Epoch 4/5
1999/1999 [==============================] - 0s 35us/sample - loss: 0.8991
Epoch 5/5
1999/1999 [==============================] - 0s 35us/sample - loss: 0.7495
Out[11]: <tensorflow.python.keras.callbacks.History at 0x15704240>

Exercise - Metrics and validation with Keras

We trained a model to predict sign language letters in the previous exercise, but it is unclear how successful we were in doing so. In this exercise, we will try to improve upon the interpretability of our results. Since we did not use a validation split, we only observed performance improvements within the training set; however, it is unclear how much of that was due to overfitting. Furthermore, since we did not supply a metric, we only saw decreases in the loss function, which do not have any clear interpretation.

Note that keras has been imported for you from tensorflow.		

		# Define sequential model
		model = keras.Sequential()

		# Define the first layer
		model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))

		# Add activation function to classifier
		model.add(keras.layers.Dense(4, activation='softmax'))

		# Set the optimizer, loss function, and metrics
		model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])

		# Add the number of epochs and the validation split
		model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.1)

Nice work! With the keras API, you only needed 14 lines of code to define, compile, train, and validate a model. You may have noticed that your model performed quite well. In just 10 epochs, we achieved a classification accuracy of around 98% in the validation sample!

Train on 1799 samples, validate on 200 samples
Epoch 1/10
1799/1799 [==============================] - 0s 160us/sample - loss: 1.1564 - accuracy: 0.5225 - val_loss: 1.0957 - val_accuracy: 0.5150
Epoch 2/10
1799/1799 [==============================] - 0s 50us/sample - loss: 0.8116 - accuracy: 0.7715 - val_loss: 0.7550 - val_accuracy: 0.6750
Epoch 3/10
1799/1799 [==============================] - 0s 44us/sample - loss: 0.6114 - accuracy: 0.8621 - val_loss: 0.5705 - val_accuracy: 0.8050
Epoch 4/10
1799/1799 [==============================] - 0s 50us/sample - loss: 0.4731 - accuracy: 0.9066 - val_loss: 0.4033 - val_accuracy: 0.9600
Epoch 5/10
1799/1799 [==============================] - 0s 50us/sample - loss: 0.3622 - accuracy: 0.9500 - val_loss: 0.4058 - val_accuracy: 0.8200
Epoch 6/10
1799/1799 [==============================] - 0s 44us/sample - loss: 0.2902 - accuracy: 0.9489 - val_loss: 0.3609 - val_accuracy: 0.8000
Epoch 7/10
1799/1799 [==============================] - 0s 44us/sample - loss: 0.2358 - accuracy: 0.9622 - val_loss: 0.2070 - val_accuracy: 0.9850
Epoch 8/10
1799/1799 [==============================] - 0s 44us/sample - loss: 0.1889 - accuracy: 0.9778 - val_loss: 0.2602 - val_accuracy: 0.8750
Epoch 9/10
1799/1799 [==============================] - 0s 44us/sample - loss: 0.1589 - accuracy: 0.9767 - val_loss: 0.1492 - val_accuracy: 0.9850
Epoch 10/10
1799/1799 [==============================] - 0s 44us/sample - loss: 0.1297 - accuracy: 0.9844 - val_loss: 0.4145 - val_accuracy: 0.7750

Exercise - Overfitting detection

In this exercise, we'll work with a small subset of the examples from the original sign language letters dataset. A small sample, coupled with a heavily-parameterized model, will generally lead to overfitting. This means that your model will simply memorize the class of each example, rather than identifying features that generalize to many examples.

You will detect overfitting by checking whether the validation sample loss is substantially higher than the training sample loss and whether it increases with further training. With a small sample and a high learning rate, the model will struggle to converge on an optimum. You will set a low learning rate for the optimizer, which will make it easier to identify overfitting.

Note that keras has been imported from tensorflow.

		# Define sequential model
		model = keras.Sequential()

		# Define the first layer
		model.add(keras.layers.Dense(1024, activation='relu',input_shape=(784,)))

		# Add activation function to classifier
		model.add(keras.layers.Dense(4, activation='softmax'))

		# Finish the model compilation
		model.compile(optimizer=keras.optimizers.Adam(lr=0.01), 
					  loss='categorical_crossentropy', metrics=['accuracy'])

		# Complete the model fit operation
		model.fit(sign_language_features, sign_language_labels, epochs=200, validation_split=0.5)

   Train on 25 samples, validate on 25 samples
    Epoch 1/200
    
25/25 [==============================] - 0s 16ms/sample - loss: 1.5469 - accuracy: 0.2000 - val_loss: 48.8668 - val_accuracy: 0.2400
    Epoch 2/200
25/25 [==============================] - 0s 373us/sample - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.5236 - val_accuracy: 0.8400

Excellent work! You may have noticed that the validation loss, val_loss, was substantially higher than the training loss, loss. Furthermore, if val_loss started to increase before the training process was terminated, then we may have overfitted. When this happens, you will want to try decreasing the number of epochs.

Exercise - 	Evaluating models

Two models have been trained and are available: large_model, which has many parameters; and small_model, which has fewer parameters. Both models have been trained using train_features and train_labels, which are available to you. A separate test set, which consists of test_features and test_labels, is also available.

Your goal is to evaluate relative model performance and also determine whether either model exhibits signs of overfitting. You will do this by evaluating large_model and small_model on both the train and test sets. For each model, you can do this by applying the .evaluate(x, y) method to compute the loss for features x and labels y. You will then compare the four losses generated.

		# Evaluate the small model using the train data
		small_train = small_model.evaluate(train_features, train_labels)

		# Evaluate the small model using the test data
		small_test = small_model.evaluate(test_features, test_labels)

		# Evaluate the large model using the train data
		large_train = large_model.evaluate(train_features, train_labels)

		# Evaluate the large model using the test data
		large_test = large_model.evaluate(test_features, test_labels)

		# Print losses
		print('\n Small - Train: {}, Test: {}'.format(small_train, small_test))
		print('Large - Train: {}, Test: {}'.format(large_train, large_test))

Small - Train: 1.0207647275924683, Test: 1.1109790706634521
    Large - Train: 0.04989723406732082, Test: 0.20585484504699708
	
Great job! Notice that the gap between the test and train set losses is substantially higher for large_model, suggesting that overfitting may be an issue. Furthermore, both test and train set performance is better for large_model. This suggests that we may want to use large_model, but reduce the number of training epochs.

Estimators - Model specification and training
	1. Define feature columns
	2. Load and transform data
	3. Define an estimator
	4. Apply train operation

Define feature columns	
	# Import tensorflow under its standard alias
	import tensorflow as tf
	# Define a numeric feature column
	size = tf.feature_column.numeric_column("size")
	# Define a categorical feature column
	rooms = tf.feature_column.categorical_column_with_vocabulary_list("rooms",["1", "2", "3", "4", "5"])
	# Create feature column list
	features_list = [size, rooms]
	# Define a matrix feature column
	features_list = [tf.feature_column.numeric_column('image', shape=(784,))]
Loading and transforming data
	# Define input data function
	def input_fn():
		# Define feature dictionary
		features = {"size": [1340, 1690, 2720], "rooms": [1, 3, 4]}
		# Define labels
		labels = [221900, 538000, 180000]
		return features, labels	
Define and train a regression estimator		
	# Define a deep neural network regression
	model0 = tf.estimator.DNNRegressor(feature_columns=feature_list,hidden_units=[10, 6, 6, 3])
	# Train the regression model
	model0.train(input_fn, steps=20)		
Define and train a deep neural network
	# Define a deep neural network classifier
	model1 = tf.estimator.DNNClassifier(feature_columns=feature_list, hidden_units=[32, 16, 8], n_classes=4)
	# Train the classifier
	model1.train(input_fn, steps=20)	
	
Exercise - Preparing to train with Estimators

For this exercise, we'll return to the King County housing transaction dataset from chapter 2. We will again develop and train a machine learning model to predict house prices; however, this time, we'll do it using the estimator API.

Rather than completing everything in one step, we'll break this procedure down into parts. We'll begin by defining the feature columns and loading the data. In the next exercise, we'll define and train a premade estimator. Note that feature_column has been imported for you from tensorflow. Additionally, numpy has been imported as np, and the Kings County housing dataset is available as a pandas DataFrame: housing.

		# Define feature columns for bedrooms and bathrooms
		bedrooms = feature_column.numeric_column("bedrooms")
		bathrooms = feature_column.numeric_column("bathrooms")

		# Define the list of feature columns
		feature_list = [bedrooms, bathrooms]

		def input_fn():
			# Define the labels
			labels = np.array(housing.price)
			# Define the features
			features = {'bedrooms':np.array(housing['bedrooms']), 
						'bathrooms':np.array(housing['bathrooms'])}
			return features, labels
			
Exercise - Defining Estimators

In the previous exercise, you defined a list of feature columns, feature_list, and a data input function, input_fn(). In this exercise, you will build on that work by defining an estimator that makes use of input data.

# Define the model and set the number of steps
model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])
model.train(input_fn, steps=1)

model = estimator.LinearRegressor(feature_columns=feature_list)
model.train(input_fn, steps=2)

