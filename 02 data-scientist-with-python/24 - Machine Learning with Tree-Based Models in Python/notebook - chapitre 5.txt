Inspecting the hyperparameters of a CART in sklearn

	# Import DecisionTreeClassifier
	from sklearn.tree import DecisionTreeClassifier
	# Set seed to 1 for reproducibility
	SEED = 1
	# Instantiate a DecisionTreeClassifier 'dt'
	dt = DecisionTreeClassifier(random_state=SEED)
	# Print out 'dt's hyperparameters
	print(dt.get_params())

	{'class_weight': None,
	'criterion': 'gini',
	'max_depth': None,
	'max_features': None,
	'max_leaf_nodes': None,
	'min_impurity_decrease': 0.0,
	'min_impurity_split': None,
	'min_samples_leaf': 1,
	'min_samples_split': 2,
	'min_weight_fraction_leaf': 0.0,
	'presort': False,
	'random_state': 1,
	'splitter': 'best'}

	# Import GridSearchCV
	from sklearn.model_selection import GridSearchCV
	# Define the grid of hyperparameters 'params_dt'
	params_dt = {
		'max_depth': [3, 4,5, 6],
		'min_samples_leaf': [0.04, 0.06, 0.08],
		'max_features': [0.2, 0.4,0.6, 0.8]
		}
	# Instantiate a 10-fold CV grid search object 'grid_dt'
	grid_dt = GridSearchCV(estimator=dt,
		param_grid=params_dt,
		scoring='accuracy',
		cv=10,
		n_jobs=-1)
	# Fit 'grid_dt' to the training data
	grid_dt.fit(X_train, y_train)

	# Extract best hyperparameters from 'grid_dt'
	best_hyperparams = grid_dt.best_params_
	print('Best hyerparameters:\n', best_hyperparams)
		Best hyerparameters:
		{'max_depth': 3, 'max_features': 0.4, 'min_samples_leaf': 0.06}
	# Extract best CV score from 'grid_dt'
	best_CV_score = grid_dt.best_score_
	print('Best CV accuracy'.format(best_CV_score))
		Best CV accuracy: 0.938

	# Extract best model from 'grid_dt'
	best_model = grid_dt.best_estimator_
	# Evaluate test set accuracy
	test_acc = best_model.score(X_test,y_test)
	# Print test set accuracy
	print("Test set accuracy of best model: {:.3f}".format(test_acc))
	Test set accuracy of best model: 0.947

Exercise - 	Tree hyperparameters

In the following exercises you'll revisit the Indian Liver Patient dataset which was introduced in a previous chapter.

Your task is to tune the hyperparameters of a classification tree. Given that this dataset is imbalanced, you'll be using the ROC AUC score as a metric instead of accuracy.

We have instantiated a DecisionTreeClassifier and assigned to dt with sklearn's default hyperparameters. You can inspect the hyperparameters of dt in your console.

Which of the following is not a hyperparameter of dt?

Exercise - Set the tree's hyperparameter grid

In this exercise, you'll manually set the grid of hyperparameters that will be used to tune the classification tree dt and find the optimal classifier in the next exercise.

		# Define params_dt
		params_dt = {'max_depth':[2,3,4], 'min_samples_leaf':[0.12,0.14,0.16,0.18]}

Exercise - Search for the optimal tree

In this exercise, you'll perform grid search using 5-fold cross validation to find dt's optimal hyperparameters. Note that because grid search is an exhaustive process, it may take a lot time to train the model. Here you'll only be instantiating the GridSearchCV object without fitting it to the training set. As discussed in the video, you can train such an object similar to any scikit-learn estimator by using the .fit() method:

grid_object.fit(X_train, y_train)

An untuned classification tree dt as well as the dictionary params_dt that you defined in the previous exercise are available in your workspace.

		# Import GridSearchCV
		from sklearn.model_selection import GridSearchCV

		# Instantiate grid_dt
		grid_dt = GridSearchCV(estimator=dt,
							   param_grid=params_dt,
							   scoring='roc_auc',
							   cv=5,
							   n_jobs=-1)
							   
Exercise - Evaluate the optimal tree

In this exercise, you'll evaluate the test set ROC AUC score of grid_dt's optimal model.

The dataset is already loaded and processed for you (numerical features are standardized); it is split into 80% train and 20% test. X_test, y_test are available in your workspace. In addition, we have also loaded the trained GridSearchCV object grid_dt that you instantiated in the previous exercise. Note that grid_dt was trained as follows:

grid_dt.fit(X_train, y_train)

# Import roc_auc_score from sklearn.metrics
from sklearn.metrics import roc_auc_score

# Extract the best estimator
best_model = grid_dt.best_estimator_

# Predict the test set probabilities of the positive class
y_pred_proba = best_model.predict_proba(X_test)[:,1]

# Compute test_roc_auc
test_roc_auc = roc_auc_score(y_test, y_pred_proba)

# Print test_roc_auc
print('Test set ROC AUC score: {:.3f}'.format(test_roc_auc))

<script.py> output:
    Test set ROC AUC score: 0.610
Great work! An untuned classification-tree would achieve a ROC AUC score of 0.54!


Inspecting RF Hyperparameters in sklearn
# Import RandomForestRegressor
from sklearn.ensemble import RandomForestRegressor
# Set seed for reproducibility
SEED = 1
# Instantiate a random forests regressor 'rf'
rf = RandomForestRegressor(random_state= SEED)

# Inspect rf' s hyperparameters
rf.get_params()

{'bootstrap': True,
'criterion': 'mse',
'max_depth': None,
'max_features': 'auto',
'max_leaf_nodes': None,
'min_impurity_decrease': 0.0,
'min_impurity_split': None,
'min_samples_leaf': 1,
'min_samples_split': 2,
'min_weight_fraction_leaf': 0.0,
'n_estimators': 10,
'n_jobs': -1,
'oob_score': False,
'random_state': 1,
'verbose': 0,
'warm_start': False}

	#Basic imports
	from sklearn.metrics import mean_squared_error as MSE
	from sklearn.model_selection import GridSearchCV
	# Define a grid of hyperparameter 'params_rf'
	params_rf = {
		'n_estimators': [300, 400, 500],
		'max_depth': [4, 6, 8],
		'min_samples_leaf': [0.1, 0.2],
		'max_features': ['log2', 'sqrt']
		}
	# Instantiate 'grid_rf'
	grid_rf = GridSearchCV(estimator=rf,
		param_grid=params_rf,
		cv=3,
		scoring='neg_mean_squared_error',
		verbose=1,
		n_jobs=-1)
	# Fit 'grid_rf' to the training set
	grid_rf.fit(X_train, y_train)
		Fitting 3 folds for each of 36 candidates, totalling 108 fits
		[Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 10.0s
		[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed: 24.3s finished
		RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=4,
			max_features='log2', max_leaf_nodes=None,
			min_impurity_decrease=0.0, min_impurity_split=None,
			min_samples_leaf=0.1, min_samples_split=2,
			min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,
			oob_score=False, random_state=1, verbose=0, warm_start=False)

Extracting the best hyperparameters
	# Extract best hyperparameters from 'grid_rf'
	best_hyperparams = grid_rf.best_params_
	print('Best hyerparameters:\n', best_hyperparams)
		Best hyerparameters:
			{'max_depth': 4,
			'max_features': 'log2',
			'min_samples_leaf': 0.1,
			'n_estimators': 400}

Evaluating the best model performance
	# Extract best model from 'grid_rf'
	best_model = grid_rf.best_estimator_
	# Predict the test set labels
	y_pred = best_model.predict(X_test)
	# Evaluate the test set RMSE
	rmse_test = MSE(y_test, y_pred)**(1/2)
	# Print the test set RMSE
	print('Test set RMSE of rf: {:.2f}'.format(rmse_test))
		Test set RMSE of rf: 3.89

Exercise - 	Random forests hyperparameters

In the following exercises, you'll be revisiting the Bike Sharing Demand dataset that was introduced in a previous chapter. Recall that your task is to predict the bike rental demand using historical weather data from the Capital Bikeshare program in Washington, D.C.. For this purpose, you'll be tuning the hyperparameters of a Random Forests regressor.

We have instantiated a RandomForestRegressor called rf using sklearn's default hyperparameters. You can inspect the hyperparameters of rf in your console.

Which of the following is not a hyperparameter of rf?

		In [1]: rf.get_params()
		Out[1]: 
		{'bootstrap': True,
		 'criterion': 'mse',
		 'max_depth': None,
		 'max_features': 'auto',
		 'max_leaf_nodes': None,
		 'min_impurity_decrease': 0.0,
		 'min_impurity_split': None,
		 'min_samples_leaf': 1,
		 'min_samples_split': 2,
		 'min_weight_fraction_leaf': 0.0,
		 'n_estimators': 10,
		 'n_jobs': -1,
		 'oob_score': False,
		 'random_state': 2,
		 'verbose': 0,
		 'warm_start': False}
		 
Exercise - Set the hyperparameter grid of RF

In this exercise, you'll manually set the grid of hyperparameters that will be used to tune rf's hyperparameters and find the optimal regressor. For this purpose, you will be constructing a grid of hyperparameters and tune the number of estimators, the maximum number of features used when splitting each node and the minimum number of samples (or fraction) per leaf.

		# Define the dictionary 'params_rf'
		params_rf = {'n_estimators':[100,350,500], 'max_features':['log2','auto','sqrt'], 'min_samples_leaf':[2,10,30]}
		
Exercise - Search for the optimal forest

In this exercise, you'll perform grid search using 3-fold cross validation to find rf's optimal hyperparameters. To evaluate each model in the grid, you'll be using the negative mean squared error metric.

Note that because grid search is an exhaustive search process, it may take a lot time to train the model. Here you'll only be instantiating the GridSearchCV object without fitting it to the training set. As discussed in the video, you can train such an object similar to any scikit-learn estimator by using the .fit() method:

grid_object.fit(X_train, y_train)

The untuned random forests regressor model rf as well as the dictionary params_rf that you defined in the previous exercise are available in your workspace.

		# Import GridSearchCV
		from sklearn.model_selection import GridSearchCV

		# Instantiate grid_rf
		grid_rf = GridSearchCV(estimator=rf,
							   param_grid=params_rf,
							   scoring='neg_mean_squared_error',
							   cv=3,
							   verbose=1,
							   n_jobs=-1)

Exercise - Evaluate the optimal forest

In this last exercise of the course, you'll evaluate the test set RMSE of grid_rf's optimal model.

The dataset is already loaded and processed for you and is split into 80% train and 20% test. In your environment are available X_test, y_test and the function mean_squared_error from sklearn.metrics under the alias MSE. In addition, we have also loaded the trained GridSearchCV object grid_rf that you instantiated in the previous exercise. Note that grid_rf was trained as follows:

grid_rf.fit(X_train, y_train)

		# Import mean_squared_error from sklearn.metrics as MSE 
		from sklearn.metrics import mean_squared_error

		# Extract the best estimator
		best_model = grid_rf.best_estimator_

		# Predict test set labels
		y_pred = best_model.predict(X_test)

		# Compute rmse_test
		rmse_test = mean_squared_error(y_test, y_pred) ** (1/2)

		# Print rmse_test
		print('Test RMSE of best model: {:.3f}'.format(rmse_test)) 
