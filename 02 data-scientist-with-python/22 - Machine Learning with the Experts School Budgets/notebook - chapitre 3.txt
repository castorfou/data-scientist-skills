The pipeline workflow
? Repeatable way to go from raw data to trained model
? Pipeline object takes sequential list of steps
? Output of one step is input to next step
? Each step is a tuple with two elements
? Name: string
? Transform: obj implementing .fit()
and .transform()
? Flexible: a step can itself be another pipeline!

Instantiate pipeline, Preprocessing numeric features with missing data
	from sklearn.pipeline import Pipeline
	from sklearn.linear_model import LogisticRegression
	from sklearn.multiclass import OneVsRestClassifier
	from sklearn.preprocessing import Imputer
	X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric',
	...: 'with_missing']], pd.get_dummies(
	...: sample_df['label']), random_state=2)
	pl = Pipeline([
	....: ('imp', Imputer()),
	....: ('clf', OneVsRestClassifier(LogisticRegression()))
	....: ])
	pipeline.fit(X_train, y_train)
	accuracy = pl.score(X_test, y_test)
	print('accuracy on all numeric, incl nans: ', accuracy)
		accuracy on all numeric, incl nans: 0.48
	
Exercise - Instantiate pipeline

In order to make your life easier as you start to work with all of the data in your original DataFrame, df, it's time to turn to one of scikit-learn's most useful objects: the Pipeline.

For the next few exercises, you'll reacquaint yourself with pipelines and train a classifier on some synthetic (sample) data of multiple datatypes before using the same techniques on the main dataset.

The sample data is stored in the DataFrame, sample_df, which has three kinds of feature data: numeric, text, and numeric with missing values. It also has a label column with two classes, a and b.

In this exercise, your job is to instantiate a pipeline that trains using the numeric column of the sample data.- 	

		# Import Pipeline
		from sklearn.pipeline import Pipeline

		# Import other necessary modules
		from sklearn.model_selection import train_test_split
		from sklearn.linear_model import LogisticRegression
		from sklearn.multiclass import OneVsRestClassifier

		# Split and select numeric data only, no nans 
		X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric']],
															pd.get_dummies(sample_df['label']), 
															random_state=22)

		# Instantiate Pipeline object: pl
		pl = Pipeline([
				('clf', OneVsRestClassifier(LogisticRegression()))
			])

		# Fit the pipeline to the training data
		pl.fit(X_train, y_train)

		# Compute and print accuracy
		accuracy = pl.score(X_test, y_test)
		print("\nAccuracy on sample data - numeric, no nans: ", accuracy)

<script.py> output:
    
    Accuracy on sample data - numeric, no nans:  0.62
	
Exercise - Preprocessing numeric features

What would have happened if you had included the with 'with_missing' column in the last exercise? Without imputing missing values, the pipeline would not be happy (try it and see). So, in this exercise you'll improve your pipeline a bit by using the Imputer() imputation transformer from scikit-learn to fill in missing values in your sample data.

By default, the imputer transformer replaces NaNs with the mean value of the column. That's a good enough imputation strategy for the sample data, so you won't need to pass anything extra to the imputer.

After importing the transformer, you will edit the steps list used in the previous exercise by inserting a (name, transform) tuple. Recall that steps are processed sequentially, so make sure the new tuple encoding your preprocessing step is put in the right place.

The sample_df is in the workspace, in case you'd like to take another look. Make sure to select both numeric columns- in the previous exercise we couldn't use with_missing because we had no preprocessing step!


		# Import the Imputer object
		from sklearn.preprocessing import Imputer

		# Create training and test sets using only numeric data
		X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing']],
															pd.get_dummies(sample_df['label']), 
															random_state=456)

		# Insantiate Pipeline object: pl
		pl = Pipeline([
				('imp', Imputer()),
				('clf', OneVsRestClassifier(LogisticRegression()))
			])

		# Fit the pipeline to the training data
		pl.fit(X_train, y_train)

		# Compute and print accuracy
		accuracy = pl.score(X_test,y_test)
		print("\nAccuracy on sample data - all numeric, incl nans: ", accuracy)

<script.py> output:
    
    Accuracy on sample data - all numeric, incl nans:  0.636
	
Nice! Now you know how to use preprocessing in pipelines with numeric data, and it looks like the accuracy has improved because of it! Text data preprocessing is next!

Preprocessing text features
	from sklearn.feature_extraction.text import CountVectorizer
	X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],
		...: pd.get_dummies(
		...: sample_df['label']),
		...: random_state=2)
	pl = Pipeline([
		....: ('vec', CountVectorizer()),
		....: ('clf', OneVsRestClassifier(LogisticRegression()))
		....: ])
	pl.fit(X_train, y_train)
		Out[4]:
		Pipeline(steps=[('vec', CountVectorizer(analyzer='word', binary=False,
		decode_error='strict', dtype=<class 'numpy.int64'>, encoding='utf-8',
		input='content', lowercase=True, max_df=1.0, max_features=None, min_df=1,
		ngram_range=(1, 1), preprocessor=None, stop_words=None, strip_...=None,
		solver='liblinear', tol=0.0001, verbose=0, warm_start=False), n_jobs=1))])
	accuracy = pl.score(X_test, y_test)
	print('accuracy on sample data: ', accuracy)
		accuracy on sample data: 0.64

Preprocessing multiple dtypes,  FunctionTransformer, FeatureUnion Text and Numeric Features, Pu!ing it all together
		
	X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric',
		...: 'with_missing', 'text']], pd.get_dummies(
		...: sample_df['label']), random_state=2)
	from sklearn.preprocessing import FunctionTransformer
	from sklearn.pipeline import FeatureUnion
	
	
	get_text_data = FunctionTransformer(lambda x: x['text'],
		...: validate=False)
	get_numeric_data = FunctionTransformer(lambda x: x[['numeric',
		...: 'with_missing']], validate=False)
		
		
	from sklearn.pipeline import FeatureUnion
	union = FeatureUnion([
		...: ('numeric', numeric_pipeline),
		...: ('text', text_pipeline)
		...: ])


	numeric_pipeline = Pipeline([
		...: ('selector', get_numeric_data),
		...: ('imputer', Imputer())
		...: ])
	text_pipeline = Pipeline([
		...: ('selector', get_text_data),
		...: ('vectorizer', CountVectorizer())
		...: ])
	pl = Pipeline([
		...: ('union', FeatureUnion([
		...: ('numeric', numeric_pipeline),
		...: ('text', text_pipeline)
		...: ])),
		...: ('clf', OneVsRestClassifier(LogisticRegression()))
		...: ])

Exercise - Preprocessing text features

Here, you'll perform a similar preprocessing pipeline step, only this time you'll use the text column from the sample data.

To preprocess the text, you'll turn to CountVectorizer() to generate a bag-of-words representation of the data, as in Chapter 2. Using the default arguments, add a (step, transform) tuple to the steps list in your pipeline.

Make sure you select only the text column for splitting your training and test sets.

As usual, your sample_df is ready and waiting in the workspace.

		# Import the CountVectorizer
		from sklearn.feature_extraction.text import CountVectorizer

		# Split out only the text data
		X_train, X_test, y_train, y_test = train_test_split(sample_df['text'],
															pd.get_dummies(sample_df['label']), 
															random_state=456)

		# Instantiate Pipeline object: pl
		pl = Pipeline([
				('vec', CountVectorizer()),
				('clf', OneVsRestClassifier(LogisticRegression()))
			])

		# Fit to the training data
		pl.fit(X_train, y_train)

		# Compute and print accuracy
		accuracy = pl.score(X_test,y_test)
		print("\nAccuracy on sample data - just text data: ", accuracy)

<script.py> output:
    
    Accuracy on sample data - just text data:  0.808
	
Exercise - 	Multiple types of processing: FunctionTransformer

The next two exercises will introduce new topics you'll need to make your pipeline truly excel.

Any step in the pipeline must be an object that implements the fit and transform methods. The FunctionTransformer creates an object with these methods out of any Python function that you pass to it. We'll use it to help select subsets of data in a way that plays nicely with pipelines.

You are working with numeric data that needs imputation, and text data that needs to be converted into a bag-of-words. You'll create functions that separate the text from the numeric variables and see how the .fit() and .transform() methods work.

		# Import FunctionTransformer
		from sklearn.preprocessing import FunctionTransformer

		# Obtain the text data: get_text_data
		get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)

		# Obtain the numeric data: get_numeric_data
		get_numeric_data = FunctionTransformer(lambda x: x[['numeric', 'with_missing']], validate=False)

		# Fit and transform the text data: just_text_data
		just_text_data = get_text_data.fit_transform(sample_df)

		# Fit and transform the numeric data: just_numeric_data
		just_numeric_data = get_numeric_data.fit_transform(sample_df)

		# Print head to check results
		print('Text Data')
		print(just_text_data.head())
		print('\nNumeric Data')
		print(just_numeric_data.head())
		
Exercise - Multiple types of processing: FeatureUnion

Now that you can separate text and numeric data in your pipeline, you're ready to perform separate steps on each by nesting pipelines and using FeatureUnion().

These tools will allow you to streamline all preprocessing steps for your model, even when multiple datatypes are involved. Here, for example, you don't want to impute our text data, and you don't want to create a bag-of-words with our numeric data. Instead, you want to deal with these separately and then join the results together using FeatureUnion().

In the end, you'll still have only two high-level steps in your pipeline: preprocessing and model instantiation. The difference is that the first preprocessing step actually consists of a pipeline for numeric data and a pipeline for text data. The results of those pipelines are joined using FeatureUnion().

		# Import FeatureUnion
		from sklearn.pipeline import FeatureUnion

		# Split using ALL data in sample_df
		X_train, X_test, y_train, y_test = train_test_split(sample_df[['numeric', 'with_missing', 'text']],
															pd.get_dummies(sample_df['label']), 
															random_state=22)

		# Create a FeatureUnion with nested pipeline: process_and_join_features
		process_and_join_features = FeatureUnion(
					transformer_list = [
						('numeric_features', Pipeline([
							('selector', get_numeric_data),
							('imputer', Imputer())
						])),
						('text_features', Pipeline([
							('selector', get_text_data),
							('vectorizer', CountVectorizer())
						]))
					 ]
				)

		# Instantiate nested pipeline: pl
		pl = Pipeline([
				('union', process_and_join_features),
				('clf', OneVsRestClassifier(LogisticRegression()))
			])


		# Fit pl to the training data
		pl.fit(X_train, y_train)

		# Compute and print accuracy
		accuracy = pl.score(X_test, y_test)
		print("\nAccuracy on sample data - all data: ", accuracy)

<script.py> output:
    
    Accuracy on sample data - all data:  0.928

	
Using pipeline with the main dataset

In [1]: LABELS = ['Function', 'Use', 'Sharing', 'Reporting', 'Student_Type',
...: 'Position_Type', 'Object_Type', 'Pre_K', 'Operating_Status']
In [2]: NON_LABELS = [c for c in df.columns if c not in LABELS]
In [3]: len(NON_LABELS) - len(NUMERIC_COLUMNS)
Out[3]: 14
In [4]: import numpy as np
In [5]: import pandas as pd
In [6]: df = pd.read_csv('TrainingSetSample.csv', index_col=0)
In [7]: dummy_labels = pd.get_dummies(df[LABELS])
In [8]: X_train, X_test, y_train, y_test = multilabel_train_test_split(
...: df[NON_LABELS], dummy_labels,
...: 0.2)
In [10]: get_text_data = FunctionTransformer(combine_text_columns,
...: validate=False)
In [11]: get_numeric_data = FunctionTransformer(lambda x:
...: x[NUMERIC_COLUMNS], validate=False)
In [12]: pl = Pipeline([
...: ('union', FeatureUnion([
...: ('numeric_features', Pipeline([
...: ('selector', get_numeric_data),
...: ('imputer', Imputer())
...: ])),
...: ('text_features', Pipeline([
...: ('selector', get_text_data),
...: ('vectorizer', CountVectorizer())
...: ]))
...: ])
...: ),
...: ('clf', OneVsRestClassifier(LogisticRegression()))
...: ])
In [13]: pl.fit(X_train, y_train)
Out[13]:
Pipeline(steps=[('union', FeatureUnion(n_jobs=1,
transformer_list=[('numeric_features', Pipeline(steps=[('selector',
FunctionTransformer(accept_sparse=False, func=<function <lambda> at
0x11415ec80>, pass_y=False, validate=False)), ('imputer', Imputer(axis=0,
copy=True, missing_valu...=None, solver='liblinear', tol=0.0001, verbose=0,
warm_start=False),n_jobs=1))])

Flexibility of model step
? Is current model the best?
? Can quickly try different models with pipelines
? Pipeline preprocessing steps unchanged
? Edit the model step in your pipeline
? Random Forest, Naïve Bayes, k-NN

Easily try new models using pipeline
In [14]: from sklearn.ensemble import RandomForestClassifier
In [15]: pl = Pipeline([
		...: ('union', FeatureUnion(
			...: transformer_list = [
				...: ('numeric_features', Pipeline([
					...: ('selector', get_numeric_data),
					...: ('imputer', Imputer())
				...: ])),
				...: ('text_features', Pipeline([
					...: ('selector', get_text_data),
					...: ('vectorizer', CountVectorizer())
				...: ]))
			...: ]
		...: )),
		...: ('clf', OneVsRest(RandomForestClassifier()))
	...: ])


Exercise - Using FunctionTransformer on the main dataset

In this exercise you're going to use FunctionTransformer on the primary budget data, before instantiating a multiple-datatype pipeline in the next exercise.

Recall from Chapter 2 that you used a custom function combine_text_columns to select and properly format text data for tokenization; it is loaded into the workspace and ready to be put to work in a function transformer!

Concerning the numeric data, you can use NUMERIC_COLUMNS, preloaded as usual, to help design a subset-selecting lambda function.

You're all finished with sample data. The original df is back in the workspace, ready to use.

		# Import FunctionTransformer
		from sklearn.preprocessing import FunctionTransformer

		# Get the dummy encoding of the labels
		dummy_labels = pd.get_dummies(df[LABELS])

		# Get the columns that are features in the original df
		NON_LABELS = [c for c in df.columns if c not in LABELS]

		# Split into training and test sets
		X_train, X_test, y_train, y_test = multilabel_train_test_split(df[NON_LABELS],
																	   dummy_labels,
																	   0.2, 
																	   seed=123)

		# Preprocess the text data: get_text_data
		get_text_data = FunctionTransformer(combine_text_columns, validate=False)

		# Preprocess the numeric data: get_numeric_data
		get_numeric_data = FunctionTransformer(lambda x: x[NUMERIC_COLUMNS], validate=False)

		
Exercise - Add a model to the pipeline

You're about to take everything you've learned so far and implement it in a Pipeline that works with the real, DrivenData budget line item data you've been exploring.

Surprise! The structure of the pipeline is exactly the same as earlier in this chapter:

    the preprocessing step uses FeatureUnion to join the results of nested pipelines that each rely on FunctionTransformer to select multiple datatypes
    the model step stores the model object

You can then call familiar methods like .fit() and .score() on the Pipeline object pl.

		# Complete the pipeline: pl
		pl = Pipeline([
				('union', FeatureUnion(
					transformer_list = [
						('numeric_features', Pipeline([
							('selector', get_numeric_data),
							('imputer', Imputer())
						])),
						('text_features', Pipeline([
							('selector', get_text_data),
							('vectorizer', CountVectorizer())
						]))
					 ]
				)),
				('clf', OneVsRestClassifier(LogisticRegression()))
			])

		# Fit to the training data
		pl.fit(X_train, y_train)

		# Compute and print accuracy
		accuracy = pl.score(X_test, y_test)
		print("\nAccuracy on budget dataset: ", accuracy)

<script.py> output:
    
    Accuracy on budget dataset:  0.20384615384615384
	
Great work! Now that you've built the entire pipeline, you can easily start trying out different models by just modifying the 'clf' step.	


Exercise - Try a different class of model

Now you're cruising. One of the great strengths of pipelines is how easy they make the process of testing different models.

Until now, you've been using the model step ('clf', OneVsRestClassifier(LogisticRegression())) in your pipeline.

But what if you want to try a different model? Do you need to build an entirely new pipeline? New nests? New FeatureUnions? Nope! You just have a simple one-line change, as you'll see in this exercise.

In particular, you'll swap out the logistic-regression model and replace it with a random forest classifier, which uses the statistics of an ensemble of decision trees to generate predictions.

		# Import random forest classifer
		from sklearn.ensemble import RandomForestClassifier

		# Edit model step in pipeline
		pl = Pipeline([
				('union', FeatureUnion(
					transformer_list = [
						('numeric_features', Pipeline([
							('selector', get_numeric_data),
							('imputer', Imputer())
						])),
						('text_features', Pipeline([
							('selector', get_text_data),
							('vectorizer', CountVectorizer())
						]))
					 ]
				)),
				('clf', RandomForestClassifier())
			])

		# Fit to the training data
		pl.fit(X_train, y_train)

		# Compute and print accuracy
		accuracy = pl.score(X_test, y_test)
		print("\nAccuracy on budget dataset: ", accuracy)

<script.py> output:
    
    Accuracy on budget dataset:  0.2826923076923077
	
Exercise - Can you adjust the model or parameters to improve accuracy?

You just saw a substantial improvement in accuracy by swapping out the model. Pipelines are amazing!

Can you make it better? Try changing the parameter n_estimators of RandomForestClassifier(), whose default value is 10, to 15.

		# Import RandomForestClassifier
		from sklearn.ensemble import RandomForestClassifier

		# Add model step to pipeline: pl
		pl = Pipeline([
				('union', FeatureUnion(
					transformer_list = [
						('numeric_features', Pipeline([
							('selector', get_numeric_data),
							('imputer', Imputer())
						])),
						('text_features', Pipeline([
							('selector', get_text_data),
							('vectorizer', CountVectorizer())
						]))
					 ]
				)),
				('clf', RandomForestClassifier(n_estimators=15))
			])

		# Fit to the training data
		pl.fit(X_train, y_train)

		# Compute and print accuracy
		accuracy = pl.score(X_test, y_test)
		print("\nAccuracy on budget dataset: ", accuracy)
		
<script.py> output:
    
    Accuracy on budget dataset:  0.3211538461538462
	
	