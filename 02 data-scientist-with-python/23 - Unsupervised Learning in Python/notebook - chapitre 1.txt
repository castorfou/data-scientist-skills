k-means clustering with scikit-learn
	print(samples)
		[[ 5. 3.3 1.4 0.2]
		[ 5. 3.5 1.3 0.3]
		[ 4.9 2.4 3.3 1. ]
		[ 6.3 2.8 5.1 1.5]
		...
		[ 7.2 3.2 6. 1.8]]
	from sklearn.cluster import KMeans
	model = KMeans(n_clusters=3)
	model.fit(samples)
		Out[4]: KMeans(algorithm='auto', ...)
	labels = model.predict(samples)
	print(labels)
		[0 0 1 1 0 1 2 1 0 1 ...]
		
Cluster labels for new samples
	print(new_samples)
		[[ 5.7 4.4 1.5 0.4]
		[ 6.5 3. 5.5 1.8]
		[ 5.8 2.7 5.1 1.9]]
	new_labels = model.predict(new_samples)
	print(new_labels)
		[0 2 1]
		
Scatter plots
	import matplotlib.pyplot as plt
	xs = samples[:,0]
	ys = samples[:,2]
	plt.scatter(xs, ys, c=labels)
	plt.show()

Exercise - Clustering 2D points

From the scatter plot of the previous exercise, you saw that the points seem to separate into 3 clusters. You'll now create a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise. After the model has been fit, you'll obtain the cluster labels for some new points using the .predict() method.

You are given the array points from the previous exercise, and also an array new_points.

		# Import KMeans
		from sklearn.cluster import KMeans

		# Create a KMeans instance with 3 clusters: model
		model = KMeans(n_clusters=3)

		# Fit model to points
		model.fit(points)

		# Determine the cluster labels of new_points: labels
		labels = model.predict(new_points)

		# Print cluster labels of new_points
		print(labels)

Great work! You've successfully performed k-Means clustering and predicted the labels of new points. But it is not easy to inspect the clustering by just looking at the printed labels. A visualization would be far more useful. In the next exercise, you'll inspect your clustering with a scatter plot!

Exercise - Inspect your clustering

Let's now inspect the clustering you performed in the previous exercise!

A solution to the previous exercise has already run, so new_points is an array of points and labels is the array of their cluster labels.

		# Import pyplot
		import matplotlib.pyplot as plt

		# Assign the columns of new_points: xs and ys
		xs = new_points[:,0]
		ys = new_points[:,1]

		# Make a scatter plot of xs and ys, using labels to define the colors
		plt.scatter(xs, ys, c=labels, alpha=0.5)

		# Assign the cluster centers: centroids
		centroids = model.cluster_centers_

		# Assign the columns of centroids: centroids_x, centroids_y
		centroids_x = centroids[:,0]
		centroids_y = centroids[:,1]

		# Make a scatter plot of centroids_x and centroids_y
		plt.scatter(centroids_x, centroids_y, marker='D', s=50)
		plt.show()
		
Fantastic! The clustering looks great! But how can you be sure that 3 clusters is the correct choice? In other words, how can you evaluate the quality of a clustering? Tune into the next video in which Ben will explain how to evaluate a clustering!

Cross tabulation with pandas
		print(species)
			['setosa', 'setosa', 'versicolor', 'virginica', ... ]
	Aligning labels and species
		import pandas as pd
		df = pd.DataFrame({'labels': labels, 'species': species})
		print(df)
			labels species
			0 1 setosa
			1 1 setosa
			2 2 versicolor
			3 2 virginica
			4 1 setosa
		ct = pd.crosstab(df['labels'], df['species'])
		print(ct)
			species setosa versicolor virginica
			labels
			0 0 2 36
			1 50 0 0
			2 0 48 14

Inertia measures clustering quality
	from sklearn.cluster import KMeans
	model = KMeans(n_clusters=3)
	model.fit(samples)
	print(model.inertia_)
		78.9408414261

How many clusters to choose?
? A good clustering has tight clusters (so low inertia)
? ... but not too many clusters!
? Choose an "elbow" in the inertia plot
? Where inertia begins to decrease more slowly

Exercise - How many clusters of grain?

In the video, you learned how to choose a good number of clusters for a dataset using the k-means inertia graph. You are given an array samples containing the measurements (such as area, perimeter, length, and several others) of samples of grain. What's a good number of clusters in this case?

KMeans and PyPlot (plt) have already been imported for you.

		ks = range(1, 6)
		inertias = []

		for k in ks:
			# Create a KMeans instance with k clusters: model
			model = KMeans(n_clusters=k)
			
			# Fit model to samples
			model.fit(samples)
			
			# Append the inertia to the list of inertias
			inertias.append(model.inertia_)
			
		# Plot ks vs inertias
		plt.plot(ks, inertias, '-o')
		plt.xlabel('number of clusters, k')
		plt.ylabel('inertia')
		plt.xticks(ks)
		plt.show()

Excellent job! The inertia decreases very slowly from 3 clusters to 4, so it looks like 3 clusters would be a good choice for this data.

Exercise - Evaluating the grain clustering

In the previous exercise, you observed from the inertia plot that 3 is a good number of clusters for the grain data. In fact, the grain samples come from a mix of 3 different grain varieties: "Kama", "Rosa" and "Canadian". In this exercise, cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a cross-tabulation.

You have the array samples of grain samples, and a list varieties giving the grain variety for each sample. Pandas (pd) and KMeans have already been imported for you.

		# Create a KMeans model with 3 clusters: model
		model = KMeans(n_clusters=3)

		# Use fit_predict to fit model and obtain cluster labels: labels
		labels = model.fit_predict(samples)

		# Create a DataFrame with labels and varieties as columns: df
		df = pd.DataFrame({'labels': labels, 'varieties': varieties})

		# Create crosstab: ct
		ct = pd.crosstab(df['labels'], df['varieties'])

		# Display ct
		print(ct)

Great work! The cross-tabulation shows that the 3 varieties of grain separate really well into 3 clusters. But depending on the type of data you are working with, the clustering may not always be this good. Is there anything you can do in such situations to improve your clustering? You'll find out in the next video!	


Pipelines combine multiple steps 	StandardScaler() and KMeans()

	from sklearn.preprocessing import StandardScaler
	from sklearn.cluster import KMeans
	scaler = StandardScaler()
	kmeans = KMeans(n_clusters=3)
	from sklearn.pipeline import make_pipeline
	pipeline = make_pipeline(scaler, kmeans)
	pipeline.fit(samples)
		Out[7]: Pipeline(steps=...)
	labels = pipeline.predict(samples)

	df = pd.DataFrame({'labels': labels, 'varieties': varieties})
	ct = pd.crosstab(df['labels'], df['varieties'])
	print(ct)
		varieties Barbera Barolo Grignolino
		labels
		0 0 59 3
		1 48 0 3
		2 0 0 65

Exercise - Scaling fish data for clustering

You are given an array samples giving measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, you'll need to standardize these features first. In this exercise, you'll build a pipeline to standardize and cluster the data.

		# Perform the necessary imports
		from sklearn.pipeline import make_pipeline
		from sklearn.preprocessing import StandardScaler
		from sklearn.cluster import KMeans

		# Create scaler: scaler
		scaler = StandardScaler()

		# Create KMeans instance: kmeans
		kmeans = KMeans(n_clusters=4)

		# Create pipeline: pipeline
		pipeline = make_pipeline(scaler,kmeans)
		
Exercise - Clustering the fish data

You'll now use your standardization and clustering pipeline from the previous exercise to cluster the fish by their measurements, and then create a cross-tabulation to compare the cluster labels with the fish species.

As before, samples is the 2D array of fish measurements. Your pipeline is available as pipeline, and the species of every fish sample is given by the list species.

		# Import pandas
		import pandas as pd

		# Fit the pipeline to samples
		pipeline.fit(samples)

		# Calculate the cluster labels: labels
		labels = pipeline.predict(samples)

		# Create a DataFrame with labels and species as columns: df
		df = pd.DataFrame({'labels':labels, 'species':species})

		# Create crosstab: ct
		ct = pd.crosstab(df['labels'],df['species'])

		# Display ct
		print(ct)	

Exercise - Clustering stocks using KMeans

In this exercise, you'll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day). You are given a NumPy array movements of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day.

Some stocks are more expensive than others. To account for this, include a Normalizer at the beginning of your pipeline. The Normalizer will separately transform each company's stock price to a relative scale before the clustering begins.

Note that Normalizer() is different to StandardScaler(), which you used in the previous exercise. While StandardScaler() standardizes features (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, Normalizer() rescales each sample - here, each company's stock price - independently of the other.

KMeans and make_pipeline have already been imported for you.
	
		# Import Normalizer
		from sklearn.preprocessing import Normalizer

		# Create a normalizer: normalizer
		normalizer = Normalizer()

		# Create a KMeans model with 10 clusters: kmeans
		kmeans = KMeans(n_clusters=10)

		# Make a pipeline chaining normalizer and kmeans: pipeline
		pipeline = make_pipeline(normalizer, kmeans)

		# Fit pipeline to the daily price movements
		pipeline.fit(movements)

Exercise - Which stocks move together?

In the previous exercise, you clustered companies by their daily stock price movements. So which company have stock prices that tend to change in the same way? You'll now inspect the cluster labels from your clustering to find out.

Your solution to the previous exercise has already been run. Recall that you constructed a Pipeline pipeline containing a KMeans model and fit it to the NumPy array movements of daily stock movements. In addition, a list companies of the company names is available.

		# Import pandas
		import pandas as pd

		# Predict the cluster labels: labels
		labels = pipeline.predict(movements)

		# Create a DataFrame aligning labels and companies: df
		df = pd.DataFrame({'labels': labels, 'companies': companies})

		# Display df sorted by cluster label
		print(df.sort_values('labels'))
		
Fantastic job - you have completed Chapter 1! Take a look at the clusters. Are you surprised by any of the results? In the next chapter, you'll learn about how to communicate results such as this through visualizations.

		