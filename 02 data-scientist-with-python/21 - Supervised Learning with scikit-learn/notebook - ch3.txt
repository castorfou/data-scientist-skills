Confusion matrix in scikit-learn
		from sklearn.metrics import classification_report
		from sklearn.metrics import confusion_matrix
		knn = KNeighborsClassifier(n_neighbors=8)
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)
		knn.fit(X_train, y_train)
		y_pred = knn.predict(X_test)

		print(confusion_matrix(y_test, y_pred))
			[[52 7]
			[ 3 112]]
		print(classification_report(y_test, y_pred))
			            precision recall f1-score support
			0           0.95      0.88   0.91      59
			1           0.94      0.97   0.96     115
			avg / total 0.94      0.94   0.94     174
			
Exercise - 	Metrics for classification

In Chapter 1, you evaluated the performance of your k-NN classifier based on its accuracy. However, as Andy discussed, accuracy is not always an informative metric. In this exercise, you will dive more deeply into evaluating the performance of binary classifiers by computing a confusion matrix and generating a classification report.

You may have noticed in the video that the classification report consisted of three rows, and an additional support column. The support gives the number of samples of the true response that lie in that class - so in the video example, the support was the number of Republicans or Democrats in the test set on which the classification report was computed. The precision, recall, and f1-score columns, then, gave the respective metrics for that particular class.

Here, you'll work with the PIMA Indians dataset obtained from the UCI Machine Learning Repository. The goal is to predict whether or not a given female patient will contract diabetes based on features such as BMI, age, and number of pregnancies. Therefore, it is a binary classification problem. A target value of 0 indicates that the patient does not have diabetes, while a value of 1 indicates that the patient does have diabetes. As in Chapters 1 and 2, the dataset has been preprocessed to deal with missing values.

The dataset has been loaded into a DataFrame df and the feature and target variable arrays X and y have been created for you. In addition, sklearn.model_selection.train_test_split and sklearn.neighbors.KNeighborsClassifier have already been imported.

Your job is to train a k-NN classifier to the data and evaluate its performance by generating a confusion matrix and classification report.

		# Import necessary modules
		from sklearn.metrics import classification_report
		from sklearn.metrics import confusion_matrix

		# Create training and test set
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

		# Instantiate a k-NN classifier: knn
		knn = KNeighborsClassifier(n_neighbors=6)

		# Fit the classifier to the training data
		knn.fit(X_train, y_train)

		# Predict the labels of the test data: y_pred
		y_pred = knn.predict(X_test)

		# Generate the confusion matrix and classification report
		print(confusion_matrix(y_test, y_pred))
		print(classification_report(y_test, y_pred))

<script.py> output:
    [[176  30]
     [ 52  50]]
                 precision    recall  f1-score   support
    
              0       0.77      0.85      0.81       206
              1       0.62      0.49      0.55       102
    
    avg / total       0.72      0.73      0.72       308

Excellent work! By analyzing the confusion matrix and classification report, you can get a much better understanding of your classifier's performance.

Logistic regression in scikit-learn
	from sklearn.linear_model import LogisticRegression
	from sklearn.model_selection import train_test_split
	logreg = LogisticRegression()
	X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.4, random_state=42)
	logreg.fit(X_train, y_train)
	y_pred = logreg.predict(X_test)

Plotting the ROC curve
	from sklearn.metrics import roc_curve
	y_pred_prob = logreg.predict_proba(X_test)[:,1]
	fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
	plt.plot([0, 1], [0, 1], 'k--')
	plt.plot(fpr, tpr, label='Logistic Regression')
	plt.xlabel('False Positive Rate’)
	plt.ylabel('True Positive Rate')
	plt.title('Logistic Regression ROC Curve')
	plt.show();	
		
Exercise - 	Building a logistic regression model

Time to build your first logistic regression model! As Hugo showed in the video, scikit-learn makes it very easy to try different models, since the Train-Test-Split/Instantiate/Fit/Predict paradigm applies to all classifiers and regressors - which are known in scikit-learn as 'estimators'. You'll see this now for yourself as you train a logistic regression model on exactly the same data as in the previous exercise. Will it outperform k-NN? There's only one way to find out!

The feature and target variable arrays X and y have been pre-loaded, and train_test_split has been imported for you from sklearn.model_selection.

		# Import the necessary modules
		from sklearn.linear_model import LogisticRegression
		from sklearn.metrics import confusion_matrix, classification_report

		# Create training and test sets
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)

		# Create the classifier: logreg
		logreg = LogisticRegression()

		# Fit the classifier to the training data
		logreg.fit(X_train, y_train)

		# Predict the labels of the test set: y_pred
		y_pred = logreg.predict(X_test)

		# Compute and print the confusion matrix and classification report
		print(confusion_matrix(y_test, y_pred))
		print(classification_report(y_test, y_pred))

You now know how to use logistic regression for binary classification - great work! Logistic regression is used in a variety of machine learning applications and will become a vital part of your data science toolbox.

Exercise - Plottin an ROC curve

Classification reports and confusion matrices are great methods to quantitatively evaluate model performance, while ROC curves provide a way to visually evaluate models. As Hugo demonstrated in the video, most classifiers in scikit-learn have a .predict_proba() method which returns the probability of a given sample being in a particular class. Having built a logistic regression model, you'll now evaluate its performance by plotting an ROC curve. In doing so, you'll make use of the .predict_proba() method and become familiar with its functionality.

Here, you'll continue working with the PIMA Indians diabetes dataset. The classifier has already been fit to the training data and is available as logreg.

		# Import necessary modules
		from sklearn.metrics import roc_curve

		# Compute predicted probabilities: y_pred_prob
		y_pred_prob = logreg.predict_proba(X_test)[:,1]

		# Generate ROC curve values: fpr, tpr, thresholds
		fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

		# Plot ROC curve
		plt.plot([0, 1], [0, 1], 'k--')
		plt.plot(fpr, tpr)
		plt.xlabel('False Positive Rate')
		plt.ylabel('True Positive Rate')
		plt.title('ROC Curve')
		plt.show()

Exercise - Precision-recall Curve

When looking at your ROC curve, you may have noticed that the y-axis (True positive rate) is also known as recall. Indeed, in addition to the ROC curve, there are other ways to visually evaluate model performance. One such way is the precision-recall curve, which is generated by plotting the precision and recall for different thresholds. As a reminder, precision and recall are defined as:

Precision = TP / (TP + FP)

Recall = TP / (TP + FN)

On the right, a precision-recall curve has been generated for the diabetes dataset. The classification report and confusion matrix are displayed in the IPython Shell.

Study the precision-recall curve and then consider the statements given below. Choose the one statement that is not true. Note that here, the class is positive (1) if the individual has diabetes.


AUC in scikit-learn
		from sklearn.metrics import roc_auc_score
		logreg = LogisticRegression()
		X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.4, random_state=42)
		logreg.fit(X_train, y_train)
		y_pred_prob = logreg.predict_proba(X_test)[:,1]
		roc_auc_score(y_test, y_pred_prob)
			Out[6]: 0.997466216216

AUC using cross-validation
		from sklearn.model_selection import cross_val_score
		cv_scores = cross_val_score(logreg, X, y, cv=5,scoring='roc_auc')
		print(cv_scores)
			[ 0.99673203 0.99183007 0.99583796 1. 0.96140652]

Exercise - AUC computation

Say you have a binary classifier that in fact is just randomly making guesses. It would be correct approximately 50% of the time, and the resulting ROC curve would be a diagonal line in which the True Positive Rate and False Positive Rate are always equal. The Area under this ROC curve would be 0.5. This is one way in which the AUC, which Hugo discussed in the video, is an informative metric to evaluate a model. If the AUC is greater than 0.5, the model is better than random guessing. Always a good sign!

In this exercise, you'll calculate AUC scores using the roc_auc_score() function from sklearn.metrics as well as by performing cross-validation on the diabetes dataset.

X and y, along with training and test sets X_train, X_test, y_train, y_test, have been pre-loaded for you, and a logistic regression classifier logreg has been fit to the training data.

		# Import necessary modules
		from sklearn.metrics import roc_auc_score
		from sklearn.model_selection import cross_val_score

		# Compute predicted probabilities: y_pred_prob
		y_pred_prob = logreg.predict_proba(X_test)[:,1]

		# Compute and print AUC score
		print("AUC: {}".format(roc_auc_score(y_test, y_pred_prob)))

		# Compute cross-validated AUC scores: cv_auc
		cv_auc = cross_val_score(logreg,X, y, cv=5, scoring='roc_auc')

		# Print list of AUC scores
		print("AUC scores computed using 5-fold cross-validation: {}".format(cv_auc))

<script.py> output:
    AUC: 0.8254806777079764
    AUC scores computed using 5-fold cross-validation: [ 0.80148148  0.8062963   0.81481481  0.86245283  0.8554717 ]

Hyperparameter tuning
? Linear regression: Choosing parameters
? Ridge/lasso regression: Choosing alpha
? k-Nearest Neighbors: Choosing n_neighbors
? Parameters like alpha and k: Hyperparameters
? Hyperparameters cannot be learned by fitting the model

GridSearchCV in scikit-learn
	from sklearn.model_selection import GridSearchCV
	param_grid = {'n_neighbors': np.arange(1, 50)}
	knn = KNeighborsClassifier()
	knn_cv = GridSearchCV(knn, param_grid, cv=5)
	knn_cv.fit(X, y)
	knn_cv.best_params_
		Out[6]: {'n_neighbors': 12}
	knn_cv.best_score_
		Out[7]: 0.933216168717
		
Exercise - 	Hyperparameter tuning with GridSearchCV

Hugo demonstrated how to tune the n_neighbors parameter of the KNeighborsClassifier() using GridSearchCV on the voting dataset. You will now practice this yourself, but by using logistic regression on the diabetes dataset instead!

Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a regularization parameter: C. C controls the inverse of the regularization strength, and this is what you will tune in this exercise. A large C can lead to an overfit model, while a small C can lead to an underfit model.

The hyperparameter space for C has been setup for you. Your job is to use GridSearchCV and logistic regression to find the optimal C in this hyperparameter space. The feature array is available as X and target variable array is available as y.

You may be wondering why you aren't asked to split the data into training and test sets. Good observation! Here, we want you to focus on the process of setting up the hyperparameter grid and performing grid-search cross-validation. In practice, you will indeed want to hold out a portion of your data for evaluation purposes, and you will learn all about this in the next video!

		# Import necessary modules
		from sklearn.linear_model import LogisticRegression
		from sklearn.model_selection import GridSearchCV

		# Setup the hyperparameter grid
		c_space = np.logspace(-5, 8, 15)
		param_grid = {'C': c_space}

		# Instantiate a logistic regression classifier: logreg
		logreg = LogisticRegression()

		# Instantiate the GridSearchCV object: logreg_cv
		logreg_cv = GridSearchCV(logreg, param_grid, cv=5)

		# Fit it to the data
		logreg_cv.fit(X,y)

		# Print the tuned parameters and score
		print("Tuned Logistic Regression Parameters: {}".format(logreg_cv.best_params_)) 
		print("Best score is {}".format(logreg_cv.best_score_))

<script.py> output:
    Tuned Logistic Regression Parameters: {'C': 3.7275937203149381}
    Best score is 0.7708333333333334

Exercise - Hyperparameter tuning with RandomizedSearchCV	

GridSearchCV can be computationally expensive, especially if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use RandomizedSearchCV, in which not all hyperparameter values are tried out. Instead, a fixed number of hyperparameter settings is sampled from specified probability distributions. You'll practice using RandomizedSearchCV in this exercise and see how this works.

Here, you'll also be introduced to a new model: the Decision Tree. Don't worry about the specifics of how this model works. Just like k-NN, linear regression, and logistic regression, decision trees in scikit-learn have .fit() and .predict() methods that you can use in exactly the same way as before. Decision trees have many parameters that can be tuned, such as max_features, max_depth, and min_samples_leaf: This makes it an ideal use case for RandomizedSearchCV.

As before, the feature array X and target variable array y of the diabetes dataset have been pre-loaded. The hyperparameter settings have been specified for you. Your goal is to use RandomizedSearchCV to find the optimal hyperparameters. Go for it!

		# Import necessary modules
		from scipy.stats import randint
		from sklearn.tree import DecisionTreeClassifier
		from sklearn.model_selection import RandomizedSearchCV

		# Setup the parameters and distributions to sample from: param_dist
		param_dist = {"max_depth": [3, None],
					  "max_features": randint(1, 9),
					  "min_samples_leaf": randint(1, 9),
					  "criterion": ["gini", "entropy"]}

		# Instantiate a Decision Tree classifier: tree
		tree = DecisionTreeClassifier()

		# Instantiate the RandomizedSearchCV object: tree_cv
		tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)

		# Fit it to the data
		tree_cv.fit(X,y)

		# Print the tuned parameters and score
		print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
		print("Best score is {}".format(tree_cv.best_score_))

<script.py> output:
    Tuned Decision Tree Parameters: {'criterion': 'entropy', 'max_depth': 3, 'max_features': 7, 'min_samples_leaf': 1}
    Best score is 0.7317708333333334

Great work! You'll see a lot more of decision trees and RandomizedSearchCV as you continue your machine learning journey. Note that RandomizedSearchCV will never outperform GridSearchCV. Instead, it is valuable because it saves on computation time.	
	
Hold-out set reasoning
? How well can the model perform on never before seen data?
? Using ALL data for cross-validation is not ideal
? Split data into training and hold-out set at the beginning
? Perform grid search cross-validation on training set
? Choose best hyperparameters and evaluate on hold-out set

Exercise - 	Hold-out set in practice I: Classification

You will now practice evaluating a model with tuned hyperparameters on a hold-out set. The feature array and target variable array from the diabetes dataset have been pre-loaded as X and y.

In addition to C, logistic regression has a 'penalty' hyperparameter which specifies whether to use 'l1' or 'l2' regularization. Your job in this exercise is to create a hold-out set, tune the 'C' and 'penalty' hyperparameters of a logistic regression classifier using GridSearchCV on the training set.
	
	
		# Import necessary modules
		from sklearn.model_selection import train_test_split
		from sklearn.linear_model import LogisticRegression
		from sklearn.model_selection import GridSearchCV

		# Create the hyperparameter grid
		c_space = np.logspace(-5, 8, 15)
		param_grid = {'C': c_space, 'penalty': ['l1', 'l2']}

		# Instantiate the logistic regression classifier: logreg
		logreg = LogisticRegression()

		# Create train and test sets
		X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)

		# Instantiate the GridSearchCV object: logreg_cv
		logreg_cv = GridSearchCV(logreg, param_grid, cv=5)

		# Fit it to the training data
		logreg_cv.fit((X_train,y_train)

		# Print the optimal parameters and best score
		print("Tuned Logistic Regression Parameter: {}".format(logreg_cv.best_params_))
		print("Tuned Logistic Regression Accuracy: {}".format(logreg_cv.best_score_))

<script.py> output:
    Tuned Logistic Regression Parameter: {'C': 0.43939705607607948, 'penalty': 'l1'}
    Tuned Logistic Regression Accuracy: 0.7652173913043478

Exercise - Hold-out set in practice II: Regression

Remember lasso and ridge regression from the previous chapter? Lasso used the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2

penalties:  a*L1+b*L2

In scikit-learn, this term is represented by the 'l1_ratio' parameter: An 'l1_ratio' of 1 corresponds to an L1 penalty, and anything lower is a combination of L1 and L2.

In this exercise, you will GridSearchCV to tune the 'l1_ratio' of an elastic net model trained on the Gapminder data. As in the previous exercise, use a hold-out set to evaluate your model's performance.

		# Import necessary modules
		from sklearn.linear_model import ElasticNet
		from sklearn.metrics import mean_squared_error
		from sklearn.model_selection import GridSearchCV, train_test_split

		# Create train and test sets
		X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

		# Create the hyperparameter grid
		l1_space = np.linspace(0, 1, 30)
		param_grid = {'l1_ratio': l1_space}

		# Instantiate the ElasticNet regressor: elastic_net
		elastic_net = ElasticNet()

		# Setup the GridSearchCV object: gm_cv
		gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)

		# Fit it to the training data
		gm_cv.fit(X_train, y_train)

		# Predict on the test set and compute metrics
		y_pred = gm_cv.predict(X_test)
		r2 = gm_cv.score(X_test, y_test)
		mse = mean_squared_error(y_test, y_pred)
		print("Tuned ElasticNet l1 ratio: {}".format(gm_cv.best_params_))
		print("Tuned ElasticNet R squared: {}".format(r2))
		print("Tuned ElasticNet MSE: {}".format(mse))

<script.py> output:
    Tuned ElasticNet l1 ratio: {'l1_ratio': 0.20689655172413793}
    Tuned ElasticNet R squared: 0.8668305372460283
    Tuned ElasticNet MSE: 10.05791413339844
Fantastic! Now that you understand how to fine-tune your models, it's time to learn about preprocessing techniques and how to piece together all the different stages of the machine learning process into a pipeline!



