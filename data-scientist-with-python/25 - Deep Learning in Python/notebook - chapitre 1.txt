Forward propagation code
	import numpy as np
	input_data = np.array([2, 3])
	weights = {'node_0': np.array([1, 1]),
		...: 'node_1': np.array([-1, 1]),
		...: 'output': np.array([2, -1])}
	node_0_value = (input_data * weights['node_0']).sum()
	node_1_value = (input_data * weights['node_1']).sum()
	hidden_layer_values = np.array([node_0_value, node_1_value])
	print(hidden_layer_values)
		[5, 1]
	output = (hidden_layer_values * weights['output']).sum()
	print(output)
		9

		Activation functions
	import numpy as np
	input_data = np.array([-1, 2])
	weights = {'node_0': np.array([3, 3]),
		...: 'node_1': np.array([1, 5]),
		...: 'output': np.array([2, -1])}
	node_0_input = (input_data * weights['node_0']).sum()
	node_0_output = np.tanh(node_0_input)
	node_1_input = (input_data * weights['node_1']).sum()
	node_1_output = np.tanh(node_1_input)
	hidden_layer_outputs = np.array([node_0_output, node_1_output])
	output = (hidden_layer_output * weights['output']).sum()
	print(output)
		1.2382242525694254		

Exercise - The Rectified Linear Activation Function

As Dan explained to you in the video, an "activation function" is a function applied at each node. It converts the node's input into some output.

The rectified linear activation function (called ReLU) has been shown to lead to very high-performance networks. This function takes a single number as an input, returning 0 if the input is negative, and the input if the input is positive.

Here are some examples:
relu(3) = 3
relu(-3) = 0

		def relu(input):
			'''Define your relu activation function here'''
			# Calculate the value for the output of the relu function: output
			output = max(0, input)
			
			# Return the value just calculated
			return(output)

		# Calculate node 0 value: node_0_output
		node_0_input = (input_data * weights['node_0']).sum()
		node_0_output = relu(node_0_input)

		# Calculate node 1 value: node_1_output
		node_1_input = (input_data * weights['node_1']).sum()
		node_1_output = relu(node_1_input)

		# Put node values into array: hidden_layer_outputs
		hidden_layer_outputs = np.array([node_0_output, node_1_output])

		# Calculate model output (do not apply relu)
		model_output = (hidden_layer_outputs * weights['output']).sum()

		# Print model output
		print(model_output)

Great work! You predicted 52 transactions. Without this activation function, you would have predicted a negative number! The real power of activation functions will come soon when you start tuning model weights.

Exercise - Applying the network to many observations/rows of data

You'll now define a function called predict_with_network() which will generate predictions for multiple data observations, which are pre-loaded as input_data. As before, weights are also pre-loaded. In addition, the relu() function you defined in the previous exercise has been pre-loaded.

		# Define predict_with_network()
		def predict_with_network(input_data_row, weights):

			# Calculate node 0 value
			node_0_input = (input_data_row * weights['node_0']).sum()
			node_0_output = relu(node_0_input)

			# Calculate node 1 value
			node_1_input = (input_data_row * weights['node_1']).sum()
			node_1_output = relu(node_1_input)

			# Put node values into array: hidden_layer_outputs
			hidden_layer_outputs = np.array([node_0_output, node_1_output])
			
			# Calculate model output
			input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()
			model_output = relu(input_to_final_layer)
			
			# Return model output
			return(model_output)


		# Create empty list to store prediction results
		results = []
		for input_data_row in input_data:
			# Append prediction to results
			results.append(predict_with_network(input_data_row, weights))

		# Print results
		print(results)

Exercise - 	Multi-layer neural networks

In this exercise, you'll write code to do forward propagation for a neural network with 2 hidden layers. Each hidden layer has two nodes. The input data has been preloaded as input_data. The nodes in the first hidden layer are called node_0_0 and node_0_1. Their weights are pre-loaded as weights['node_0_0'] and weights['node_0_1'] respectively.

The nodes in the second hidden layer are called node_1_0 and node_1_1. Their weights are pre-loaded as weights['node_1_0'] and weights['node_1_1'] respectively.

We then create a model output from the hidden nodes using weights pre-loaded as weights['output'].

		def predict_with_network(input_data):
			# Calculate node 0 in the first hidden layer
			node_0_0_input = (input_data * weights['node_0_0']).sum()
			node_0_0_output = relu(node_0_0_input)

			# Calculate node 1 in the first hidden layer
			node_0_1_input = (input_data * weights['node_0_1']).sum()
			node_0_1_output = relu(node_0_1_input)

			# Put node values into array: hidden_0_outputs
			hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])
			
			# Calculate node 0 in the second hidden layer
			node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()
			node_1_0_output = relu(node_1_0_input)

			# Calculate node 1 in the second hidden layer
			node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()
			node_1_1_output = relu(node_1_1_input)

			# Put node values into array: hidden_1_outputs
			hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])

			# Calculate model output: model_output
			model_output = (hidden_1_outputs * weights['output']).sum()
			
			# Return model_output
			return(model_output)

		output = predict_with_network(input_data)
		print(output)	
