Exercise - The vote for the Civil Rights Act in 1964

		# Construct arrays of data: dems, reps
		dems = np.array([True] * 153 + [False] * 91)
		reps = np.array([True] * 136 + [False] * 35)

		def frac_yea_dems(dems, reps):
			"""Compute fraction of Democrat yea votes."""
			frac = np.sum(dems) / len(dems)
			return frac

		# Acquire permutation samples: perm_replicates
		perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, 10000)

		# Compute and print p-value: p
		p = np.sum(perm_replicates <= 153/244) / len(perm_replicates)
		print('p-value =', p)
		
p-value = 0.0002
Great work! This small p-value suggests that party identity had a lot to do with the voting. Importantly, the South had a higher fraction of Democrat representatives, and consequently also a more racist bias.

Exercise - A time-on-website analog - 
We return to the no-hitter data set. In 1920, Major League Baseball implemented important rule changes that ended the so-called dead ball era. Importantly, the pitcher was no longer allowed to spit on or scuff the ball, an activity that greatly favors pitchers. In this problem you will perform an A/B test to determine if these rule changes resulted in a slower rate of no-hitters (i.e., longer average time between no-hitters) using the difference in mean inter-no-hitter time as your test statistic.

		# Compute the observed difference in mean inter-no-hitter times: nht_diff_obs
		nht_diff_obs = diff_of_means(nht_dead, nht_live)
		# Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates
		perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, 10000)
		# Compute and print the p-value: p
		p = np.sum(perm_replicates>=nht_diff_obs)/len(perm_replicates)
		print('p-val =', p)

Your p-value is 0.0001, which means that only one out of your 10,000 replicates had a result as extreme as the actual difference between the dead ball and live ball eras. This suggests strong statistical significance. Watch out, though, you could very well have gotten zero replicates that were as extreme as the observed value. This just means that the p-value is quite small, almost certainly smaller than 0.001.

Exercise - Hypothesis test on Pearson correlation

The observed correlation between female illiteracy and fertility may just be by chance; the fertility of a given country may actually be totally independent of its illiteracy. You will test this hypothesis. To do so, permute the illiteracy values but leave the fertility values fixed. This simulates the hypothesis that they are totally independent of each other. For each permutation, compute the Pearson correlation coefficient and assess how many of your permutation replicates have a Pearson correlation coefficient greater than the observed one. 

		# Compute observed correlation: r_obs
		r_obs = pearson_r(illiteracy,fertility)

		# Initialize permutation replicates: perm_replicates
		perm_replicates = np.empty(10000)

		# Draw replicates
		for i in range(10000):
			# Permute illiteracy measurments: illiteracy_permuted
			illiteracy_permuted = np.random.permutation(illiteracy)

			# Compute Pearson correlation
			perm_replicates[i] = pearson_r(illiteracy_permuted, fertility)

		# Compute p-value: p
		p = np.sum(perm_replicates >= r_obs) / len(perm_replicates)
		print('p-val =', p)
	
You got a p-value of zero. In hacker statistics, this means that your p-value is very low, since you never got a single replicate in the 10,000 you took that had a Pearson correlation greater than the observed one. You could try increasing the number of replicates you take to continue to move the upper bound on your p-value lower and lower.

Exercise - EDA Do neonicotinoid insecticides have unintended consequences?

		# Compute x,y values for ECDFs
		x_control, y_control = ecdf(control)
		x_treated, y_treated = ecdf(treated)

		# Plot the ECDFs
		plt.plot(x_control, y_control, marker='.', linestyle='none')
		plt.plot(x_treated, y_treated, marker='.', linestyle='none')

		# Set the margins
		plt.margins(0.02)

		# Add a legend
		plt.legend(('control', 'treated'), loc='lower right')

		# Label axes and show plot
		plt.xlabel('millions of alive sperm per mL')
		plt.ylabel('ECDF')
		plt.show()

Exercise - Bootstrap hypothesis test on bee sperm counts

		# Compute the difference in mean sperm count: diff_means
		diff_means = np.mean(control)-np.mean(treated)

		# Compute mean of pooled data: mean_count
		mean_count = np.mean(np.concatenate((control,treated)))

		# Generate shifted data sets
		control_shifted = control - np.mean(control) + mean_count
		treated_shifted = treated - np.mean(treated) + mean_count

		# Generate bootstrap replicates
		bs_reps_control = draw_bs_reps(control_shifted,
							   np.mean, size=10000)
		bs_reps_treated = draw_bs_reps(treated_shifted,
							   np.mean, size=10000)

		# Get replicates of difference of means: bs_replicates
		bs_replicates = bs_reps_control - bs_reps_treated

		# Compute and print p-value: p
		p = np.sum(bs_replicates >= np.mean(control) - np.mean(treated)) \
					/ len(bs_replicates)
		print('p-value =', p)
		
Nice work! The p-value is small, most likely less than 0.0001, since you never saw a bootstrap replicated with a difference of means at least as extreme as what was observed. In fact, when I did the calculation with 10 million replicates, I got a p-value of 2e-05.		
		