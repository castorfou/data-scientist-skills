Your well-equipped toolbox
● Graphical and quantitative EDA
● Parameter estimation
● Confidence interval calculation
● Hypothesis testing

Investigation of G. scandens beak depth
● EDA of beak depths in 1975 and 2012
● Parameter estimates of mean beak depth
● Hypothesis test: did the beaks get deeper?

Exercise - EDA of beak depths of Darwin's finches

	For your first foray into the Darwin finch data, you will study how the beak depth (the distance, top to bottom, of a closed beak) of the finch species Geospiza scandens has changed over time. The Grants have noticed some changes of beak geometry depending on the types of seeds available on the island, and they also noticed that there was some interbreeding with another major species on Daphne Major, Geospiza fortis. These effects can lead to changes in the species over time.

	In the next few problems, you will look at the beak depth of G. scandens on Daphne Major in 1975 and in 2012. To start with, let's plot all of the beak depth measurements in 1975 and 2012 in a bee swarm plot.

	The data are stored in a pandas DataFrame called df with columns 'year' and 'beak_depth'. The units of beak depth are millimeters (mm).

		# Create bee swarm plot
		_ = sns.swarmplot(x='year',y='beak_depth',data=df)

		# Label the axes
		_ = plt.xlabel('year')
		_ = plt.ylabel('beak depth (mm)')

		# Show the plot
		plt.show()
		
	It is kind of hard to see if there is a clear difference between the 1975 and 2012 data set. Eyeballing it, it appears as though the mean of the 2012 data set might be slightly higher, and it might have a bigger variance.

Exercise - 	ECDFs of beak depths

	While bee swarm plots are useful, we found that ECDFs are often even better when doing EDA. Plot the ECDFs for the 1975 and 2012 beak depth measurements on the same plot.
	For your convenience, the beak depths for the respective years has been stored in the NumPy arrays bd_1975 and bd_2012.
	
		# Compute ECDFs
		x_1975, y_1975 = ecdf(bd_1975)
		x_2012, y_2012 = ecdf(bd_2012)

		# Plot the ECDFs
		_ = plt.plot(x_1975, y_1975, marker='.', linestyle='none')
		_ = plt.plot(x_2012, y_2012, marker='.', linestyle='none')

		# Set margins
		plt.margins(0.02)

		# Add axis labels and legend
		_ = plt.xlabel('beak depth (mm)')
		_ = plt.ylabel('ECDF')
		_ = plt.legend(('1975', '2012'), loc='lower right')

		# Show the plot
		plt.show()

	The differences are much clearer in the ECDF. The mean is larger in the 2012 data, and the variance does appear larger as well.
	
Exercise - Parameter estimates of beak depths

	Estimate the difference of the mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval.

	Since in this exercise you will use the draw_bs_reps() function you wrote in chapter 2, it may be helpful to refer back to it.
	
		# Compute the difference of the sample means: mean_diff
		mean_diff = np.mean(bd_2012)-np.mean(bd_1975)

		# Get bootstrap replicates of means
		bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, 10000)
		bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, 10000)

		# Compute samples of difference of means: bs_diff_replicates
		bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975

		# Compute 95% confidence interval: conf_int
		conf_int = np.percentile(bs_diff_replicates,[2.5,97.5])

		# Print the results
		print('difference of means =', mean_diff, 'mm')
		print('95% confidence interval =', conf_int, 'mm')		
	
	<script.py> output:
    difference of means = 0.22622047244094645 mm
    95% confidence interval = [0.05633521 0.39190544] mm
	
Exercise - 	Hypothesis test: Are beaks deeper in 2012?

		# Compute mean of combined data set: combined_mean
		combined_mean = np.mean(np.concatenate((bd_1975, bd_2012)))

		# Shift the samples
		bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean
		bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean

		# Get bootstrap replicates of shifted data sets
		bs_replicates_1975 = draw_bs_reps(bd_1975_shifted,np.mean,10000)
		bs_replicates_2012 = draw_bs_reps(bd_2012_shifted,np.mean,10000)

		# Compute replicates of difference of means: bs_diff_replicates
		bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975

		# Compute the p-value
		p = np.sum(bs_diff_replicates >= mean_diff) / len(bs_diff_replicates)

		# Print p-value
		print('p =', p)

	We get a p-value of 0.0034, which suggests that there is a statistically significant difference. But remember: it is very important to know how different they are! In the previous exercise, you got a difference of 0.2 mm between the means. You should combine this with the statistical significance. Changing by 0.2 mm in 37 years is substantial by evolutionary standards. If it kept changing at that rate, the beak depth would double in only 400 years.

Exercise - EDA of beak length and depth

		# Make scatter plot of 1975 data
		_ = plt.plot(bl_1975, bd_1975, marker='.',
					 linestyle='None', color='blue', alpha=0.5)

		# Make scatter plot of 2012 data
		_ = plt.plot(bl_2012, bd_2012, marker='.',
					linestyle='None', color='red', alpha=0.5)

		# Label axes and make legend
		_ = plt.xlabel('beak length (mm)')
		_ = plt.ylabel('beak depth (mm)')
		_ = plt.legend(('1975', '2012'), loc='upper left')

		# Show the plot
		plt.show()	
		
	Great work! In looking at the plot, we see that beaks got deeper (the red points are higher up in the y-direction), but not really longer. If anything, they got a bit shorter, since the red dots are to the left of the blue dots. So, it does not look like the beaks kept the same shape; they became shorter and deeper.
	
Exercise - Linear regressions

	Perform a linear regression for both the 1975 and 2012 data. Then, perform pairs bootstrap estimates for the regression parameters. Report 95% confidence intervals on the slope and intercept of the regression line.
	You will use the draw_bs_pairs_linreg() function you wrote back in chapter 2.
	As a reminder, its call signature is draw_bs_pairs_linreg(x, y, size=1), and it returns bs_slope_reps and bs_intercept_reps. The beak length data are stored as bl_1975 and bl_2012, and the beak depth data is stored in bd_1975 and bd_2012

		# Compute the linear regressions
		slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975, 1)
		slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012, 1)

		# Perform pairs bootstrap for the linear regressions
		bs_slope_reps_1975, bs_intercept_reps_1975 = \
				draw_bs_pairs_linreg(bl_1975, bd_1975, 1000)
		bs_slope_reps_2012, bs_intercept_reps_2012 = \
				draw_bs_pairs_linreg(bl_2012, bd_2012, 1000)

		# Compute confidence intervals of slopes
		slope_conf_int_1975 = np.percentile(bs_slope_reps_1975,[2.5,97.5])
		slope_conf_int_2012 = np.percentile(bs_slope_reps_2012,[2.5,97.5])
		intercept_conf_int_1975 = np.percentile(bs_intercept_reps_1975,[2.5,97.5])

		intercept_conf_int_2012 = np.percentile(bs_intercept_reps_2012,[2.5,97.5])


		# Print the results
		print('1975: slope =', slope_1975,
			  'conf int =', slope_conf_int_1975)
		print('1975: intercept =', intercept_1975,
			  'conf int =', intercept_conf_int_1975)
		print('2012: slope =', slope_2012,
			  'conf int =', slope_conf_int_2012)
		print('2012: intercept =', intercept_2012,
			  'conf int =', intercept_conf_int_2012)
			  
	1975: slope = 0.4652051691605937 conf int = [0.33851226 0.59306491]
    1975: intercept = 2.3908752365842263 conf int = [0.64892945 4.18037063]
    2012: slope = 0.462630358835313 conf int = [0.33137479 0.60695527]
    2012: intercept = 2.977247498236019 conf int = [1.06792753 4.70599387]
	
	Nicely done! It looks like they have the same slope, but different intercepts.
	
Exercise - Displaying the linear regression results

		# Make scatter plot of 1975 data
		_ = plt.plot(bl_1975, bd_1975, marker='.',
					 linestyle='none', color='blue', alpha=0.5)

		# Make scatter plot of 2012 data
		_ = plt.plot(bl_2012, bd_2012, marker='.',
					 linestyle='none', color='red', alpha=0.5)

		# Label axes and make legend
		_ = plt.xlabel('beak length (mm)')
		_ = plt.ylabel('beak depth (mm)')
		_ = plt.legend(('1975', '2012'), loc='upper left')

		# Generate x-values for bootstrap lines: x
		x = np.array([10, 17])

		# Plot the bootstrap lines
		for i in range(100):
			plt.plot(x, bs_slope_reps_1975[i] * x + bs_intercept_reps_1975[i],
					 linewidth=0.5, alpha=0.2, color='blue')
			plt.plot(x, bs_slope_reps_2012[i] * x + bs_intercept_reps_2012[i],
					 linewidth=0.5, alpha=0.2, color='red')

		# Draw the plot again
		plt.show()

Exercise - Beak length to depth ratio

		# Compute length-to-depth ratios
		ratio_1975 = bl_1975 / bd_1975
		ratio_2012 = bl_2012 / bd_2012

		# Compute means
		mean_ratio_1975 = np.mean(ratio_1975)
		mean_ratio_2012 = np.mean(ratio_2012)

		# Generate bootstrap replicates of the means
		bs_replicates_1975 = draw_bs_reps(ratio_1975, np.mean,10000)
		bs_replicates_2012 = draw_bs_reps(ratio_2012, np.mean,10000)

		# Compute the 99% confidence intervals
		conf_int_1975 = np.percentile(bs_replicates_1975, [0.5,99.5])
		conf_int_2012 = np.percentile(bs_replicates_2012, [0.5,99.5])

		# Print the results
		print('1975: mean ratio =', mean_ratio_1975,
			  'conf int =', conf_int_1975)
		print('2012: mean ratio =', mean_ratio_2012,
			  'conf int =', conf_int_2012)

	1975: mean ratio = 1.5788823771858533 conf int = [1.55668803 1.60073509]
    2012: mean ratio = 1.4658342276847767 conf int = [1.44363932 1.48729149]
	
Exercise - EDA of heritability

		# Make scatter plots
		_ = plt.plot(bd_parent_fortis, bd_offspring_fortis,
					 marker='.', linestyle='none', color='blue', alpha=0.5)
		_ = plt.plot(bd_parent_scandens, bd_offspring_scandens,
					 marker='.', linestyle='none', color='red', alpha=0.5)

		# Label axes
		_ = plt.xlabel('parental beak depth (mm)')
		_ = plt.ylabel('offspring beak depth (mm)')

		# Add legend
		_ = plt.legend(('G. fortis', 'G. scandens'), loc='lower right')

		# Show plot
		plt.show()		

	It appears as though there is a stronger correlation in G. fortis than in G. scandens. This suggests that beak depth is more strongly inherited in G. fortis. We'll quantify this correlation next.

Exercise - Correlation of offspring and parental data

	In an effort to quantify the correlation between offspring and parent beak depths, we would like to compute statistics, such as the Pearson correlation coefficient, between parents and offspring. To get confidence intervals on this, we need to do a pairs bootstrap.

	You have already written a function to do pairs bootstrap to get estimates for parameters derived from linear regression. Your task in this exercise is to make a new function with call signature draw_bs_pairs(x, y, func, size=1) that performs pairs bootstrap and computes a single statistic on pairs samples defined. The statistic of interest in computed by calling func(bs_x, bs_y). In the next exercise, you will use pearson_r for func.

		def draw_bs_pairs(x, y, func, size=1):
			"""Perform pairs bootstrap for a single statistic."""

			# Set up array of indices to sample from: inds
			inds = np.arange(len(x))

			# Initialize replicates: bs_replicates
			bs_replicates = np.empty(size)

			# Generate replicates
			for i in range(size):
				bs_inds = np.random.choice(inds,size=len(inds))
				bs_x, bs_y = x[bs_inds], y[bs_inds]
				bs_replicates[i] = func(bs_x,bs_y)

			return bs_replicates
	
Exercise - Pearson correlation of offspring and parental data

	The Pearson correlation coefficient seems like a useful measure of how strongly the beak depth of parents are inherited by their offspring. Compute the Pearson correlation coefficient between parental and offspring beak depths for G. scandens. Do the same for G. fortis. Then, use the function you wrote in the last exercise to compute a 95% confidence interval using pairs bootstrap. 

		# Compute the Pearson correlation coefficients
		r_scandens = pearson_r(bd_parent_scandens,bd_offspring_scandens)
		r_fortis = pearson_r(bd_parent_fortis, bd_offspring_fortis)

		# Acquire 1000 bootstrap replicates of Pearson r
		bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, 1000)

		bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, 1000)


		# Compute 95% confidence intervals
		conf_int_scandens = np.percentile(bs_replicates_scandens,[2.5,97.5])
		conf_int_fortis = np.percentile(bs_replicates_fortis,[2.5,97.5])

		# Print results
		print('G. scandens:', r_scandens, conf_int_scandens)
		print('G. fortis:', r_fortis, conf_int_fortis)
	
	G. scandens: 0.4117063629401258 [0.26564228 0.54388972]
    G. fortis: 0.7283412395518487 [0.6694112  0.77840616]
	It is clear from the confidence intervals that beak depth of the offspring of G. fortis parents is more strongly correlated with their offspring than their G. scandens counterparts.
	
Exercise - Measuring heritability

	Remember that the Pearson correlation coefficient is the ratio of the covariance to the geometric mean of the variances of the two data sets. This is a measure of the correlation between parents and offspring, but might not be the best estimate of heritability. If we stop and think, it makes more sense to define heritability as the ratio of the covariance between parent and offspring to the variance of the parents alone. In this exercise, you will estimate the heritability and perform a pairs bootstrap calculation to get the 95% confidence interval.

	This exercise highlights a very important point. Statistical inference (and data analysis in general) is not a plug-n-chug enterprise. You need to think carefully about the questions you are seeking to answer with your data and analyze them appropriately. If you are interested in how heritable traits are, the quantity we defined as the heritability is more apt than the off-the-shelf statistic, the Pearson correlation coefficient.


	You have to be careful about how you index covariance_matrix. The covariance of the trait in parents and offspring can be accessed with the index [0,1] while the variance of the trait in the parents can be accessed with the index [0,0].
	
	
		def heritability(parents, offspring):
			"""Compute the heritability from parent and offspring samples."""
			covariance_matrix = np.cov(parents, offspring)
			return covariance_matrix[0,1] / covariance_matrix[0,0]

		# Compute the heritability
		heritability_scandens = heritability(bd_parent_scandens, bd_offspring_scandens)
		heritability_fortis = heritability(bd_parent_fortis, bd_offspring_fortis)

		# Acquire 1000 bootstrap replicates of heritability
		replicates_scandens = draw_bs_pairs(
				bd_parent_scandens, bd_offspring_scandens, heritability, size=1000)
				
		replicates_fortis = draw_bs_pairs(
				bd_parent_fortis, bd_offspring_fortis, heritability, size=1000)


		# Compute 95% confidence intervals
		conf_int_scandens = np.percentile(replicates_scandens,[2.5,97.5])
		conf_int_fortis =  np.percentile(replicates_fortis,[2.5,97.5])

		# Print results
		print('G. scandens:', heritability_scandens, conf_int_scandens)
		print('G. fortis:', heritability_fortis, conf_int_fortis)
	
	G. scandens: 0.5485340868685982 [0.34395487 0.75638267]
    G. fortis: 0.7229051911438159 [0.64655013 0.79688342]
	
	Here again, we see that G. fortis has stronger heritability than G. scandens. This suggests that the traits of G. fortis may be strongly incorporated into G. scandens by introgressive hybridization.
	
Exercise - Is beak depth heritable at all in G. scandens?

	# Initialize array of replicates: perm_replicates
	perm_replicates = np.empty(10000)

	# Draw replicates
	for i in range(10000):
		# Permute parent beak depths
		bd_parent_permuted = np.random.permutation(bd_parent_scandens)
		perm_replicates[i] = heritability(bd_parent_permuted, bd_offspring_scandens)


	# Compute p-value: p
	p = np.sum(perm_replicates >= heritability_scandens) / len(perm_replicates)

	# Print the p-value
	print('p-val =', p)

	
	You get a p-value of zero, which means that none of the 10,000 permutation pairs replicates you drew had a heritability high enough to match that which was observed. This strongly suggests that beak depth is heritable in G. scandens, just not as much as in G. fortis. If you like, you can plot a histogram of the heritability replicates to get a feel for how extreme of a value of heritability you might expect by chance.