Bagging
	Bootstrap Aggregation
	samples with replacement
	
Bagging: Classification & Regression

	Classification:
		Aggregates predictions by majority voting.
		BaggingClassifier in scikit-learn.
	Regression:
		Aggregates predictions through averaging.
		BaggingRegressor in scikit-learn.
	
Bagging Classifier in sklearn

	# Import models and utility functions
	from sklearn.ensemble import BaggingClassifier
	from sklearn.tree import DecisionTreeClassifier
	from sklearn.metrics import accuracy_score
	from sklearn.model_selection import train_test_split
	# Set seed for reproducibility
	SEED = 1
	# Split data into 70% train and 30% test
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
		stratify=y,
		random_state=SEED)
	# Instantiate a classification-tree 'dt'
	dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)
	# Instantiate a BaggingClassifier 'bc'
	bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1)
	# Fit 'bc' to the training set
	bc.fit(X_train, y_train)
	# Predict test set labels
	y_pred = bc.predict(X_test)
	# Evaluate and print test-set accuracy
	accuracy = accuracy_score(y_test, y_pred)
	print('Accuracy of Bagging Classifier: {:.3f}'.format(accuracy))

Accuracy of Bagging Classifier: 0.936

Exercise - Define the bagging classifier

In the following exercises you'll work with the Indian Liver Patient dataset from the UCI machine learning repository. Your task is to predict whether a patient suffers from a liver disease using 10 features including Albumin, age and gender. You'll do so using a Bagging Classifier.

		# Import DecisionTreeClassifier
		from sklearn.tree import DecisionTreeClassifier

		# Import BaggingClassifier
		from sklearn.ensemble import BaggingClassifier

		# Instantiate dt
		dt = DecisionTreeClassifier(random_state=1)

		# Instantiate bc
		bc = BaggingClassifier(base_estimator=dt, n_estimators=50, random_state=1)

Exercise - Evaluate Bagging performance

Now that you instantiated the bagging classifier, it's time to train it and evaluate its test set accuracy.

The Indian Liver Patient dataset is processed for you and split into 80% train and 20% test. The feature matrices X_train and X_test, as well as the arrays of labels y_train and y_test are available in your workspace. In addition, we have also loaded the bagging classifier bc that you instantiated in the previous exercise and the function accuracy_score() from sklearn.metrics.

		# Fit bc to the training set
		bc.fit(X_train, y_train)

		# Predict test set labels
		y_pred = bc.predict(X_test)

		# Evaluate acc_test
		acc_test = accuracy_score(y_test, y_pred)
		print('Test set accuracy of bc: {:.2f}'.format(acc_test)) 
		
Great work! A single tree dt would have achieved an accuracy of 63% which is 8% lower than bc's accuracy!

OOB Evaluation in sklearn

	# Import models and split utility function
	from sklearn.ensemble import BaggingClassifier
	from sklearn.tree import DecisionTreeClassifier
	from sklearn.metrics import accuracy_score
	from sklearn.model_selection import train_test_split
	# Set seed for reproducibility
	SEED = 1
	# Split data into 70% train and 30% test
	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.3,
		stratify= y,
		random_state=SEED)
	# Instantiate a classification-tree 'dt'
	dt = DecisionTreeClassifier(max_depth=4,
		min_samples_leaf=0.16,
		random_state=SEED)
	# Instantiate a BaggingClassifier 'bc'; set oob_score= True
	bc = BaggingClassifier(base_estimator=dt, n_estimators=300,
		oob_score=True, n_jobs=-1)
	# Fit 'bc' to the traing set
	bc.fit(X_train, y_train)
	# Predict the test set labels
	y_pred = bc.predict(X_test)
	# Evaluate test set accuracy
	test_accuracy = accuracy_score(y_test, y_pred)
	# Extract the OOB accuracy from 'bc'
	oob_accuracy = bc.oob_score_
	# Print test set accuracy
	print('Test set accuracy: {:.3f}'.format(test_accuracy))
		Test set accuracy: 0.936
	# Print OOB accuracy
	print('OOB accuracy: {:.3f}'.format(oob_accuracy))
		OOB accuracy: 0.925	

Exercise - Prepare the ground

In the following exercises, you'll compare the OOB accuracy to the test set accuracy of a bagging classifier trained on the Indian Liver Patient dataset.

In sklearn, you can evaluate the OOB accuracy of an ensemble classifier by setting the parameter oob_score to True during instantiation. After training the classifier, the OOB accuracy can be obtained by accessing the .oob_score_ attribute from the corresponding instance.

In your environment, we have made available the class DecisionTreeClassifier from sklearn.tree.

		# Import DecisionTreeClassifier
		from sklearn.tree import DecisionTreeClassifier

		# Import BaggingClassifier
		from sklearn.ensemble import BaggingClassifier

		# Instantiate dt
		dt = DecisionTreeClassifier(min_samples_leaf=8, random_state=1)

		# Instantiate bc
		bc = BaggingClassifier(base_estimator=dt, 
					n_estimators=50,
					oob_score=True,
					random_state=1)

Exercise - OOB Score vs Test Set Score

Now that you instantiated bc, you will fit it to the training set and evaluate its test set and OOB accuracies.

The dataset is processed for you and split into 80% train and 20% test. The feature matrices X_train and X_test, as well as the arrays of labels y_train and y_test are available in your workspace. In addition, we have also loaded the classifier bc instantiated in the previous exercise and the function accuracy_score() from sklearn.metrics.

		# Fit bc to the training set 
		bc.fit(X_train, y_train)

		# Predict test set labels
		y_pred = bc.predict(X_test)

		# Evaluate test set accuracy
		acc_test = accuracy_score(y_test, y_pred)

		# Evaluate OOB accuracy
		acc_oob = bc.oob_score_

		# Print acc_test and acc_oob
		print('Test set accuracy: {:.3f}, OOB accuracy: {:.3f}'.format(acc_test, acc_oob))

<script.py> output:
    Test set accuracy: 0.698, OOB accuracy: 0.704

Random Forests Regressor in sklearn

	# Basic imports
	from sklearn.ensemble import RandomForestRegressor
	from sklearn.model_selection import train_test_split
	from sklearn.metrics import mean_squared_error as MSE
	# Set seed for reproducibility
	SEED = 1
	# Split dataset into 70% train and 30% test
	X_train, X_test, y_train, y_test = train_test_split(X, y,
		test_size=0.3,
		random_state=SEED)
	# Instantiate a random forests regressor 'rf' 400 estimators
	rf = RandomForestRegressor(n_estimators=400,
	min_samples_leaf=0.12,
	random_state=SEED)
	# Fit 'rf' to the training set
	rf.fit(X_train, y_train)
	# Predict the test set labels 'y_pred'
	y_pred = rf.predict(X_test)
	# Evaluate the test set RMSE
	rmse_test = MSE(y_test, y_pred)**(1/2)
	# Print the test set RMSE
	print('Test set RMSE of rf: {:.2f}'.format(rmse_test))
		Test set RMSE of rf: 3.98

feature importance

	import pandas as pd
	import matplotlib.pyplot as plt
	# Create a pd.Series of features importances
	importances_rf = pd.Series(rf.feature_importances_, index = X.columns)
	# Sort importances_rf
	sorted_importances_rf = importances_rf.sort_values()
	# Make a horizontal bar plot
	sorted_importances_rf.plot(kind='barh', color='lightgreen'); plt.show()

Exercise - Train an RF regressor

In the following exercises you'll predict bike rental demand in the Capital Bikeshare program in Washington, D.C using historical weather data from the Bike Sharing Demand dataset available through Kaggle. For this purpose, you will be using the random forests algorithm. As a first step, you'll define a random forests regressor and fit it to the training set.

The dataset is processed for you and split into 80% train and 20% test. The features matrix X_train and the array y_train are available in your workspace.

		# Import RandomForestRegressor
		from sklearn.ensemble import RandomForestRegressor

		# Instantiate rf
		rf = RandomForestRegressor(n_estimators=25,
					random_state=2)
					
		# Fit rf to the training set    
		rf.fit(X_train, y_train)

Exercise - Evaluate the RF regressor

You'll now evaluate the test set RMSE of the random forests regressor rf that you trained in the previous exercise.

The dataset is processed for you and split into 80% train and 20% test. The features matrix X_test, as well as the array y_test are available in your workspace. In addition, we have also loaded the model rf that you trained in the previous exercise.

		# Import mean_squared_error as MSE
		from sklearn.metrics import mean_squared_error as MSE

		# Predict the test set labels
		y_pred = rf.predict(X_test)

		# Evaluate the test set RMSE
		rmse_test = MSE(y_test,y_pred)**(1/2)

		# Print rmse_test
		print('Test set RMSE of rf: {:.2f}'.format(rmse_test))

<script.py> output:
    Test set RMSE of rf: 51.97
	
Great work! You can try training a single CART on the same dataset. The test set RMSE achieved by rf is significantly smaller than that achieved by a single CART!	

Exercise - Visualizing features importances

In this exercise, you'll determine which features were the most predictive according to the random forests regressor rf that you trained in a previous exercise.
For this purpose, you'll draw a horizontal barplot of the feature importance as assessed by rf. Fortunately, this can be done easily thanks to plotting capabilities of pandas.
We have created a pandas.Series object called importances containing the feature names as index and their importances as values. In addition, matplotlib.pyplot is available as plt and pandas as pd.

		# Create a pd.Series of features importances
		importances = pd.Series(data=rf.feature_importances_,
								index= X_train.columns)

		# Sort importances
		importances_sorted = importances.sort_values()

		# Draw a horizontal barplot of importances_sorted
		importances_sorted.plot(kind='barh', color='lightgreen')
		plt.title('Features Importances')
		plt.show()

