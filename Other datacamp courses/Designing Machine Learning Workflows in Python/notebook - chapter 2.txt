Exercise - Is the source or the destination bad?
In the previous lesson, you used the destination computer as your entity of interest. However, your cybersecurity analyst just told you that it is the infected machines that generate the bad traffic, and will therefore appear as a source, not a destination, in the flows dataset.

The data flows has been preloaded, as well as the list bad of infected IDs and the feature extractor featurizer() from the previous lesson. You also have numpy available as np, AdaBoostClassifier(), and cross_val_score().

		# Group by source computer, and apply the feature extractor
		out = flows.groupby('source_computer').apply(featurize)

		# Convert the iterator to a dataframe by calling list on it
		X = pd.DataFrame(list(out), index=out.index)

		# Check which sources in X.index are bad to create labels
		y = [x in bads for x in X.index]

		# Report the average accuracy of Adaboost over 3-fold CV
		print(np.mean(cross_val_score(AdaBoostClassifier(), X, y)))
		
Exercise - Feature engineering on grouped data
You will now build on the previous exercise, by considering one additional feature: the number of unique protocols used by each source computer. Note that with grouped data, it is always possible to construct features in this manner: you can take the number of unique elements of all categorical columns, and the mean of all numeric columns as your starting point. As before, you have flows preloaded, cross_val_score() for measuring accuracy, AdaBoostClassifier(), pandas as pd and numpy as np.

Weights 

weights = [1.0]*len(y_train) + [0.1]*len(y_weak_train)

Exercise - Turning a heuristic into a classifier
You are surprised by the fact that heuristics can be so helpful. So you decide to treat the heuristic that "too many unique ports is suspicious" as a classifier in its own right. You achieve that by thresholding the number of unique ports per source by the average number used in bad source computers -- these are computers for which the label is True. The dataset is preloaded and split into training and test, so you have objects X_train, X_test, y_train and y_test in memory. Your imports include accuracy_score(), and numpy as np. To clarify: you won't be fitting a classifier from scikit-learn in this exercise, but instead you will define your own classification rule explicitly!

		# Create a new dataset X_train_bad by subselecting bad hosts
		X_train_bad = X_train[y_train]

		# Calculate the average of unique_ports in bad examples
		avg_bad_ports = np.mean(X_train_bad['unique_ports'])

		# Label as positive sources that use more ports than that
		pred_port = X_test['unique_ports'] > avg_bad_ports

		# Print the accuracy of the heuristic
		print(accuracy_score(y_test, pred_port))
		
Exercise - Combining heuristics
A different cyber analyst tells you that during certain types of attack, the infected source computer sends small bits of traffic, to avoid detection. This makes you wonder whether it would be better to create a combined heuristic that simultaneously looks for large numbers of ports and small packet sizes. Does this improve performance over the simple port heuristic? As with the last exercise, you have X_train, X_test, y_train and y_test in memory. The sample code also helps you reproduce the outcome of the port heuristic, pred_port. You also have numpy as np and accuracy_score() preloaded.

		# Compute the mean of average_packet for bad sources
		avg_bad_packet = np.mean(X_train[y_train]['average_packet'])

		# Label as positive if average_packet is lower than that
		pred_packet = X_test['average_packet'] < avg_bad_packet

		# Find indices where pred_port and pred_packet both True
		pred_port = X_test['unique_ports'] > avg_bad_ports
		pred_both = pred_packet & pred_port

		# Ports only produced an accuracy of 0.919. Is this better?
		print(accuracy_score(y_test, pred_both))
		
Exercise - Dealing with label noise
One of your cyber analysts informs you that many of the labels for the first 100 source computers in your training data might be wrong because of a database error. She hopes you can still use the data because most of the labels are still correct, but asks you to treat these 100 labels as "noisy". Thankfully you know how to do that, using weighted learning. The contaminated data is available in your workspace as X_train, X_test, y_train_noisy, y_test. You want to see if you can improve the performance of a GaussianNB() classifier using weighted learning. You can use the optional parameter sample_weight, which is supported by the .fit() methods of most popular classifiers. The function accuracy_score() is preloaded. You can consult the image below for guidance.

		# Fit a Gaussian Naive Bayes classifier to the training data
		clf = GaussianNB().fit(X_train, y_train_noisy)

		# Report its accuracy on the test data
		print(accuracy_score(y_test, clf.predict(X_test)))

		# Assign half the weight to the first 100 noisy examples
		weights = [0.5]*100 + [1.0]*(len(y_train_noisy)-100)

		# Refit using weights and report accuracy. Has it improved?
		clf_weights = GaussianNB().fit(X_train, y_train_noisy, sample_weight=weights)
		print(accuracy_score(y_test, clf_weights.predict(X_test)))
		
Scalar performance metrics and confusion matrix

	conf_mat = confusion_matrix(ground_truth, predictions)
	tn, fp, fn, tp = conf_mat.ravel()
	(fp, fn)
	
								(positives)			(negatives)
								Actually Bad		Actually Normal
	Predicted as Bad  		TP True Positives	FP False Positives
	Predicted as Normal		FN False Negatives	TN True Negatives
	
	accuracy = 1-(fp + fn)/len(ground_truth)
		accuracy = proportion of true positives and true negatives over the sum of all examples
	recall = tp/(tp+fn)
		recall = true positive rate = proportion of true positives over all positives examples
	fpr = fp/(tn+fp)
		fpr = false positives rate = proportion of false positives over all negatives examples
	precision = tp/(tp+fp)
		precision = proportion of true positives over all examples classified by the algorythm as positive
	f1 = 2*(precision*recall)/(precision+recall)
		f1 is the harmonic mean of precision and recall

	available from sklearn.metrics
		accuracy_score(ground_truth, predictions)
		recall_score(ground_truth, predictions)
		precision_score(ground_truth, predictions)
		f1_score(ground_truth, predictions)

Exercise - Reminder of performance metrics
Remember the credit dataset? With all the extra knowledge you now have about metrics, let's have another look at how good a random forest is on this dataset. You have already trained your classifier and obtained your confusion matrix on the test data. The test data and the results are available to you as tp, fp, fn and tn, for true positives, false positives, false negatives, and true negatives respectively. You also have the ground truth labels for the test data, y_test and the predicted labels, preds. The functions f1_score() and precision_score() have also been imported. 

		print(f1_score(y_test, preds))
		print(precision_score(y_test, preds))
		print((tp + tn)/len(y_test))

Exercise - Real-world cost analysis
You will still work on the credit dataset for this exercise. Recall that a "positive" in this dataset means "bad credit", i.e., a customer who defaulted on their loan, and a "negative" means a customer who continued to pay without problems. The bank manager informed you that the bank makes 10K profit on average from each "good risk" customer, but loses 150K from each "bad risk" customer. Your algorithm will be used to screen applicants, so those that are labeled as "negative" will be given a loan, and the "positive" ones will be turned down. What is the total cost of your classifier? The data is available as X_train, X_test, y_train and y_test. The functions confusion_matrix(), f1_score(), and precision_score() and RandomForestClassifier() are available.

		# Fit a random forest classifier to the training data
		clf = RandomForestClassifier(random_state=2).fit(X_train, y_train)

		# Label the test data
		preds = clf.predict(X_test)

		# Get false positives/negatives from the confusion matrix
		tp, fp, fn, tn = confusion_matrix(y_test, preds).ravel()

		# Now compute the cost using the manager's advice
		cost = fp*10 + fn*150


Exercise - Confusion matrix calculations
Your classifier on the credit data achieved the following statistics: 168 true positives, 19 false positives, 49 false negatives, and 25 true negatives. These numbers are preloaded in the console environment for you as tp, fp, fn and tn respectively. The following statements involve two metrics: accuracy, given by the proportion of examples classified correctly, and recall, which is the proportion of truly positive examples that were classified as positive. Which of the statements is true?

		Possible Answers
		The recall of the classifier is approximately 0.916
		The recall of the classifier is approximately 0.63
		The accuracy of the classifier is approximately 0.27
	**	The recall of the classifier is approximately 0.77

That's correct! Recall is given by tp/(tp+fn) and is a very useful and popular metric.


ROC curve (Receiver Operating Characteristic)

	scores = clf.predict_proba(X_test)
	[s[1] > 0.5 for s in scores] == clf.predict(X_test)

	fpr, tpr, thres = roc_curve(ground_truth,[s[1] for s in scores])
	plt.plot(fpr, tpr)
	plt.xlabel('False Positive Rate')
	plt.ylabel('True Positive Rate')
	
	
AUC (area under curve)

	clf = AdaBoostClassifier().fit(X_train, y_train)
	scores_ab = clf.predict_proba(X_test)
	roc_auc_score(ground_truth, [s[1] for s in scores_ab])	
	
Cost minimisation

	def my_scorer(y_test, y_est, cost_fp=10.0, cost_fn=1.0):
		tn, fp, fn, tp = confusion_matrix(y_test, y_est).ravel()
		return cost_fp*fp + cost_fn*fn

	t_range = [0.0, 0.25, 0.5, 0.75, 1.0]
	costs = [my_scorer(y_test, [s[1] > thres for s in scores]) for thres in t_range]
	
Exercise - Default thresholding
You would like to confirm that the DecisionTreeClassifier() uses the same default classification threshold as mentioned in the previous lesson, namely 0.5. It seems strange to you that all classifiers should use the same threshold. Let's check! A fitted decision tree classifier clf has been preloaded for you, as have the training and test data with their usual names: X_train, X_test, y_train and y_test. You will have to extract probability scores from the classifier using the .predict_proba() method.

		# Score the test data using the given classifier
		scores = clf.predict_proba(X_test)

		# Get labels from the scores using the default threshold
		preds = [s[1] > 0.5 for s in scores]

		# Use the predict method to label the test data again
		preds_default = clf.predict(X_test)

		# Compare the two sets of predictions
		all(preds == preds_default)

Exercise - Optimizing the threshold
You heard that the default value of 0.5 maximizes accuracy in theory, but you want to test what happens in practice. So you try out a number of different threshold values, to see what accuracy you get, and hence determine the best-performing threshold value. You repeat this experiment for the F1 score. Is 0.5 the optimal threshold? Is the optimal threshold for accuracy and for the F1 score the same? Go ahead and find out! You have a scores matrix available, obtained by scoring the test data. The ground truth labels for the test data is also available as y_test. Finally, two numpy functions are preloaded, argmin() and argmax(), which retrieve the index of the minimum and maximum values in an array respectively, in addition to the metrics accuracy_score() and f1_score().

		# Create a range of equally spaced threshold values
		t_range = [0.0, 0.25, 0.5, 0.75, 1.0]

		# Store the predicted labels for each value of the threshold
		preds = [[s[1] > thr for s in scores] for thr in t_range]

		# Compute the accuracy for each threshold
		accuracies = [accuracy_score(y_test, p) for p in preds]

		# Compute the F1 score for each threshold
		f1_scores = [f1_score(y_test, p) for p in preds]

		# Report the optimal threshold for accuracy, and for F1
		print(t_range[argmax(accuracies)], t_range[argmax(f1_scores)])

Exercise - Bringing it all together
One of the engineers in your arrhythmia detection startup rushes into your office to let you know that there is a problem with the ECG sensor for overweight users. You decide to reduce the influence of examples with weight over 80 by 50%. You are also told that since your startup is targeting the fitness market and makes no medical claims, scaring an athlete unnecessarily is costlier than missing a possible case of arrhythmia. You decide to create a custom loss that makes each "false alarm" ten times costlier than missing a case of arrhythmia. Does down-weighting overweight subjects improve this custom loss? Your training data X_train, y_train and test data X_test, y_test are preloaded, as are confusion_matrix(), numpy as np, and DecisionTreeClassifier().


		# Create a scorer assigning more cost to false positives
		def my_scorer(y_test, y_est, cost_fp=10.0, cost_fn=1.0):
			tn, fp, fn, tp = confusion_matrix(y_test, y_est).ravel()
			return fp*cost_fp+fn*cost_fn

		# Fit a DecisionTreeClassifier to the data and compute the loss
		clf = DecisionTreeClassifier(random_state=2).fit(X_train, y_train)
		print(my_scorer(y_test, clf.predict(X_test)))

		# Refit with same seed, downweighting subjects weighing > 80
		weights = [0.5 if w > 80 else 1.0 for w in X_train.weight]
		clf_weighted = DecisionTreeClassifier(random_state=2).fit(X_train,y_train,sample_weight=weights)
		print(my_scorer(y_test, clf_weighted.predict(X_test)))


	