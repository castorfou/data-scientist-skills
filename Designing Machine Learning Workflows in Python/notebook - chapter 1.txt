Exercise - Feature engineering

You are tasked to predict whether a new cohort of loan applicants are likely to default on their loans. You have a historical dataset and wish to train a classifier on it. You notice that many features are in string format, which is a problem for your classifiers. You hence decide to encode the string columns numerically using LabelEncoder(). The function has been preloaded for you from the preprocessing submodule of sklearn. The dataset credit is also preloaded, as is a list of all column names whose data types are string, stored in non_numeric_columns.

		# Inspect the first few lines of your data using head()
		credit.head(3)

		# Create a label encoder for each column. Encode the values
		for column in non_numeric_columns:
			le = LabelEncoder()
			credit[column] = le.fit_transform(credit[column])

		# Inspect the data types of the columns of the data frame
		print(credit.dtypes)

Exercise - Your first pipeline

Your colleague has used AdaBoostClassifier for the credit scoring dataset. You want to also try out a random forest classifier. In this exercise, you will fit this classifier to the data and compare it to AdaBoostClassifier. Make sure to use train/test data splitting to avoid overfitting. The data is preloaded and transformed so that all features are numeric. The features are available as X and the labels as y. The module RandomForestClassifier has also been preloaded.

		# Split the data into train and test, with 20% as test
		X_train, X_test, y_train, y_test = train_test_split(
		  X, y, test_size=0.2, random_state=1)

		# Create a random forest classifier, fixing the seed to 2
		rf_model = RandomForestClassifier(random_state=2).fit(
		  X_train, y_train)

		# Use it to predict the labels of the test data
		rf_predictions = rf_model.predict(X_test)

		# Assess the accuracy of both classifiers
		accuracies['rf'] = accuracy_score(y_test, rf_predictions)

Exercise - Grid search CV for model complexity

In the last slide, you saw how most classifiers have one or more hyperparameters that control its complexity. You also learned to tune them using GridSearchCV(). In this exercise, you will perfect this skill. You will experiment with:

    The number of trees, n_estimators, in a RandomForestClassifier.
    The maximum depth, max_depth, of the decision trees used in an AdaBoostClassifier.
    The number of nearest neighbors, n_neighbors, in KNeighborsClassifier.
	
		#%% Exercise - Grid search CV for model complexity
		from sklearn.model_selection import GridSearchCV

		# Set a range for n_estimators from 10 to 40 in steps of 10
		param_grid = {'n_estimators': range(10, 50, 10)}

		# Optimize for a RandomForestClassifier() using GridSearchCV
		grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)
		grid.fit(X, y)
		grid.best_params_

		# Define a grid for n_estimators ranging from 1 to 10
		param_grid = {'n_estimators': range(1, 11)}

		# Optimize for a AdaBoostClassifier() using GridSearchCV
		grid = GridSearchCV(AdaBoostClassifier(), param_grid, cv=3)
		grid.fit(X, y)
		grid.best_params_

		# Define a grid for n_neighbors with values 10, 50 and 100
		param_grid = {'n_neighbors': [10,50,100]}

		# Optimize for KNeighborsClassifier() using GridSearchCV
		grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3)
		grid.fit(X, y)
		grid.best_params_


That's right! The best-performing tree depth increases as the number of estimators grows in this case. This is in fact what tends to happen in most cases.


	