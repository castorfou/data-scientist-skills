Exercise - Feature engineering

You are tasked to predict whether a new cohort of loan applicants are likely to default on their loans. You have a historical dataset and wish to train a classifier on it. You notice that many features are in string format, which is a problem for your classifiers. You hence decide to encode the string columns numerically using LabelEncoder(). The function has been preloaded for you from the preprocessing submodule of sklearn. The dataset credit is also preloaded, as is a list of all column names whose data types are string, stored in non_numeric_columns.

		# Inspect the first few lines of your data using head()
		credit.head(3)

		# Create a label encoder for each column. Encode the values
		for column in non_numeric_columns:
			le = LabelEncoder()
			credit[column] = le.fit_transform(credit[column])

		# Inspect the data types of the columns of the data frame
		print(credit.dtypes)

Exercise - Your first pipeline

Your colleague has used AdaBoostClassifier for the credit scoring dataset. You want to also try out a random forest classifier. In this exercise, you will fit this classifier to the data and compare it to AdaBoostClassifier. Make sure to use train/test data splitting to avoid overfitting. The data is preloaded and transformed so that all features are numeric. The features are available as X and the labels as y. The module RandomForestClassifier has also been preloaded.

		# Split the data into train and test, with 20% as test
		X_train, X_test, y_train, y_test = train_test_split(
		  X, y, test_size=0.2, random_state=1)

		# Create a random forest classifier, fixing the seed to 2
		rf_model = RandomForestClassifier(random_state=2).fit(
		  X_train, y_train)

		# Use it to predict the labels of the test data
		rf_predictions = rf_model.predict(X_test)

		# Assess the accuracy of both classifiers
		accuracies['rf'] = accuracy_score(y_test, rf_predictions)

Exercise - Grid search CV for model complexity

In the last slide, you saw how most classifiers have one or more hyperparameters that control its complexity. You also learned to tune them using GridSearchCV(). In this exercise, you will perfect this skill. You will experiment with:

    The number of trees, n_estimators, in a RandomForestClassifier.
    The maximum depth, max_depth, of the decision trees used in an AdaBoostClassifier.
    The number of nearest neighbors, n_neighbors, in KNeighborsClassifier.
	
		#%% Exercise - Grid search CV for model complexity
		from sklearn.model_selection import GridSearchCV

		# Set a range for n_estimators from 10 to 40 in steps of 10
		param_grid = {'n_estimators': range(10, 50, 10)}

		# Optimize for a RandomForestClassifier() using GridSearchCV
		grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)
		grid.fit(X, y)
		grid.best_params_

		# Define a grid for n_estimators ranging from 1 to 10
		param_grid = {'n_estimators': range(1, 11)}

		# Optimize for a AdaBoostClassifier() using GridSearchCV
		grid = GridSearchCV(AdaBoostClassifier(), param_grid, cv=3)
		grid.fit(X, y)
		grid.best_params_

		# Define a grid for n_neighbors with values 10, 50 and 100
		param_grid = {'n_neighbors': [10,50,100]}

		# Optimize for KNeighborsClassifier() using GridSearchCV
		grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3)
		grid.fit(X, y)
		grid.best_params_


That's right! The best-performing tree depth increases as the number of estimators grows in this case. This is in fact what tends to happen in most cases.

Feature engineering and overfitting
	
Keyword encoding for categorical variables	
	
	from sklearn.feature_extraction.text import CountVectorizer
	vec = CountVectorizer()
	credit_scoring['purpose'] = credit_scoring['purpose'].apply(
		lambda s: ' '.join(s.split('_')), 0)
	dummy_matrix = vec.fit_transform(credit_scoring['purpose']).toarray()
	pd.DataFrame(dummy_matrix, columns=vec.get_feature_names()).head()

Feature selection

	from np.random import uniform
	fakes = pd.DataFrame(
		uniform(low=0.0, high=1.0, size=n * 100).reshape(X.shape[0], 100),
		columns=['fake_' + str(j) for j in range(100)]
	)
	X_with_fakes = pd.concat([X, fakes], 1)
	
	from sklearn.feature_selection import chi2, SelectKBest
	sk = SelectKBest(chi2, k=20)
	which_selected = sk.fit(X_with_fakes, y).get_support()
	X_with_fakes.columns[which_selected]

Exercise - Categorical encodings

		# Create numeric encoding for credit_history
		credit_history_num = LabelEncoder().fit_transform(
		  credit['credit_history'])

		# Create a new feature matrix including the numeric encoding
		X_num = pd.concat([X, pd.Series(credit_history_num)], axis=1)

		# Create new feature matrix with dummies for credit_history
		X_hot = pd.concat(
		  [X, pd.get_dummies(credit['credit_history'])], axis=1)

		# Compare the number of features of the resulting DataFrames
		X_hot.shape[1] > X_num.shape[1]	
		
Congratulations, you now have the choice between label and one-hot encoding at your fingertips!

Exercise - Feature transformations
You are discussing the credit dataset with the bank manager. She suggests that the safest loan applications tend to request mid-range credit amounts. Values that are either too low or too high suggest high risk. This means that a non-linear relationship might exist between this variable and the class. You want to test this hypothesis. You will construct a non-linear transformation of the feature. Then, you will assess which of the two features is better at predicting the class using SelectKBest() and the chi2() metric, both of which have been preloaded.

The data is available as a pandas DataFrame called credit, with the class contained in the column class. You also have preloaded pandas as pd and numpy as np.

		# Function computing absolute difference from column mean
		def abs_diff(x):
			return np.abs(x-np.mean(x))

		# Apply it to the credit amount and store to new column
		credit['diff'] = abs_diff(credit.credit_amount)

		# Create a feature selector with chi2 that picks one feature
		sk = SelectKBest(chi2, k=1)

		# Use the selector to pick between credit_amount and diff
		sk.fit(credit[['credit_amount', 'diff']], credit['class'])

		# Inspect the results
		sk.get_support()
		
Well done! You now have one more tool at your disposal to decide which features are worth introducing to your dataset.

Exercise - Bringing it all together

You just joined an arrhythmia detection startup and want to train a model on the arrhythmias dataset arrh. You noticed that random forests tend to win quite a few Kaggle competitions, so you want to try that out with a maximum depth of 2, 5, or 10, using grid search. You also observe that the dimension of the dataset is quite high so you wish to consider the effect of a feature selection method.

To make sure you don't overfit by mistake, you have already split your data. You will use X_train and y_train for the grid search, and X_test and y_test to decide if feature selection helps. All four dataset folds are preloaded in your environment. You also have access to GridSearchCV(), train_test_split(), SelectKBest(), chi2() and RandomForestClassifier as rfc.

		grid_search = GridSearchCV(
		  rfc(random_state=1), param_grid={'max_depth':[2,5,10]})
		best_value = grid_search.fit(
		  X_train, y_train).best_params_['max_depth']

		# Using the best value from above, fit a random forest
		clf = rfc(
		  random_state=1, max_depth=best_value).fit(X_train, y_train)

		# Apply SelectKBest with chi2 and pick top 100 features
		vt = SelectKBest(chi2, k=100).fit(X_train, y_train)

		# Create a new dataset only containing the selected features
		X_train_reduced = vt.transform(X_train)

You are already able to handle hundreds of features in a few lines of code! But what if the optimal number of estimators is different if you first apply feature selection? In Chapter 3 you will learn how to put your pipelines on steroids so that such questions can be asked in just one line of code.
