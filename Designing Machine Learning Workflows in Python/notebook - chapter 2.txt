Exercise - Is the source or the destination bad?
In the previous lesson, you used the destination computer as your entity of interest. However, your cybersecurity analyst just told you that it is the infected machines that generate the bad traffic, and will therefore appear as a source, not a destination, in the flows dataset.

The data flows has been preloaded, as well as the list bad of infected IDs and the feature extractor featurizer() from the previous lesson. You also have numpy available as np, AdaBoostClassifier(), and cross_val_score().

		# Group by source computer, and apply the feature extractor
		out = flows.groupby('source_computer').apply(featurize)

		# Convert the iterator to a dataframe by calling list on it
		X = pd.DataFrame(list(out), index=out.index)

		# Check which sources in X.index are bad to create labels
		y = [x in bads for x in X.index]

		# Report the average accuracy of Adaboost over 3-fold CV
		print(np.mean(cross_val_score(AdaBoostClassifier(), X, y)))
		
Exercise - Feature engineering on grouped data
You will now build on the previous exercise, by considering one additional feature: the number of unique protocols used by each source computer. Note that with grouped data, it is always possible to construct features in this manner: you can take the number of unique elements of all categorical columns, and the mean of all numeric columns as your starting point. As before, you have flows preloaded, cross_val_score() for measuring accuracy, AdaBoostClassifier(), pandas as pd and numpy as np.

Weights 

weights = [1.0]*len(y_train) + [0.1]*len(y_weak_train)

Exercise - Turning a heuristic into a classifier
You are surprised by the fact that heuristics can be so helpful. So you decide to treat the heuristic that "too many unique ports is suspicious" as a classifier in its own right. You achieve that by thresholding the number of unique ports per source by the average number used in bad source computers -- these are computers for which the label is True. The dataset is preloaded and split into training and test, so you have objects X_train, X_test, y_train and y_test in memory. Your imports include accuracy_score(), and numpy as np. To clarify: you won't be fitting a classifier from scikit-learn in this exercise, but instead you will define your own classification rule explicitly!

		# Create a new dataset X_train_bad by subselecting bad hosts
		X_train_bad = X_train[y_train]

		# Calculate the average of unique_ports in bad examples
		avg_bad_ports = np.mean(X_train_bad['unique_ports'])

		# Label as positive sources that use more ports than that
		pred_port = X_test['unique_ports'] > avg_bad_ports

		# Print the accuracy of the heuristic
		print(accuracy_score(y_test, pred_port))
		
Exercise - Combining heuristics
A different cyber analyst tells you that during certain types of attack, the infected source computer sends small bits of traffic, to avoid detection. This makes you wonder whether it would be better to create a combined heuristic that simultaneously looks for large numbers of ports and small packet sizes. Does this improve performance over the simple port heuristic? As with the last exercise, you have X_train, X_test, y_train and y_test in memory. The sample code also helps you reproduce the outcome of the port heuristic, pred_port. You also have numpy as np and accuracy_score() preloaded.

		# Compute the mean of average_packet for bad sources
		avg_bad_packet = np.mean(X_train[y_train]['average_packet'])

		# Label as positive if average_packet is lower than that
		pred_packet = X_test['average_packet'] < avg_bad_packet

		# Find indices where pred_port and pred_packet both True
		pred_port = X_test['unique_ports'] > avg_bad_ports
		pred_both = pred_packet & pred_port

		# Ports only produced an accuracy of 0.919. Is this better?
		print(accuracy_score(y_test, pred_both))
		
Exercise - Dealing with label noise
One of your cyber analysts informs you that many of the labels for the first 100 source computers in your training data might be wrong because of a database error. She hopes you can still use the data because most of the labels are still correct, but asks you to treat these 100 labels as "noisy". Thankfully you know how to do that, using weighted learning. The contaminated data is available in your workspace as X_train, X_test, y_train_noisy, y_test. You want to see if you can improve the performance of a GaussianNB() classifier using weighted learning. You can use the optional parameter sample_weight, which is supported by the .fit() methods of most popular classifiers. The function accuracy_score() is preloaded. You can consult the image below for guidance.

		# Fit a Gaussian Naive Bayes classifier to the training data
		clf = GaussianNB().fit(X_train, y_train_noisy)

		# Report its accuracy on the test data
		print(accuracy_score(y_test, clf.predict(X_test)))

		# Assign half the weight to the first 100 noisy examples
		weights = [0.5]*100 + [1.0]*(len(y_train_noisy)-100)

		# Refit using weights and report accuracy. Has it improved?
		clf_weights = GaussianNB().fit(X_train, y_train_noisy, sample_weight=weights)
		print(accuracy_score(y_test, clf_weights.predict(X_test)))
		
Scalar performance metrics and confusion matrix

	conf_mat = confusion_matrix(ground_truth, predictions)
	tn, fp, fn, tp = conf_mat.ravel()
	(fp, fn)
	
								(positives)			(negatives)
								Actually Bad		Actually Normal
	Predicted as Bad  		TP True Positives	FP False Positives
	Predicted as Normal		FN False Negatives	TN True Negatives
	
	accuracy = 1-(fp + fn)/len(ground_truth)
		accuracy = proportion of true positives and true negatives over the sum of all examples
	recall = tp/(tp+fn)
		recall = true positive rate = proportion of true positives over all positives examples
	fpr = fp/(tn+fp)
		fpr = false positives rate = proportion of false positives over all negatives examples
	precision = tp/(tp+fp)
		precision = proportion of true positives over all examples classified by the algorythm as positive
	f1 = 2*(precision*recall)/(precision+recall)
		f1 is the harmonic mean of precision and recall

	available from sklearn.metrics
		accuracy_score(ground_truth, predictions)
		recall_score(ground_truth, predictions)
		precision_score(ground_truth, predictions)
		f1_score(ground_truth, predictions)

Exercise - Reminder of performance metrics
Remember the credit dataset? With all the extra knowledge you now have about metrics, let's have another look at how good a random forest is on this dataset. You have already trained your classifier and obtained your confusion matrix on the test data. The test data and the results are available to you as tp, fp, fn and tn, for true positives, false positives, false negatives, and true negatives respectively. You also have the ground truth labels for the test data, y_test and the predicted labels, preds. The functions f1_score() and precision_score() have also been imported. 

		print(f1_score(y_test, preds))
		print(precision_score(y_test, preds))
		print((tp + tn)/len(y_test))

		